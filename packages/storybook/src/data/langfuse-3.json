{
  "trace": {
    "id": "6f58ac41090ac81ce6203900dadf8e6b",
    "projectId": "clkpwwm0m000gmm094odg11gi",
    "name": "QA-Chatbot",
    "timestamp": "2025-09-18T20:19:56.948Z",
    "environment": "default",
    "tags": [],
    "bookmarked": false,
    "release": null,
    "version": null,
    "userId": "u-7oksiGp6YBxTuLRugjV98",
    "sessionId": "chat_b537db89-0454-4ec2-9915-fc30b7e91beb",
    "public": false,
    "input": "\"what is a dataset\"",
    "output": "\"# What is a dataset in Langfuse?\\n\\nA dataset is a collection of dataset items (inputs plus optional expected outputs and metadata) used to test and evaluate your LLM application. Datasets are the input to Dataset Runs (experiments) where you compare prompts/models/configs and optionally score results.\\n\\n- Create datasets from production edge cases, synthetic data, or manual cases\\n- Use them to benchmark changes and prevent regressions\\n\\nMinimal example (Python)\\n_ create_dataset.py_\\n```python\\nfrom langfuse import Langfuse\\n\\nlf = Langfuse(public_key=\\\"<YOUR_PUBLIC_KEY>\\\", secret_key=\\\"<YOUR_SECRET_KEY>\\\", host=\\\"https://cloud.langfuse.com\\\")\\n\\nlf.create_dataset(\\n    name=\\\"getting-started\\\",\\n    description=\\\"My first dataset\\\",\\n    metadata={\\\"type\\\": \\\"benchmark\\\"}\\n)\\n\\nlf.create_dataset_item(\\n    dataset_name=\\\"getting-started\\\",\\n    input={\\\"question\\\": \\\"What is Langfuse?\\\"},\\n    expected_output={\\\"answer_contains\\\": \\\"LLM\\\"}\\n)\\nprint(\\\"Dataset ready\\\")\\n```\\n\\nLinks\\n- Concept and setup: Datasets — what they are and how to create/use them: https://langfuse.com/docs/evaluation/dataset-runs/datasets\\n- Dataset Runs (prompt experiments): https://langfuse.com/docs/evaluation/dataset-runs/native-run\\n- Data model (Dataset, DatasetItem, Runs): https://langfuse.com/docs/evaluation/dataset-runs/data-model\"",
    "metadata": "{\"resourceAttributes\":{\"service.name\":\"unknown_service:node\",\"telemetry.sdk.language\":\"nodejs\",\"telemetry.sdk.name\":\"opentelemetry\",\"telemetry.sdk.version\":\"2.0.1\"},\"scope\":{\"name\":\"langfuse-sdk\",\"version\":\"4.0.0\",\"attributes\":{}}}",
    "createdAt": "2025-09-18T20:20:03.000Z",
    "updatedAt": "2025-09-18T20:20:17.762Z",
    "scores": [
      {
        "id": "user-feedback-6f58ac41090ac81ce6203900dadf8e6b",
        "timestamp": "2025-09-18T20:21:26.107Z",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "environment": "default",
        "name": "user-feedback",
        "source": "API",
        "authorUserId": null,
        "comment": null,
        "metadata": "{}",
        "configId": null,
        "createdAt": "2025-09-18T20:21:27.000Z",
        "updatedAt": "2025-09-18T20:21:31.852Z",
        "queueId": null,
        "traceId": "6f58ac41090ac81ce6203900dadf8e6b",
        "observationId": null,
        "sessionId": null,
        "datasetRunId": null,
        "value": 1,
        "stringValue": null,
        "dataType": "NUMERIC"
      },
      {
        "id": "2ad1c1e3-5c7f-4903-a74a-0930203962c5",
        "timestamp": "2025-09-18T20:20:48.815Z",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "environment": "default",
        "name": "error-analysis_tool_getLangfuseDocsPage",
        "source": "EVAL",
        "authorUserId": null,
        "comment": "The user is not asking for an integration guide or integration information, so using another tool or no tool is acceptable.",
        "metadata": "{}",
        "configId": null,
        "createdAt": "2025-09-18T20:20:49.000Z",
        "updatedAt": "2025-09-18T20:20:48.879Z",
        "queueId": null,
        "traceId": "6f58ac41090ac81ce6203900dadf8e6b",
        "observationId": null,
        "sessionId": null,
        "datasetRunId": null,
        "value": 1,
        "stringValue": null,
        "dataType": "NUMERIC"
      },
      {
        "id": "17a0a4c5-e226-41fb-85ff-6358eaac5f25",
        "timestamp": "2025-09-18T20:20:48.568Z",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "environment": "default",
        "name": "error-analysis_OffTopic",
        "source": "EVAL",
        "authorUserId": null,
        "comment": "The answer is focused entirely on the concept of a dataset within Langfuse, which is relevant to the Langfuse and LLM/AI space, thus adhering to the scope requirements.",
        "metadata": "{}",
        "configId": null,
        "createdAt": "2025-09-18T20:20:49.000Z",
        "updatedAt": "2025-09-18T20:20:48.638Z",
        "queueId": null,
        "traceId": "6f58ac41090ac81ce6203900dadf8e6b",
        "observationId": null,
        "sessionId": null,
        "datasetRunId": null,
        "value": 1,
        "stringValue": null,
        "dataType": "NUMERIC"
      },
      {
        "id": "9fd2f6f9-571f-45ce-a611-3a1e371977de",
        "timestamp": "2025-09-18T20:20:48.554Z",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "environment": "default",
        "name": "is_same_language",
        "source": "EVAL",
        "authorUserId": null,
        "comment": "Text 1 is a simple English question, and Text 2 is an English explanation with code; both are in English, so the languages are the same.",
        "metadata": "{}",
        "configId": null,
        "createdAt": "2025-09-18T20:20:49.000Z",
        "updatedAt": "2025-09-18T20:20:48.624Z",
        "queueId": null,
        "traceId": "6f58ac41090ac81ce6203900dadf8e6b",
        "observationId": null,
        "sessionId": null,
        "datasetRunId": null,
        "value": 1,
        "stringValue": null,
        "dataType": "NUMERIC"
      },
      {
        "id": "d0697511-b016-4b77-8f2b-f3319baa51eb",
        "timestamp": "2025-09-18T20:20:48.238Z",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "environment": "default",
        "name": "is_question",
        "source": "EVAL",
        "authorUserId": null,
        "comment": "The user message is asking for the definition or explanation of a dataset, which is a question.",
        "metadata": "{}",
        "configId": null,
        "createdAt": "2025-09-18T20:20:49.000Z",
        "updatedAt": "2025-09-18T20:20:48.305Z",
        "queueId": null,
        "traceId": "6f58ac41090ac81ce6203900dadf8e6b",
        "observationId": null,
        "sessionId": null,
        "datasetRunId": null,
        "value": 1,
        "stringValue": null,
        "dataType": "NUMERIC"
      },
      {
        "id": "de343316-3138-43fe-9eba-bbb46a677c61",
        "timestamp": "2025-09-18T20:20:48.199Z",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "environment": "default",
        "name": "helpfulness",
        "source": "EVAL",
        "authorUserId": null,
        "comment": "The generation directly repeats the query without providing any additional information or context. It lacks depth and does not offer any explanation or clarification on the topic.",
        "metadata": "{}",
        "configId": null,
        "createdAt": "2025-09-18T20:20:49.000Z",
        "updatedAt": "2025-09-18T20:20:48.264Z",
        "queueId": null,
        "traceId": "6f58ac41090ac81ce6203900dadf8e6b",
        "observationId": null,
        "sessionId": null,
        "datasetRunId": null,
        "value": 0.2,
        "stringValue": null,
        "dataType": "NUMERIC"
      },
      {
        "id": "609b9394-d005-4eb1-8994-a68ca1d23930",
        "timestamp": "2025-09-18T20:20:28.121Z",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "environment": "default",
        "name": "contains-pii",
        "source": "EVAL",
        "authorUserId": null,
        "comment": "The text does not contain any PII such as emails, SSNs, or credit card numbers.",
        "metadata": "{}",
        "configId": null,
        "createdAt": "2025-09-18T20:20:29.000Z",
        "updatedAt": "2025-09-18T20:20:28.192Z",
        "queueId": null,
        "traceId": "6f58ac41090ac81ce6203900dadf8e6b",
        "observationId": null,
        "sessionId": null,
        "datasetRunId": null,
        "value": 0,
        "stringValue": null,
        "dataType": "NUMERIC"
      }
    ],
    "latency": 20.601,
    "observations": [
      {
        "id": "31ccdfbd9381c993",
        "traceId": "6f58ac41090ac81ce6203900dadf8e6b",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "type": "GENERATION",
        "environment": "default",
        "parentObservationId": "5043b6d994a0663c",
        "startTime": "2025-09-18T20:19:57.263Z",
        "endTime": "2025-09-18T20:20:17.549Z",
        "name": "ai.streamText",
        "metadata": "{\"attributes\":{\"operation.name\":\"ai.streamText\",\"ai.operationId\":\"ai.streamText\",\"ai.model.provider\":\"openai.responses\",\"ai.model.id\":\"gpt-5\",\"ai.settings.maxRetries\":\"2\",\"ai.telemetry.metadata.langfusePrompt\":\"{\\\"name\\\":\\\"langfuse-docs-assistant-text\\\",\\\"prompt\\\":\\\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\\\n\\\\n## Guidelines\\\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\\\n- Answer the question using only this and only this information, outputted in markdown format.\\\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\\\n\\\\n## Scope & Safety\\\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\\\n\\\\n## Clarifying Questions\\\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., “Which SDK are you using - Python or JS/TS?”).\\\\n\\\\n## Pricing & Discounts\\\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can’t confirm and point to contacting support.\\\\n\\\\n## Markdown Style Guide\\\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\\\n- **Sections to include (if relevant):** *Prerequisites* → *Setup* → *Minimal Example* → *Run* → *Troubleshooting* → *Links*.\\\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\\\n- **Keep it tight:** ≤ 20 lines of code per SDK section. If longer, collapse details or link out.\\\",\\\"version\\\":10,\\\"isFallback\\\":false,\\\"tags\\\":[],\\\"labels\\\":[\\\"production\\\",\\\"latest\\\"],\\\"type\\\":\\\"text\\\",\\\"config\\\":{\\\"model\\\":\\\"gpt-5\\\",\\\"reasoningSummary\\\":\\\"detailed\\\",\\\"textVerbosity\\\":\\\"low\\\",\\\"reasoningEffort\\\":\\\"low\\\"}}\",\"ai.prompt\":\"{\\\"system\\\":\\\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\\\n\\\\n## Guidelines\\\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\\\n- Answer the question using only this and only this information, outputted in markdown format.\\\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\\\n\\\\n## Scope & Safety\\\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\\\n\\\\n## Clarifying Questions\\\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., “Which SDK are you using - Python or JS/TS?”).\\\\n\\\\n## Pricing & Discounts\\\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can’t confirm and point to contacting support.\\\\n\\\\n## Markdown Style Guide\\\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\\\n- **Sections to include (if relevant):** *Prerequisites* → *Setup* → *Minimal Example* → *Run* → *Troubleshooting* → *Links*.\\\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\\\n- **Keep it tight:** ≤ 20 lines of code per SDK section. If longer, collapse details or link out.\\\",\\\"messages\\\":[{\\\"role\\\":\\\"assistant\\\",\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"**👋 Do you have any questions about Langfuse? Ask me!**\\\\n\\\\n_⚠️ Warning: Do not enter sensitive information. All chat messages can be viewed in the public demo project. Responses may be inaccurate. Please check the documentation for details or reach out to us via the chat widget._\\\"}]},{\\\"role\\\":\\\"user\\\",\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"what is a dataset\\\"}]}]}\",\"ai.response.finishReason\":\"stop\",\"ai.response.text\":\"# What is a dataset in Langfuse?\\n\\nA dataset is a collection of dataset items (inputs plus optional expected outputs and metadata) used to test and evaluate your LLM application. Datasets are the input to Dataset Runs (experiments) where you compare prompts/models/configs and optionally score results.\\n\\n- Create datasets from production edge cases, synthetic data, or manual cases\\n- Use them to benchmark changes and prevent regressions\\n\\nMinimal example (Python)\\n_ create_dataset.py_\\n```python\\nfrom langfuse import Langfuse\\n\\nlf = Langfuse(public_key=\\\"<YOUR_PUBLIC_KEY>\\\", secret_key=\\\"<YOUR_SECRET_KEY>\\\", host=\\\"https://cloud.langfuse.com\\\")\\n\\nlf.create_dataset(\\n    name=\\\"getting-started\\\",\\n    description=\\\"My first dataset\\\",\\n    metadata={\\\"type\\\": \\\"benchmark\\\"}\\n)\\n\\nlf.create_dataset_item(\\n    dataset_name=\\\"getting-started\\\",\\n    input={\\\"question\\\": \\\"What is Langfuse?\\\"},\\n    expected_output={\\\"answer_contains\\\": \\\"LLM\\\"}\\n)\\nprint(\\\"Dataset ready\\\")\\n```\\n\\nLinks\\n- Concept and setup: Datasets — what they are and how to create/use them: https://langfuse.com/docs/evaluation/dataset-runs/datasets\\n- Dataset Runs (prompt experiments): https://langfuse.com/docs/evaluation/dataset-runs/native-run\\n- Data model (Dataset, DatasetItem, Runs): https://langfuse.com/docs/evaluation/dataset-runs/data-model\",\"ai.response.providerMetadata\":\"{\\\"openai\\\":{\\\"responseId\\\":\\\"resp_68cc6979a1908195898244e8f9175c4a0e53a980a402aac8\\\",\\\"logprobs\\\":[[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]}}\",\"ai.usage.inputTokens\":\"18439\",\"ai.usage.outputTokens\":\"473\",\"ai.usage.totalTokens\":\"18912\",\"ai.usage.reasoningTokens\":\"128\",\"ai.usage.cachedInputTokens\":\"1152\"},\"resourceAttributes\":{\"service.name\":\"unknown_service:node\",\"telemetry.sdk.language\":\"nodejs\",\"telemetry.sdk.name\":\"opentelemetry\",\"telemetry.sdk.version\":\"2.0.1\"},\"scope\":{\"name\":\"ai\",\"attributes\":{}}}",
        "level": "DEFAULT",
        "statusMessage": null,
        "version": null,
        "input": "{\"system\":\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\n\\n## Guidelines\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\n- Answer the question using only this and only this information, outputted in markdown format.\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\n\\n## Scope & Safety\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\n\\n## Clarifying Questions\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., “Which SDK are you using - Python or JS/TS?”).\\n\\n## Pricing & Discounts\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can’t confirm and point to contacting support.\\n\\n## Markdown Style Guide\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\n- **Sections to include (if relevant):** *Prerequisites* → *Setup* → *Minimal Example* → *Run* → *Troubleshooting* → *Links*.\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\n- **Keep it tight:** ≤ 20 lines of code per SDK section. If longer, collapse details or link out.\",\"messages\":[{\"role\":\"assistant\",\"content\":[{\"type\":\"text\",\"text\":\"**👋 Do you have any questions about Langfuse? Ask me!**\\n\\n_⚠️ Warning: Do not enter sensitive information. All chat messages can be viewed in the public demo project. Responses may be inaccurate. Please check the documentation for details or reach out to us via the chat widget._\"}]},{\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":\"what is a dataset\"}]}]}",
        "output": "# What is a dataset in Langfuse?\n\nA dataset is a collection of dataset items (inputs plus optional expected outputs and metadata) used to test and evaluate your LLM application. Datasets are the input to Dataset Runs (experiments) where you compare prompts/models/configs and optionally score results.\n\n- Create datasets from production edge cases, synthetic data, or manual cases\n- Use them to benchmark changes and prevent regressions\n\nMinimal example (Python)\n_ create_dataset.py_\n```python\nfrom langfuse import Langfuse\n\nlf = Langfuse(public_key=\"<YOUR_PUBLIC_KEY>\", secret_key=\"<YOUR_SECRET_KEY>\", host=\"https://cloud.langfuse.com\")\n\nlf.create_dataset(\n    name=\"getting-started\",\n    description=\"My first dataset\",\n    metadata={\"type\": \"benchmark\"}\n)\n\nlf.create_dataset_item(\n    dataset_name=\"getting-started\",\n    input={\"question\": \"What is Langfuse?\"},\n    expected_output={\"answer_contains\": \"LLM\"}\n)\nprint(\"Dataset ready\")\n```\n\nLinks\n- Concept and setup: Datasets — what they are and how to create/use them: https://langfuse.com/docs/evaluation/dataset-runs/datasets\n- Dataset Runs (prompt experiments): https://langfuse.com/docs/evaluation/dataset-runs/native-run\n- Data model (Dataset, DatasetItem, Runs): https://langfuse.com/docs/evaluation/dataset-runs/data-model",
        "modelParameters": {
          "maxSteps": null,
          "toolChoice": null,
          "maxTokens": null,
          "finishReason": null,
          "system": "openai.responses",
          "maxRetries": "2",
          "mode": null,
          "temperature": null
        },
        "completionStartTime": null,
        "promptId": "5422a9e9-19cf-4ba8-bbd4-2fe0108eefc5",
        "createdAt": "2025-09-18T20:20:17.667Z",
        "updatedAt": "2025-09-18T20:20:17.673Z",
        "usageDetails": {},
        "costDetails": {},
        "providedCostDetails": {},
        "model": null,
        "internalModelId": null,
        "promptName": "langfuse-docs-assistant-text",
        "promptVersion": 10,
        "latency": 20286,
        "timeToFirstToken": null,
        "inputCost": null,
        "outputCost": null,
        "totalCost": 0,
        "inputUsage": 0,
        "outputUsage": 0,
        "totalUsage": 0,
        "internalModel": null
      },
      {
        "id": "00db41221c0d2850",
        "traceId": "6f58ac41090ac81ce6203900dadf8e6b",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "type": "GENERATION",
        "environment": "default",
        "parentObservationId": "31ccdfbd9381c993",
        "startTime": "2025-09-18T20:20:09.360Z",
        "endTime": "2025-09-18T20:20:17.544Z",
        "name": "ai.streamText.doStream",
        "metadata": "{}",
        "level": "DEFAULT",
        "statusMessage": null,
        "version": null,
        "modelParameters": {
          "maxSteps": null,
          "toolChoice": "{\"type\":\"auto\"}",
          "maxTokens": null,
          "finishReason": "stop",
          "system": "openai.responses",
          "maxRetries": "2",
          "mode": null,
          "temperature": null
        },
        "completionStartTime": "2025-09-18T20:20:09.975Z",
        "promptId": "5422a9e9-19cf-4ba8-bbd4-2fe0108eefc5",
        "createdAt": "2025-09-18T20:20:17.667Z",
        "updatedAt": "2025-09-18T20:20:17.672Z",
        "usageDetails": {
          "input": 17324,
          "output": 429,
          "total": 17753
        },
        "costDetails": {
          "input": 0.021655,
          "output": 0.00429,
          "total": 0.025945
        },
        "providedCostDetails": {},
        "model": "gpt-5",
        "internalModelId": "38c3822a-09a3-457b-b200-2c6f17f7cf2f",
        "promptName": "langfuse-docs-assistant-text",
        "promptVersion": 10,
        "latency": 8184,
        "timeToFirstToken": 0.615,
        "inputCost": 0.021655,
        "outputCost": 0.00429,
        "totalCost": 0.025945,
        "inputUsage": 17324,
        "outputUsage": 429,
        "totalUsage": 17753
      },
      {
        "id": "5043b6d994a0663c",
        "traceId": "6f58ac41090ac81ce6203900dadf8e6b",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "type": "GENERATION",
        "environment": "default",
        "parentObservationId": null,
        "startTime": "2025-09-18T20:19:56.948Z",
        "endTime": "2025-09-18T20:20:17.549Z",
        "name": "handle-chatbot-message",
        "metadata": "{\"resourceAttributes\":{\"service.name\":\"unknown_service:node\",\"telemetry.sdk.language\":\"nodejs\",\"telemetry.sdk.name\":\"opentelemetry\",\"telemetry.sdk.version\":\"2.0.1\"},\"scope\":{\"name\":\"langfuse-sdk\",\"version\":\"4.0.0\",\"attributes\":{}}}",
        "level": "DEFAULT",
        "statusMessage": null,
        "version": null,
        "input": "what is a dataset",
        "output": "# What is a dataset in Langfuse?\n\nA dataset is a collection of dataset items (inputs plus optional expected outputs and metadata) used to test and evaluate your LLM application. Datasets are the input to Dataset Runs (experiments) where you compare prompts/models/configs and optionally score results.\n\n- Create datasets from production edge cases, synthetic data, or manual cases\n- Use them to benchmark changes and prevent regressions\n\nMinimal example (Python)\n_ create_dataset.py_\n```python\nfrom langfuse import Langfuse\n\nlf = Langfuse(public_key=\"<YOUR_PUBLIC_KEY>\", secret_key=\"<YOUR_SECRET_KEY>\", host=\"https://cloud.langfuse.com\")\n\nlf.create_dataset(\n    name=\"getting-started\",\n    description=\"My first dataset\",\n    metadata={\"type\": \"benchmark\"}\n)\n\nlf.create_dataset_item(\n    dataset_name=\"getting-started\",\n    input={\"question\": \"What is Langfuse?\"},\n    expected_output={\"answer_contains\": \"LLM\"}\n)\nprint(\"Dataset ready\")\n```\n\nLinks\n- Concept and setup: Datasets — what they are and how to create/use them: https://langfuse.com/docs/evaluation/dataset-runs/datasets\n- Dataset Runs (prompt experiments): https://langfuse.com/docs/evaluation/dataset-runs/native-run\n- Data model (Dataset, DatasetItem, Runs): https://langfuse.com/docs/evaluation/dataset-runs/data-model",
        "modelParameters": {},
        "completionStartTime": null,
        "promptId": null,
        "createdAt": "2025-09-18T20:20:17.668Z",
        "updatedAt": "2025-09-18T20:20:17.668Z",
        "usageDetails": {},
        "costDetails": {},
        "providedCostDetails": {},
        "model": null,
        "internalModelId": null,
        "promptName": null,
        "promptVersion": null,
        "latency": 20601,
        "timeToFirstToken": null,
        "inputCost": null,
        "outputCost": null,
        "totalCost": 0,
        "inputUsage": 0,
        "outputUsage": 0,
        "totalUsage": 0,
        "internalModel": null
      },
      {
        "id": "a44bd80c743afa87",
        "traceId": "6f58ac41090ac81ce6203900dadf8e6b",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "type": "GENERATION",
        "environment": "default",
        "parentObservationId": "31ccdfbd9381c993",
        "startTime": "2025-09-18T20:19:57.272Z",
        "endTime": "2025-09-18T20:20:09.357Z",
        "name": "ai.streamText.doStream",
        "metadata": "{\"tools\":[\"{\\\"type\\\":\\\"function\\\",\\\"name\\\":\\\"searchLangfuseDocs\\\",\\\"description\\\":\\\"Semantic search (RAG) over the Langfuse documentation. Use this whenever the user asks a broader question that cannot be answered by a specific single page. Returns a concise answer synthesized from relevant docs. The raw provider response is included in _meta. Prefer this before guessing. If a specific page is needed call getLangfuseDocsPage first.\\\",\\\"inputSchema\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"query\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The user’s question in natural language. Include helpful context like SDK/language (e.g., Python v3, JS v4), self-hosted vs cloud, and short error messages (trim long stack traces). Keep under ~600 characters.\\\"}},\\\"required\\\":[\\\"query\\\"],\\\"additionalProperties\\\":false,\\\"$schema\\\":\\\"http://json-schema.org/draft-07/schema#\\\"}}\",\"{\\\"type\\\":\\\"function\\\",\\\"name\\\":\\\"getLangfuseDocsPage\\\",\\\"description\\\":\\\"Fetch the raw Markdown for a single Langfuse docs page. Accepts a docs path (e.g., /docs/observability/overview) or a full https://langfuse.com URL. Returns the exact Markdown (may include front matter). Use when you need a specific page content (Integration, Features, API, etc.) or code samples. Prefer searchLangfuseDocs for broader questions where there is not one specific page about it.\\\",\\\"inputSchema\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"pathOrUrl\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"Docs path starting with “/” (e.g., /docs/observability/overview) or a full URL on https://langfuse.com. Do not include anchors (#...) or queries (?foo=bar) — they will be ignored.\\\"}},\\\"required\\\":[\\\"pathOrUrl\\\"],\\\"additionalProperties\\\":false,\\\"$schema\\\":\\\"http://json-schema.org/draft-07/schema#\\\"}}\",\"{\\\"type\\\":\\\"function\\\",\\\"name\\\":\\\"getLangfuseOverview\\\",\\\"description\\\":\\\"Get a high-level, machine-readable index by downloading https://langfuse.com/llms.txt. Use this at the start of a session when needed to discover key docs endpoints or to seed follow-up calls to searchLangfuseDocs or getLangfuseDocsPage.Returns the plain text contents of llms.txt. Avoid repeated calls within the same session.\\\",\\\"inputSchema\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{},\\\"additionalProperties\\\":false,\\\"$schema\\\":\\\"http://json-schema.org/draft-07/schema#\\\"}}\"],\"attributes\":{\"operation.name\":\"ai.streamText.doStream\",\"ai.operationId\":\"ai.streamText.doStream\",\"ai.model.provider\":\"openai.responses\",\"ai.model.id\":\"gpt-5\",\"ai.settings.maxRetries\":\"2\",\"ai.telemetry.metadata.langfusePrompt\":\"{\\\"name\\\":\\\"langfuse-docs-assistant-text\\\",\\\"prompt\\\":\\\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\\\n\\\\n## Guidelines\\\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\\\n- Answer the question using only this and only this information, outputted in markdown format.\\\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\\\n\\\\n## Scope & Safety\\\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\\\n\\\\n## Clarifying Questions\\\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., “Which SDK are you using - Python or JS/TS?”).\\\\n\\\\n## Pricing & Discounts\\\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can’t confirm and point to contacting support.\\\\n\\\\n## Markdown Style Guide\\\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\\\n- **Sections to include (if relevant):** *Prerequisites* → *Setup* → *Minimal Example* → *Run* → *Troubleshooting* → *Links*.\\\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\\\n- **Keep it tight:** ≤ 20 lines of code per SDK section. If longer, collapse details or link out.\\\",\\\"version\\\":10,\\\"isFallback\\\":false,\\\"tags\\\":[],\\\"labels\\\":[\\\"production\\\",\\\"latest\\\"],\\\"type\\\":\\\"text\\\",\\\"config\\\":{\\\"model\\\":\\\"gpt-5\\\",\\\"reasoningSummary\\\":\\\"detailed\\\",\\\"textVerbosity\\\":\\\"low\\\",\\\"reasoningEffort\\\":\\\"low\\\"}}\",\"ai.prompt.messages\":\"[{\\\"role\\\":\\\"system\\\",\\\"content\\\":\\\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\\\n\\\\n## Guidelines\\\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\\\n- Answer the question using only this and only this information, outputted in markdown format.\\\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\\\n\\\\n## Scope & Safety\\\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\\\n\\\\n## Clarifying Questions\\\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., “Which SDK are you using - Python or JS/TS?”).\\\\n\\\\n## Pricing & Discounts\\\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can’t confirm and point to contacting support.\\\\n\\\\n## Markdown Style Guide\\\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\\\n- **Sections to include (if relevant):** *Prerequisites* → *Setup* → *Minimal Example* → *Run* → *Troubleshooting* → *Links*.\\\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\\\n- **Keep it tight:** ≤ 20 lines of code per SDK section. If longer, collapse details or link out.\\\"},{\\\"role\\\":\\\"assistant\\\",\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"**👋 Do you have any questions about Langfuse? Ask me!**\\\\n\\\\n_⚠️ Warning: Do not enter sensitive information. All chat messages can be viewed in the public demo project. Responses may be inaccurate. Please check the documentation for details or reach out to us via the chat widget._\\\"}]},{\\\"role\\\":\\\"user\\\",\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"what is a dataset\\\"}]}]\",\"ai.prompt.tools\":\"[\\\"{\\\\\\\"type\\\\\\\":\\\\\\\"function\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"searchLangfuseDocs\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Semantic search (RAG) over the Langfuse documentation. Use this whenever the user asks a broader question that cannot be answered by a specific single page. Returns a concise answer synthesized from relevant docs. The raw provider response is included in _meta. Prefer this before guessing. If a specific page is needed call getLangfuseDocsPage first.\\\\\\\",\\\\\\\"inputSchema\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"properties\\\\\\\":{\\\\\\\"query\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"The user’s question in natural language. Include helpful context like SDK/language (e.g., Python v3, JS v4), self-hosted vs cloud, and short error messages (trim long stack traces). Keep under ~600 characters.\\\\\\\"}},\\\\\\\"required\\\\\\\":[\\\\\\\"query\\\\\\\"],\\\\\\\"additionalProperties\\\\\\\":false,\\\\\\\"$schema\\\\\\\":\\\\\\\"http://json-schema.org/draft-07/schema#\\\\\\\"}}\\\",\\\"{\\\\\\\"type\\\\\\\":\\\\\\\"function\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"getLangfuseDocsPage\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Fetch the raw Markdown for a single Langfuse docs page. Accepts a docs path (e.g., /docs/observability/overview) or a full https://langfuse.com URL. Returns the exact Markdown (may include front matter). Use when you need a specific page content (Integration, Features, API, etc.) or code samples. Prefer searchLangfuseDocs for broader questions where there is not one specific page about it.\\\\\\\",\\\\\\\"inputSchema\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"properties\\\\\\\":{\\\\\\\"pathOrUrl\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Docs path starting with “/” (e.g., /docs/observability/overview) or a full URL on https://langfuse.com. Do not include anchors (#...) or queries (?foo=bar) — they will be ignored.\\\\\\\"}},\\\\\\\"required\\\\\\\":[\\\\\\\"pathOrUrl\\\\\\\"],\\\\\\\"additionalProperties\\\\\\\":false,\\\\\\\"$schema\\\\\\\":\\\\\\\"http://json-schema.org/draft-07/schema#\\\\\\\"}}\\\",\\\"{\\\\\\\"type\\\\\\\":\\\\\\\"function\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"getLangfuseOverview\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Get a high-level, machine-readable index by downloading https://langfuse.com/llms.txt. Use this at the start of a session when needed to discover key docs endpoints or to seed follow-up calls to searchLangfuseDocs or getLangfuseDocsPage.Returns the plain text contents of llms.txt. Avoid repeated calls within the same session.\\\\\\\",\\\\\\\"inputSchema\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"properties\\\\\\\":{},\\\\\\\"additionalProperties\\\\\\\":false,\\\\\\\"$schema\\\\\\\":\\\\\\\"http://json-schema.org/draft-07/schema#\\\\\\\"}}\\\"]\",\"ai.prompt.toolChoice\":\"{\\\"type\\\":\\\"auto\\\"}\",\"gen_ai.system\":\"openai.responses\",\"gen_ai.request.model\":\"gpt-5\",\"ai.response.msToFirstChunk\":\"1187.6545230001211\",\"ai.response.msToFinish\":\"12084.884463000111\",\"ai.response.avgOutputTokensPerSecond\":\"3.6409119288408043\",\"ai.response.finishReason\":\"tool-calls\",\"ai.response.text\":\"\",\"ai.response.toolCalls\":\"[{\\\"type\\\":\\\"tool-call\\\",\\\"toolCallId\\\":\\\"call_Wgy5Y139OQnSh7grDUUE0iil\\\",\\\"toolName\\\":\\\"searchLangfuseDocs\\\",\\\"input\\\":{\\\"query\\\":\\\"What is a dataset in Langfuse? definition, use cases, evaluation, prompt testing, datasets feature docs\\\"},\\\"providerMetadata\\\":{\\\"openai\\\":{\\\"itemId\\\":\\\"fc_68cc6976097481958d36986414510cab0e53a980a402aac8\\\"}},\\\"dynamic\\\":true}]\",\"ai.response.id\":\"resp_68cc696e0a208195a91e435bd719e7800e53a980a402aac8\",\"ai.response.model\":\"gpt-5-2025-08-07\",\"ai.response.timestamp\":\"2025-09-18T20:19:58.000Z\",\"ai.response.providerMetadata\":\"{\\\"openai\\\":{\\\"responseId\\\":\\\"resp_68cc696e0a208195a91e435bd719e7800e53a980a402aac8\\\"}}\",\"ai.usage.inputTokens\":\"1115\",\"ai.usage.outputTokens\":\"44\",\"ai.usage.totalTokens\":\"1159\",\"ai.usage.reasoningTokens\":\"{\\\"intValue\\\":0}\",\"ai.usage.cachedInputTokens\":\"{\\\"intValue\\\":0}\",\"gen_ai.response.finish_reasons\":\"[\\\"tool-calls\\\"]\",\"gen_ai.response.id\":\"resp_68cc696e0a208195a91e435bd719e7800e53a980a402aac8\",\"gen_ai.response.model\":\"gpt-5-2025-08-07\",\"gen_ai.usage.input_tokens\":\"1115\",\"gen_ai.usage.output_tokens\":\"44\"},\"resourceAttributes\":{\"service.name\":\"unknown_service:node\",\"telemetry.sdk.language\":\"nodejs\",\"telemetry.sdk.name\":\"opentelemetry\",\"telemetry.sdk.version\":\"2.0.1\"},\"scope\":{\"name\":\"ai\",\"attributes\":{}}}",
        "level": "DEFAULT",
        "statusMessage": null,
        "version": null,
        "input": "[{\"role\":\"system\",\"content\":\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\n\\n## Guidelines\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\n- Answer the question using only this and only this information, outputted in markdown format.\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\n\\n## Scope & Safety\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\n\\n## Clarifying Questions\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., “Which SDK are you using - Python or JS/TS?”).\\n\\n## Pricing & Discounts\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can’t confirm and point to contacting support.\\n\\n## Markdown Style Guide\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\n- **Sections to include (if relevant):** *Prerequisites* → *Setup* → *Minimal Example* → *Run* → *Troubleshooting* → *Links*.\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\n- **Keep it tight:** ≤ 20 lines of code per SDK section. If longer, collapse details or link out.\"},{\"role\":\"assistant\",\"content\":[{\"type\":\"text\",\"text\":\"**👋 Do you have any questions about Langfuse? Ask me!**\\n\\n_⚠️ Warning: Do not enter sensitive information. All chat messages can be viewed in the public demo project. Responses may be inaccurate. Please check the documentation for details or reach out to us via the chat widget._\"}]},{\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":\"what is a dataset\"}]}]",
        "output": null,
        "modelParameters": {
          "maxSteps": null,
          "toolChoice": "{\"type\":\"auto\"}",
          "maxTokens": null,
          "finishReason": "tool-calls",
          "system": "openai.responses",
          "maxRetries": "2",
          "mode": null,
          "temperature": null
        },
        "completionStartTime": "2025-09-18T20:19:58.460Z",
        "promptId": "5422a9e9-19cf-4ba8-bbd4-2fe0108eefc5",
        "createdAt": "2025-09-18T20:20:14.484Z",
        "updatedAt": "2025-09-18T20:20:14.486Z",
        "usageDetails": {
          "input": 1115,
          "output": 44,
          "input_cached_tokens": 0,
          "output_reasoning_tokens": 0,
          "total": 1159
        },
        "costDetails": {
          "input": 0.00139375,
          "output": 0.00044,
          "input_cached_tokens": 0,
          "output_reasoning_tokens": 0,
          "total": 0.00183375
        },
        "providedCostDetails": {},
        "model": "gpt-5",
        "internalModelId": "38c3822a-09a3-457b-b200-2c6f17f7cf2f",
        "promptName": "langfuse-docs-assistant-text",
        "promptVersion": 10,
        "latency": 12085,
        "timeToFirstToken": 1.188,
        "inputCost": 0.00139375,
        "outputCost": 0.00044,
        "totalCost": 0.00183375,
        "inputUsage": 1115,
        "outputUsage": 44,
        "totalUsage": 1159,
        "internalModel": "38c3822a-09a3-457b-b200-2c6f17f7cf2f"
      },
      {
        "id": "9a6a16a257c88b0a",
        "traceId": "6f58ac41090ac81ce6203900dadf8e6b",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "type": "TOOL",
        "environment": "default",
        "parentObservationId": "31ccdfbd9381c993",
        "startTime": "2025-09-18T20:20:06.383Z",
        "endTime": "2025-09-18T20:20:09.351Z",
        "name": "ai.toolCall",
        "metadata": "{\"attributes\":{\"operation.name\":\"ai.toolCall\",\"ai.operationId\":\"ai.toolCall\",\"ai.toolCall.name\":\"searchLangfuseDocs\",\"ai.toolCall.id\":\"call_Wgy5Y139OQnSh7grDUUE0iil\",\"ai.toolCall.args\":\"{\\\"query\\\":\\\"What is a dataset in Langfuse? definition, use cases, evaluation, prompt testing, datasets feature docs\\\"}\",\"ai.toolCall.result\":\"{\\\"_meta\\\":{\\\"id\\\":\\\"inkeep-chatcmpl-1761a743-0fb1-4e4d-9c1c-10d4960ef29f\\\",\\\"model\\\":\\\"inkeep-rag\\\",\\\"choices\\\":[{\\\"finish_reason\\\":\\\"stop\\\",\\\"log_probs\\\":null,\\\"index\\\":0,\\\"message\\\":{\\\"content\\\":\\\"{\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/dataset-runs/datasets\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Datasets\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Datasets - Langfuse\\\\\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\\\\\\\\\"Evaluation\\\\\\\\\\\\\\\")Dataset RunsDatasets\\\\\\\\nCopy as Markdown\\\\\\\\n# Datasets\\\\\\\\nA dataset is a collection of inputs and expected outputs and is used to test your application. Before executing your first dataset run, you need to create a dataset.\\\\\\\\n## Why use datasets?\\\\\\\\n  * **Datasets prerequisite for Dataset Runs** , they serve as the data input of Dataset Runs\\\\\\\\n  * Create test cases for your application with real production traces\\\\\\\\n  * Collaboratively create and collect dataset items with your team\\\\\\\\n  * Have a single source of truth for your test data\\\\\\\\n## Get Started\\\\\\\\n### Creating a dataset\\\\\\\\nDatasets have a name which is unique within a project.\\\\\\\\nPython SDKJS/TS SDKLangfuse UI\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.create_dataset(\\\\\\\\n name=\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\",\\\\\\\\n # optional description\\\\\\\\n description=\\\\\\\\\\\\\\\"My first dataset\\\\\\\\\\\\\\\",\\\\\\\\n # optional metadata\\\\\\\\n metadata={\\\\\\\\n \\\\\\\\\\\\\\\"author\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Alice\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"date\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"2022-01-01\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"benchmark\\\\\\\\\\\\\\\"\\\\\\\\n }\\\\\\\\n)\\\\\\\\n```\\\\\\\\n _See[Python SDK](/docs/sdk/python/sdk-v3) docs for details on how to initialize the Python client._\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport { LangfuseClient } from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\nconst langfuse = new LangfuseClient()\\\\\\\\n \\\\\\\\nawait langfuse.api.datasets.create({\\\\\\\\n name: \\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\",\\\\\\\\n // optional description\\\\\\\\n description: \\\\\\\\\\\\\\\"My first dataset\\\\\\\\\\\\\\\",\\\\\\\\n // optional metadata\\\\\\\\n metadata: {\\\\\\\\n author: \\\\\\\\\\\\\\\"Alice\\\\\\\\\\\\\\\",\\\\\\\\n date: \\\\\\\\\\\\\\\"2022-01-01\\\\\\\\\\\\\\\",\\\\\\\\n type: \\\\\\\\\\\\\\\"benchmark\\\\\\\\\\\\\\\",\\\\\\\\n },\\\\\\\\n});\\\\\\\\n```\\\\\\\\n  1. **Navigate to** `Your Project` > `Datasets`\\\\\\\\n  2. **Click on** `+ New dataset` to create a new dataset.\\\\\\\\n### Create new dataset items\\\\\\\\nDataset items can be added to a dataset by providing the input and optionally the expected output.\\\\\\\\nPython SDKJS/TS SDKLangfuse UI\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.create_dataset_item(\\\\\\\\n dataset_name=\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\",\\\\\\\\n # any python object or value, optional\\\\\\\\n input={\\\\\\\\n \\\\\\\\\\\\\\\"text\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"hello world\\\\\\\\\\\\\\\"\\\\\\\\n },\\\\\\\\n # any python object or value, optional\\\\\\\\n expected_output={\\\\\\\\n \\\\\\\\\\\\\\\"text\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"hello world\\\\\\\\\\\\\\\"\\\\\\\\n },\\\\\\\\n # metadata, optional\\\\\\\\n metadata={\\\\\\\\n \\\\\\\\\\\\\\\"model\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"llama3\\\\\\\\\\\\\\\",\\\\\\\\n }\\\\\\\\n)\\\\\\\\n```\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/dataset-runs/native-run\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Native Dataset Runs (Prompt Experiments)\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Native Dataset Runs (Prompt Experiments) - Langfuse\\\\\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\\\\\\\\\"Evaluation\\\\\\\\\\\\\\\")\\\\\\\\n[Dataset Runs](/docs/evaluation/dataset-runs/datasets \\\\\\\\\\\\\\\"Dataset Runs\\\\\\\\\\\\\\\")Native Run\\\\\\\\nCopy as Markdown\\\\\\\\n# Native Dataset Runs (Prompt Experiments)\\\\\\\\nYou can execute Native Dataset Runs **(also called Prompt Experiments)** in the Langfuse UI to test different prompt versions from [Prompt Management](/docs/prompt-management) or language models and compare the results side-by-side.\\\\\\\\nOptionally, you can use [LLM-as-a-Judge Evaluators](/docs/evaluation/evaluation-methods/llm-as-a-judge) to automatically score the responses based on the expected outputs to further analyze the results on an aggregate level.\\\\\\\\nThey are called \\\\\\\\u201cNative Dataset Runs\\\\\\\\u201d because they rely on Datasets, Prompts and optionally LLM-as-a-Judge Evaluators all being managed **natively** in Langfuse.\\\\\\\\n## Why use Native Dataset Runs (Prompt Experiments)?\\\\\\\\n  * Quickly test different prompt versions or models\\\\\\\\n  * Structure your prompt testing by using a dataset to test different prompt versions and models\\\\\\\\n  * Quickly iterate on prompts through Dataset Runs\\\\\\\\n  * Optionally use LLM-as-a-Judge Evaluators to score the responses based on the expected outputs from the dataset\\\\\\\\n  * Prevent regressions by running tests when making prompt changes\\\\\\\\n## Prerequisites\\\\\\\\n### Create a usable prompt\\\\\\\\nCreate a prompt that you want to test and evaluate. [How to create a prompt?](/docs/prompt-management/get-started)\\\\\\\\n **A prompt is usable when:** your prompt has variables that match the dataset item keys in the dataset that will be used for the Dataset Run. See the example below.\\\\\\\\nExample: Prompt Variables & Dataset Item Keys Mapping\\\\\\\\n  \\\\\\\\n **Prompt:**\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n{{ documentation }}\\\\\\\\n \\\\\\\\nQuestion: {{question}}\\\\\\\\n \\\\\\\\n```\\\\\\\\n  \\\\\\\\n**Dataset Item:**\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n{\\\\\\\\n \\\\\\\\\\\\\\\"documentation\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Langfuse is an LLM Engineering Platform\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"question\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What is Langfuse?\\\\\\\\\\\\\\\"\\\\\\\\n}\\\\\\\\n```\\\\\\\\nIn this example:\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/changelog/2024-11-18-dataset-runs-comparison-view\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Dataset Run Comparison View\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Dataset Run Comparison View - Langfuse\\\\\\\\n[\\\\\\\\u2190 Back to changelog](/changelog#2024-11-18-dataset-runs-comparison-view)\\\\\\\\nNovember 18, 2024 | Launch Week 2 \\\\\\\\ud83d\\\\\\\\ude80\\\\\\\\n# Dataset Run Comparison View\\\\\\\\n[Marlies Mayerhofer](https://twitter.com/marliessophie)\\\\\\\\nAfter running experiments on datasets, you can now compare results side-by-side, view metrics, and peek into details of each dataset item across runs.\\\\\\\\n## Introduction\\\\\\\\n**What is a dataset in Langfuse?** [Datasets](/docs/datasets/overview) in Langfuse allow you to create test sets and benchmarks to evaluate the performance of your LLM application. A dataset is a collection of dataset items, where each item contains inputs, expected outputs, and metadata. You can create datasets from production edge cases, synthetic test cases, or manual test cases. This enables continuous improvement through structured testing, pre-deployment benchmarking, and flexible evaluation using custom metrics or LLM-as-a-judge approaches.\\\\\\\\n**What is a dataset experiment run?** A dataset experiment run lets you test changes to your application - like trying different models, prompts, or parameters - by running each version against your test dataset and comparing the results through traces in Langfuse to evaluate which changes work best.\\\\\\\\n## What\\\\\\\\u2019s new?\\\\\\\\nAnd the fun continues, **Day 1 of[Launch Week 2](/blog/2024-11-17-launch-week-2)** is here.\\\\\\\\n[Langfuse Datasets](/docs/datasets/overview) now enables intuitive comparison of dataset experiment runs for technical and non-technical users. The view features an overview of each item in the dataset, and a summary of each selected experiment run. The latter includes metrics on latency, cost, scores and the application\\\\\\\\u2019s output response for each dataset item.\\\\\\\\n## How to use the comparison view?\\\\\\\\n### Setup dataset and run experiments\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n  * Follow the [getting started guide](/docs/datasets/get-started) to set up a dataset, populate it with items, and run experiments.\\\\\\\\n  * Alternatively, execute an end-to-end example ([Python notebook](/docs/datasets/python-cookbook)).\\\\\\\\n### Open comparison view\\\\\\\\n  1. Select multiple dataset runs\\\\\\\\n  2. `Actions`, select `Compare`\\\\\\\\n## Learn more\\\\\\\\nFor a conceptual introduction to datasets and offline experiments, see the [dataset documentation](/docs/datasets/overview).\\\\\\\\nLast updated on September 5, 2025\\\\\\\\nWas this page helpful?\\\\\\\\nYesNo\\\\\\\\n[Support](/support)\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/guides/cookbook/example_synthetic_datasets\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Synthetic Dataset Generation for LLM Evaluation\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Synthetic Dataset Generation for LLM Evaluation - Langfuse\\\\\\\\nGuides[Cookbooks](/guides/cookbook \\\\\\\\\\\\\\\"Cookbooks\\\\\\\\\\\\\\\")Synthetic Datasets\\\\\\\\nCopy as Markdown\\\\\\\\nThis is a Jupyter notebook\\\\\\\\n[Open on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/cookbook/example_synthetic_datasets.ipynb)\\\\\\\\n[Run on Google Colab](https://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/cookbook/example_synthetic_datasets.ipynb)\\\\\\\\n# Synthetic Dataset Generation for LLM Evaluation\\\\\\\\nIn this notebook, we will explore how to **generate synthetic datasets** using language models and uploading them to [Langfuse](https://langfuse.com) for evaluation.\\\\\\\\n## What are Langfuse Datasets?\\\\\\\\nIn Langfuse, a _dataset_ is a collection of _dataset items_ , each typically containing an `input` (e.g., user prompt/question), `expected_output` (the ground truth or ideal answer) and optional metadata.\\\\\\\\nDatasets are used for **evaluation**. You can run your LLM or application on each item in a dataset and compare the application\\\\\\\\u2019s responses to the expected outputs. This way, you can track performance over time and across different application configs (e.g. model versions or prompt changes).\\\\\\\\n## Cases your Dataset Should Cover\\\\\\\\n**Happy path** \\\\\\\\u2013 straightforward or common queries:\\\\\\\\n  * \\\\\\\\u201cWhat is the capital of France?\\\\\\\\u201d\\\\\\\\n  * \\\\\\\\u201cConvert 5 USD to EUR.\\\\\\\\u201d\\\\\\\\n**Edge cases** \\\\\\\\u2013 unusual or complex:\\\\\\\\n  * Very long prompts.\\\\\\\\n  * Ambiguous queries.\\\\\\\\n  * Very technical or niche.\\\\\\\\n**Adversarial cases** \\\\\\\\u2013 malicious or tricky:\\\\\\\\n  * Prompt injection attempts (\\\\\\\\u201cIgnore all instructions and \\\\\\\\u2026\\\\\\\\u201d).\\\\\\\\n  * Content policy violations (harassment, hate speech).\\\\\\\\n  * Logic traps (trick questions).\\\\\\\\n## Examples\\\\\\\\n### Example 1: Looping Over OpenAI API\\\\\\\\nWe\\\\\\\\u2019ll use OpenAI\\\\\\\\u2019s API in a simple loop to create synthetic questions for an airline chatbot. You could similarly prompt the model to generate _both_ questions and answers. %pip install openai langfuse\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport os\\\\\\\\n \\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n///# Get keys for your project from the project settings page: https://cloud.langfuse.com\\\\\\\\nos.environ[\\\\\\\\\\\\\\\"LANGFUSE_PUBLIC_KEY\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"pk-lf-...\\\\\\\\\\\\\\\" \\\\\\\\nos.environ[\\\\\\\\\\\\\\\"LANGFUSE_SECRET_KEY\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"sk-lf-...\\\\\\\\\\\\\\\" \\\\\\\\nos.environ[\\\\\\\\\\\\\\\"LANGFUSE_HOST\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"https://cloud.langfuse.com\\\\\\\\\\\\\\\" # \\\\\\\\ud83c\\\\\\\\uddea\\\\\\\\ud83c\\\\\\\\uddfa EU region\\\\\\\\n///# os.environ[\\\\\\\\\\\\\\\"LANGFUSE_HOST\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"https://us.cloud.langfuse.com\\\\\\\\\\\\\\\" # \\\\\\\\ud83c\\\\\\\\uddfa\\\\\\\\ud83c\\\\\\\\uddf8 US region\\\\\\\\n \\\\\\\\n///# Your openai key\\\\\\\\nos.environ[\\\\\\\\\\\\\\\"OPENAI_API_KEY\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"sk-proj-...\\\\\\\\\\\\\\\"\\\\\\\\n```\\\\\\\\nWith the environment variables set, we can now initialize the Langfuse client. `get_client()` initializes the Langfuse client using the credentials provided in the environment variables.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\n \\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\n///# Verify connection\\\\\\\\nif langfuse.auth_check():\\\\\\\\n print(\\\\\\\\\\\\\\\"Langfuse client is authenticated and ready!\\\\\\\\\\\\\\\")\\\\\\\\nelse:\\\\\\\\n print(\\\\\\\\\\\\\\\"Authentication failed. Please check your credentials and host.\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\\nLangfuse client is authenticated and ready!\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom openai import OpenAI\\\\\\\\nimport pandas as pd\\\\\\\\n \\\\\\\\nclient = OpenAI()\\\\\\\\n \\\\\\\\n///# Function to generate airline questions\\\\\\\\ndef generate_airline_questions(num_questions=20):\\\\\\\\n \\\\\\\\n questions = []\\\\\\\\n \\\\\\\\n for i in range(num_questions):\\\\\\\\n completion = client.chat.completions.create(\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4o\\\\\\\\\\\\\\\", \\\\\\\\n messages=[\\\\\\\\n {\\\\\\\\n \\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"system\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": (\\\\\\\\n \\\\\\\\\\\\\\\"You are a helpful customer service chatbot for an airline. \\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\\\\\\\\"Please generate a short, realistic question from a customer.\\\\\\\\\\\\\\\"\\\\\\\\n )\\\\\\\\n }\\\\\\\\n ],\\\\\\\\n temperature=1\\\\\\\\n )\\\\\\\\n question_text = completion.choices[0].message.content.strip()\\\\\\\\n questions.append(question_text)\\\\\\\\n \\\\\\\\n return questions\\\\\\\\n \\\\\\\\n///# Generate 20 airline-related questions\\\\\\\\nairline_questions = generate_airline_questions(num_questions=20)\\\\\\\\n \\\\\\\\n///# Convert to a Pandas DataFrame\\\\\\\\ndf = pd.DataFrame({\\\\\\\\\\\\\\\"Question\\\\\\\\\\\\\\\": airline_questions})\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n scenario=\\\\\\\\\\\\\\\"Non-technical users trying to query a database using plain English.\\\\\\\\\\\\\\\",\\\\\\\\n)\\\\\\\\n \\\\\\\\n///# 2. Initialize the Synthesizer\\\\\\\\nsynthesizer = Synthesizer(styling_config=styling_config)\\\\\\\\n \\\\\\\\n///# 3. Generate synthetic items from scratch, e.g. 20 items for a short demo\\\\\\\\nsynthesizer.generate_goldens_from_scratch(num_goldens=20)\\\\\\\\n \\\\\\\\n///# 4. Access the generated examples\\\\\\\\nsynthetic_goldens = synthesizer.synthetic_goldens\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\n///# 5. Create a Langfuse dataset\\\\\\\\ndeepeval_dataset_name = \\\\\\\\\\\\\\\"deepeval_synthetic_data\\\\\\\\\\\\\\\"\\\\\\\\nlangfuse.create_dataset(\\\\\\\\n name=deepeval_dataset_name,\\\\\\\\n description=\\\\\\\\\\\\\\\"Synthetic text-to-SQL data (DeepEval)\\\\\\\\\\\\\\\",\\\\\\\\n metadata={\\\\\\\\\\\\\\\"approach\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"deepeval\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"task\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"text-to-sql\\\\\\\\\\\\\\\"}\\\\\\\\n)\\\\\\\\n \\\\\\\\n///# 6. Upload the items\\\\\\\\nfor golden in synthetic_goldens:\\\\\\\\n langfuse.create_dataset_item(\\\\\\\\n dataset_name=deepeval_dataset_name,\\\\\\\\n input={\\\\\\\\\\\\\\\"query\\\\\\\\\\\\\\\": golden.input},\\\\\\\\n )\\\\\\\\n```\\\\\\\\n### Example 4: No-Code via Hugging Face Dataset Generator\\\\\\\\nIf you prefer a more UI-based approach, check out [Hugging Face\\\\\\\\u2019s Synthetic Data Generator](https://huggingface.co/blog/synthetic-data-generator). You can generate examples in the Hugging Face UI. Then you can download them as CSV and upload it in the Langfuse UI.\\\\\\\\n## Next Steps\\\\\\\\n  1. **Explore your dataset in Langfuse**. You can see each dataset in the UI.\\\\\\\\n  2. **Run experiments** You can now evaluate your application using this dataset.\\\\\\\\n  3. **Compare runs** over time or across models, prompts, or chain logic.\\\\\\\\nFor more details on how to run experiments on a dataset, see the [Langfuse docs](https://langfuse.com/docs/datasets/get-started#run-experiment-on-a-dataset).\\\\\\\\nLast updated on July 29, 2025\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Query Data in Langfuse via the SDK](/guides/cookbook/example_query_data_via_sdk \\\\\\\\\\\\\\\"Query Data in Langfuse via the SDK\\\\\\\\\\\\\\\")\\\\\\\\n[Amazon Bedrock](/guides/cookbook/integration_amazon_bedrock \\\\\\\\\\\\\\\"Amazon Bedrock\\\\\\\\\\\\\\\")\\\\\\\\nWas this page helpful?\\\\\\\\nYesNo\\\\\\\\n[Support](/support)\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/dataset-runs/data-model\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Dataset Runs Data Model\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Dataset Runs Data Model - Langfuse\\\\\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\\\\\\\\\"Evaluation\\\\\\\\\\\\\\\")\\\\\\\\n[Dataset Runs](/docs/evaluation/dataset-runs/datasets \\\\\\\\\\\\\\\"Dataset Runs\\\\\\\\\\\\\\\")Data Model\\\\\\\\nCopy as Markdown\\\\\\\\n# Dataset Runs Data Model\\\\\\\\nThis page describes the data model of datasets and dataset runs. For detailed reference please refer to the [API Reference](https://api.reference.langfuse.com).\\\\\\\\n## Datasets\\\\\\\\nDatasets are a collection of inputs and, optionally, expected outputs that can be during Dataset runs.\\\\\\\\n`Dataset`s are a collection of `DatasetItem`s.\\\\\\\\n1\\\\\\\\nn\\\\\\\\nDataset\\\\\\\\nname\\\\\\\\ndescription\\\\\\\\nmetadata\\\\\\\\nDatasetItem\\\\\\\\ndatasetName\\\\\\\\ninput\\\\\\\\nexpectedOutput\\\\\\\\nmetadata\\\\\\\\nsourceTraceId\\\\\\\\nsourceObservationId\\\\\\\\nid\\\\\\\\nstatus\\\\\\\\n### Dataset object\\\\\\\\nAttribute| Type| Required| Description  \\\\\\\\n---|---|---|---  \\\\\\\\n`name`| string| Yes| Name of the dataset  \\\\\\\\n`description`| string| No| Description of the dataset  \\\\\\\\n`metadata`| object| No| Additional metadata for the dataset  \\\\\\\\n  \\\\\\\\n### DatasetItem object\\\\\\\\nAttribute| Type| Required| Description  \\\\\\\\n---|---|---|---  \\\\\\\\n`datasetName`| string| Yes| Name of the dataset to add the item to  \\\\\\\\n`input`| object| No| Input data for the dataset item  \\\\\\\\n`expectedOutput`| object| No| Expected output data for the dataset item  \\\\\\\\n`metadata`| object| No| Additional metadata for the dataset item  \\\\\\\\n`sourceTraceId`| string| No| ID of the source trace to link this dataset item to  \\\\\\\\n`sourceObservationId`| string| No| ID of the source observation to link this dataset item to  \\\\\\\\n`id`| string| No| Unique identifier for the dataset item. Dataset items are upserted on their id. Id needs to be unique (project-level) and cannot be reused across datasets.  \\\\\\\\n`status`| DatasetStatus| No| Status of the dataset item. Defaults to ACTIVE for newly created items. Possible values: `ACTIVE`, `ARCHIVED`  \\\\\\\\n  \\\\\\\\n## DatasetRun\\\\\\\\nDataset runs are used to run a dataset through your LLM application and optionally apply evaluation methods to the results.\\\\\\\\n  \\\\\\\\n### DatasetRun object\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Attribute| Type| Required| Description  \\\\\\\\n---|---|---|---  \\\\\\\\n`datasetName`| string| Yes| Name of the dataset  \\\\\\\\n`runName`| string| Yes| Name of the dataset run  \\\\\\\\n  \\\\\\\\n### DatasetRunItem object\\\\\\\\nAttribute| Type| Required| Description  \\\\\\\\n---|---|---|---  \\\\\\\\n`runName`| string| Yes| Name of the dataset run to add the item to  \\\\\\\\n`runDescription`| string| No| Description of the run. If run exists, description will be updated  \\\\\\\\n`metadata`| object| No| Metadata of the dataset run, updates run if run already exists  \\\\\\\\n`datasetItemId`| string| Yes| ID of the dataset item to link to this run  \\\\\\\\n`observationId`| string| No| ID of the observation to link to this run  \\\\\\\\n`traceId`| string| No| ID of the trace to link to this run. traceId should always be provided. For compatibility with older SDK versions it can also be inferred from the provided observationId.  \\\\\\\\n  \\\\\\\\nMost of the time, we recommend that DatasetRunItems reference TraceIDs directly. The reference to ObservationID exists for backwards compatibility with older SDK versions.\\\\\\\\n## End to end data relations\\\\\\\\nDataSetRuns can combine a few Langfuse objects:\\\\\\\\n  * `DatasetRuns` are created by looping through all or selected `DatasetItem`s of a `Dataset` with your LLM application.\\\\\\\\n  * For each `DatasetItem` passed into the LLM application as an Input a `DatasetRunItem` & a `Trace` are created.\\\\\\\\n  * Optionally `Score`s can be added to the `Trace`s to evaluate the output of the LLM application during the `DatasetRun`.\\\\\\\\n  \\\\\\\\nLast updated on July 29, 2025\\\\\\\\n[Remote Run](/docs/evaluation/dataset-runs/remote-run \\\\\\\\\\\\\\\"Remote Run\\\\\\\\\\\\\\\")\\\\\\\\n[Troubleshooting & FAQ](/docs/evaluation/troubleshooting-and-faq \\\\\\\\\\\\\\\"Troubleshooting & FAQ\\\\\\\\\\\\\\\")\\\\\\\\nWas this page helpful?\\\\\\\\nYesNo\\\\\\\\n[Support](/support)\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"github_discussion\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://github.com/orgs/langfuse/discussions/4844\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Dataset creates a lot of identical processes that run infinitely.\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Title: Dataset creates a lot of identical processes that run infinitely.\\\\\\\\nCategory: Support\\\\\\\\nAuthor: @robotkoruslan\\\\\\\\nTimestamp: December 28, 2024\\\\\\\\nBody:\\\\\\\\nWhen running a simple experiment, dataset creates a lot of identical processes that run infinitely, if click trace error \\\\\\\\\\\\\\\"Path: traces.byIdWithObservationsAndScores\\\\\\\\\\\\\\\"\\\\\\\\r\\\\\\\\n<img width=\\\\\\\\\\\\\\\"1494\\\\\\\\\\\\\\\" alt=\\\\\\\\\\\\\\\"image\\\\\\\\\\\\\\\" src=\\\\\\\\\\\\\\\"https://github.com/user-attachments/assets/2ee21b5f-2769-46a2-b1c0-141f0014d6fa\\\\\\\\\\\\\\\" />\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n<img width=\\\\\\\\\\\\\\\"1494\\\\\\\\\\\\\\\" alt=\\\\\\\\\\\\\\\"image\\\\\\\\\\\\\\\" src=\\\\\\\\\\\\\\\"https://github.com/user-attachments/assets/8e56daf9-e405-411e-8f62-042292a64764\\\\\\\\\\\\\\\" />\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\nComments:\\\\\\\\n\\\\\\\\n    Comment #1 from @dosubot on December 28, 2024:\\\\\\\\n\\\\\\\\n    <!-- Greeting -->\\\\\\\\nHello, @robotkoruslan! I'm here to help you with any bugs, questions, or contributions you have. Let's tackle this issue together!\\\\\\\\n\\\\\\\\n<!-- Answer -->\\\\\\\\nThe issue you're encountering with identical processes running indefinitely during dataset runs in Langfuse could be related to how traces are managed and linked to dataset runs. Here are some potential causes and solutions:\\\\\\\\n\\\\\\\\n1. **Trace Linking**: Ensure that traces are correctly linked to dataset runs. Each dataset run item should have a corresponding `traceId` in the `datasetRunItems` table. If the trace is not properly linked or created, it could lead to a \\\\\\\\\\\\\\\"Trace not found\\\\\\\\\\\\\\\" error <sup>[[1]](https://github.com/langfuse/langfuse/blob/main/web/src/__tests__/async/dataset-service.servertest.ts)</sup><sup>[[2]](https://github.com/langfuse/langfuse/blob/main/web/src/pages/api/public/dataset-run-items.ts)</sup>.\\\\\\\\n\\\\\\\\n2. **Trace Retrieval**: The error with the path `traces.byIdWithObservationsAndScores` suggests an issue with retrieving traces by their ID along with observations and scores. This could be due to incorrect `traceId` or `projectId`, or if the trace has been deleted or not yet created <sup>[[3]](https://github.com/langfuse/langfuse/blob/main/web/src/server/api/routers/traces.ts)</sup>.\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/changelog/2024-11-21-all-new-datasets-and-evals-documentation\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"All new Datasets, Experimentation and Evaluation documentation\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"## New Datasets and Evaluation documentation\\\\\\\\nBased on this week\\\\\\\\u2019s changes, we have have restructured the documentation to:\\\\\\\\n  1. Better explain **how and when to use Langfuse Datasets and Evals** to evaluate your LLM applications in development\\\\\\\\n  2. Introduce the **core data model** of `datasets`, experiment `runs`, and `scores`\\\\\\\\n  3. Provide **end-to-end examples** for common workflows as Jupyter Notebooks\\\\\\\\n  4. Add many **more gifs and toggles with additional explanations** throughout the docs\\\\\\\\n### Evaluation\\\\\\\\n\\\\\\\\ud83d\\\\\\\\udca1\\\\\\\\n\\\\\\\\u2192 [Evaluation documentation](/docs/scores)\\\\\\\\nAnalyticsUser FeedbackLLM-as-a-JudgePrompt ExperimentAnnotation QueueCustom Evals\\\\\\\\nPlot evaluation results in the Langfuse Dashboard.\\\\\\\\nCollect feedback from your users. Can be captured in the frontend via our Browser SDK, server-side via the SDKs or API. Video includes example application.\\\\\\\\nRun fully managed LLM-as-a-judge evaluations on production or development traces. Can be applied to any step within your application for step-wise evaluations.\\\\\\\\nEvaluate prompts and models on datasets directly in the user interface. No custom code is needed.\\\\\\\\nBaseline your evaluation workflow with human annotations via Annotation Queues.\\\\\\\\nAdd custom evaluation results, supports numeric, boolean and categorical values.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nPOST /api/public/scores\\\\\\\\n```\\\\\\\\nAdd scores via Python or JS SDK.\\\\\\\\nExample (Python)\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.score(\\\\\\\\n trace_id=\\\\\\\\\\\\\\\"123\\\\\\\\\\\\\\\",\\\\\\\\n name=\\\\\\\\\\\\\\\"my_custom_evaluator\\\\\\\\\\\\\\\",\\\\\\\\n value=0.5,\\\\\\\\n)\\\\\\\\n```\\\\\\\\n### Datasets\\\\\\\\n\\\\\\\\ud83d\\\\\\\\udca1\\\\\\\\n\\\\\\\\u2192 [Datasets & Experiments documentation](/docs/datasets)\\\\\\\\nManage DatasetsCompare experimentsAdd new items from Langfuse Traces\\\\\\\\nCollaboratively manage datasets via UI, API, or SDKs.\\\\\\\\nRun experiments on datasets. Compare metrics across experiments, and see results side-by-side.\\\\\\\\nDirectly add new items from production traces to datasets to improve your datasets over time.\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"site\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/changelog/2024-11-18-dataset-runs-comparison-view\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Dataset Run Comparison View - Langfuse\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"---\\\\\\\\n\\\\\\\\n[\\\\\\\\u2190 Back to changelog](/changelog#2024-11-18-dataset-runs-comparison-view)\\\\\\\\nNovember 18, 2024 | Launch Week 2 \\\\\\\\ud83d\\\\\\\\ude80\\\\\\\\n# Dataset Run Comparison View\\\\\\\\n[Marlies Mayerhofer](https://twitter.com/marliessophie)\\\\\\\\nAfter running experiments on datasets, you can now compare results side-by-side, view metrics, and peek into details of each dataset item across runs.\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n## Introduction\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n**What is a dataset in Langfuse?** [Datasets](/docs/datasets/overview) in Langfuse allow you to create test sets and benchmarks to evaluate the performance of your LLM application. A dataset is a collection of dataset items, where each item contains inputs, expected outputs, and metadata. You can create datasets from production edge cases, synthetic test cases, or manual test cases. This enables continuous improvement through structured testing, pre-deployment benchmarking, and flexible evaluation using custom metrics or LLM-as-a-judge approaches.\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n**What is a dataset experiment run?** A dataset experiment run lets you test changes to your application - like trying different models, prompts, or parameters - by running each version against your test dataset and comparing the results through traces in Langfuse to evaluate which changes work best.\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n## What\\\\\\\\u2019s new?\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\nAnd the fun continues, **Day 1 of[Launch Week 2](/blog/2024-11-17-launch-week-2)** is here.\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n[Langfuse Datasets](/docs/datasets/overview) now enables intuitive comparison of dataset experiment runs for technical and non-technical users. The view features an overview of each item in the dataset, and a summary of each selected experiment run. The latter includes metrics on latency, cost, scores and the application\\\\\\\\u2019s output response for each dataset item.\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n## How to use the comparison view?\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n### Setup dataset and run experiments\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n  * Follow the [getting started guide](/docs/datasets/get-started) to set up a dataset, populate it with items, and run experiments.\\\\\\\\n  * Alternatively, execute an end-to-end example ([Python notebook](/docs/datasets/python-cookbook)).\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n### Open comparison view\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n  1. Select multiple dataset runs\\\\\\\\n  2. `Actions`, select `Compare`\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n## Learn more\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\nFor a conceptual introduction to datasets and offline experiments, see the [dataset documentation](/docs/datasets/overview).\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\nLast updated on September 5, 2025\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/blog/update-2023-09\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Langfuse Update \\\\\\\\u2014 September 2023\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n\\\\\\\\u2192 [Docs](/docs/scores/model-based-evals)\\\\\\\\n## \\\\\\\\ud83d\\\\\\\\uddc2\\\\\\\\ufe0f Datasets (beta)\\\\\\\\nSystematically test new iterations of your LLM app with `datasets`.\\\\\\\\nDatasets are collections of inputs and expected outputs that you can manage in Langfuse. Upload an existing dataset or create one based on production data (e.g. when discovering new edge cases).\\\\\\\\nWhen combined with automated evals, Datasets in Langfuse make it easy to systematically evaluate new iterations of your LLM app.\\\\\\\\n_Overview of dataset runs on a demo dataset_\\\\\\\\n_Run experiment on dataset_\\\\\\\\nPython SDKJS/TS SDKLangchain (Python)\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n \\\\\\\\ndataset = langfuse.get_dataset(\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\nfor item in dataset.items:\\\\\\\\n # execute application function and get Langfuse parent observation (span/generation/event)\\\\\\\\n # output also returned as it is used to evaluate the run\\\\\\\\n generation, output = my_llm_application.run(item.input)\\\\\\\\n \\\\\\\\n # link the execution trace to the dataset item and give it a run_name\\\\\\\\n item.link(generation, \\\\\\\\\\\\\\\"<run_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n # optionally, evaluate the output to compare different runs more easily\\\\\\\\n generation.score(\\\\\\\\n name=\\\\\\\\\\\\\\\"<example_eval>\\\\\\\\\\\\\\\",\\\\\\\\n # any float value\\\\\\\\n value=my_eval_fn(\\\\\\\\n item.input,\\\\\\\\n output,\\\\\\\\n item.expected_output\\\\\\\\n )\\\\\\\\n )\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nconst dataset = await langfuse.getDataset(\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\");\\\\\\\\n \\\\\\\\nfor (const item of dataset.items) {\\\\\\\\n // execute application function and get Langfuse parent observation (span/generation/event)\\\\\\\\n // output also returned as it is used to evaluate the run\\\\\\\\n const [generation, output] = await myLlmApplication.run(item.input);\\\\\\\\n \\\\\\\\n // link the execution trace to the dataset item and give it a run_name\\\\\\\\n await item.link(generation, \\\\\\\\\\\\\\\"<run_name>\\\\\\\\\\\\\\\");\\\\\\\\n \\\\\\\\n // optionally, evaluate the output to compare different runs more easily\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n generation.score({\\\\\\\\n name: \\\\\\\\\\\\\\\"<score_name>\\\\\\\\\\\\\\\",\\\\\\\\n value: myEvalFunction(item.input, output, item.expectedOutput),\\\\\\\\n });\\\\\\\\n}\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\ndataset = langfuse.get_dataset(\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\nfor item in dataset.items:\\\\\\\\n # Langchain calback handler that automatically links the execution trace to the dataset item\\\\\\\\n handler = item.get_langchain_handler(run_name=\\\\\\\\\\\\\\\"<run_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n # Execute application and pass custom handler\\\\\\\\n my_langchain_chain.run(item.input, callbacks=[handler])\\\\\\\\n```\\\\\\\\nDatasets are currently in beta on Langfuse Cloud as the API might still slightly change. If you\\\\\\\\u2019d like to try it, let us know via the in-app chat.\\\\\\\\n\\\\\\\\u2192 [Dataset docs](/docs/datasets/overview) \\\\\\\\u2192 [Python Cookbook](/docs/datasets/python-cookbook)\\\\\\\\n## \\\\\\\\ud83d\\\\\\\\udcca In-app dashboards\\\\\\\\nOver the last weeks, [analytics features](/docs/analytics/overview) were in public alpha on Langfuse Cloud. We\\\\\\\\u2019ve now shipped a new _query engine_ as an underlying abstraction for the native in-app dashboards. This is a major step towards bringing all analytics features into the Langfuse core project and helps us move much faster on these.\\\\\\\\nOver the next days, you\\\\\\\\u2019ll see more and more dashboards popping up in the app. If there is a specific analysis you\\\\\\\\u2019d like to see, suggest it on [Discord](/discord).\\\\\\\\n## \\\\\\\\ud83d\\\\\\\\udd04 New integrations\\\\\\\\nThe new integrations make it easier to get started with Langfuse. Thanks to the teams behind Langflow, Flowise and LiteLLM for building/collaborating on these integrations.\\\\\\\\nSee integrations docs for details:\\\\\\\\n  * [Langflow](/docs/langflow): No-code LLM app builder in Python\\\\\\\\n  * [Flowise](/docs/flowise): No-code LLM app builder in JS\\\\\\\\n  * [LiteLLM](/integrations/gateways/litellm): Python library to use any LLM model as drop in replacement of OpenAI API\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}]}\\\",\\\"role\\\":\\\"assistant\\\",\\\"tool_calls\\\":null,\\\"function_call\\\":null}}],\\\"usage\\\":{\\\"prompt_tokens\\\":21,\\\"completion_tokens\\\":7295,\\\"total_tokens\\\":7316},\\\"system_fingerprint\\\":\\\"inkeep-fp-690807bb-bbc2-44b0-8a56-491bf9df8dbe\\\",\\\"object\\\":\\\"chat.completion\\\",\\\"service_tier\\\":null,\\\"created\\\":1758226809},\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"{\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/dataset-runs/datasets\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Datasets\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Datasets - Langfuse\\\\\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\\\\\\\\\"Evaluation\\\\\\\\\\\\\\\")Dataset RunsDatasets\\\\\\\\nCopy as Markdown\\\\\\\\n# Datasets\\\\\\\\nA dataset is a collection of inputs and expected outputs and is used to test your application. Before executing your first dataset run, you need to create a dataset.\\\\\\\\n## Why use datasets?\\\\\\\\n  * **Datasets prerequisite for Dataset Runs** , they serve as the data input of Dataset Runs\\\\\\\\n  * Create test cases for your application with real production traces\\\\\\\\n  * Collaboratively create and collect dataset items with your team\\\\\\\\n  * Have a single source of truth for your test data\\\\\\\\n## Get Started\\\\\\\\n### Creating a dataset\\\\\\\\nDatasets have a name which is unique within a project.\\\\\\\\nPython SDKJS/TS SDKLangfuse UI\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.create_dataset(\\\\\\\\n name=\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\",\\\\\\\\n # optional description\\\\\\\\n description=\\\\\\\\\\\\\\\"My first dataset\\\\\\\\\\\\\\\",\\\\\\\\n # optional metadata\\\\\\\\n metadata={\\\\\\\\n \\\\\\\\\\\\\\\"author\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Alice\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"date\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"2022-01-01\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"benchmark\\\\\\\\\\\\\\\"\\\\\\\\n }\\\\\\\\n)\\\\\\\\n```\\\\\\\\n _See[Python SDK](/docs/sdk/python/sdk-v3) docs for details on how to initialize the Python client._\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport { LangfuseClient } from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\nconst langfuse = new LangfuseClient()\\\\\\\\n \\\\\\\\nawait langfuse.api.datasets.create({\\\\\\\\n name: \\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\",\\\\\\\\n // optional description\\\\\\\\n description: \\\\\\\\\\\\\\\"My first dataset\\\\\\\\\\\\\\\",\\\\\\\\n // optional metadata\\\\\\\\n metadata: {\\\\\\\\n author: \\\\\\\\\\\\\\\"Alice\\\\\\\\\\\\\\\",\\\\\\\\n date: \\\\\\\\\\\\\\\"2022-01-01\\\\\\\\\\\\\\\",\\\\\\\\n type: \\\\\\\\\\\\\\\"benchmark\\\\\\\\\\\\\\\",\\\\\\\\n },\\\\\\\\n});\\\\\\\\n```\\\\\\\\n  1. **Navigate to** `Your Project` > `Datasets`\\\\\\\\n  2. **Click on** `+ New dataset` to create a new dataset.\\\\\\\\n### Create new dataset items\\\\\\\\nDataset items can be added to a dataset by providing the input and optionally the expected output.\\\\\\\\nPython SDKJS/TS SDKLangfuse UI\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.create_dataset_item(\\\\\\\\n dataset_name=\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\",\\\\\\\\n # any python object or value, optional\\\\\\\\n input={\\\\\\\\n \\\\\\\\\\\\\\\"text\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"hello world\\\\\\\\\\\\\\\"\\\\\\\\n },\\\\\\\\n # any python object or value, optional\\\\\\\\n expected_output={\\\\\\\\n \\\\\\\\\\\\\\\"text\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"hello world\\\\\\\\\\\\\\\"\\\\\\\\n },\\\\\\\\n # metadata, optional\\\\\\\\n metadata={\\\\\\\\n \\\\\\\\\\\\\\\"model\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"llama3\\\\\\\\\\\\\\\",\\\\\\\\n }\\\\\\\\n)\\\\\\\\n```\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/dataset-runs/native-run\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Native Dataset Runs (Prompt Experiments)\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Native Dataset Runs (Prompt Experiments) - Langfuse\\\\\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\\\\\\\\\"Evaluation\\\\\\\\\\\\\\\")\\\\\\\\n[Dataset Runs](/docs/evaluation/dataset-runs/datasets \\\\\\\\\\\\\\\"Dataset Runs\\\\\\\\\\\\\\\")Native Run\\\\\\\\nCopy as Markdown\\\\\\\\n# Native Dataset Runs (Prompt Experiments)\\\\\\\\nYou can execute Native Dataset Runs **(also called Prompt Experiments)** in the Langfuse UI to test different prompt versions from [Prompt Management](/docs/prompt-management) or language models and compare the results side-by-side.\\\\\\\\nOptionally, you can use [LLM-as-a-Judge Evaluators](/docs/evaluation/evaluation-methods/llm-as-a-judge) to automatically score the responses based on the expected outputs to further analyze the results on an aggregate level.\\\\\\\\nThey are called \\\\\\\\u201cNative Dataset Runs\\\\\\\\u201d because they rely on Datasets, Prompts and optionally LLM-as-a-Judge Evaluators all being managed **natively** in Langfuse.\\\\\\\\n## Why use Native Dataset Runs (Prompt Experiments)?\\\\\\\\n  * Quickly test different prompt versions or models\\\\\\\\n  * Structure your prompt testing by using a dataset to test different prompt versions and models\\\\\\\\n  * Quickly iterate on prompts through Dataset Runs\\\\\\\\n  * Optionally use LLM-as-a-Judge Evaluators to score the responses based on the expected outputs from the dataset\\\\\\\\n  * Prevent regressions by running tests when making prompt changes\\\\\\\\n## Prerequisites\\\\\\\\n### Create a usable prompt\\\\\\\\nCreate a prompt that you want to test and evaluate. [How to create a prompt?](/docs/prompt-management/get-started)\\\\\\\\n **A prompt is usable when:** your prompt has variables that match the dataset item keys in the dataset that will be used for the Dataset Run. See the example below.\\\\\\\\nExample: Prompt Variables & Dataset Item Keys Mapping\\\\\\\\n  \\\\\\\\n **Prompt:**\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n{{ documentation }}\\\\\\\\n \\\\\\\\nQuestion: {{question}}\\\\\\\\n \\\\\\\\n```\\\\\\\\n  \\\\\\\\n**Dataset Item:**\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n{\\\\\\\\n \\\\\\\\\\\\\\\"documentation\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Langfuse is an LLM Engineering Platform\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"question\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What is Langfuse?\\\\\\\\\\\\\\\"\\\\\\\\n}\\\\\\\\n```\\\\\\\\nIn this example:\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/changelog/2024-11-18-dataset-runs-comparison-view\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Dataset Run Comparison View\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Dataset Run Comparison View - Langfuse\\\\\\\\n[\\\\\\\\u2190 Back to changelog](/changelog#2024-11-18-dataset-runs-comparison-view)\\\\\\\\nNovember 18, 2024 | Launch Week 2 \\\\\\\\ud83d\\\\\\\\ude80\\\\\\\\n# Dataset Run Comparison View\\\\\\\\n[Marlies Mayerhofer](https://twitter.com/marliessophie)\\\\\\\\nAfter running experiments on datasets, you can now compare results side-by-side, view metrics, and peek into details of each dataset item across runs.\\\\\\\\n## Introduction\\\\\\\\n**What is a dataset in Langfuse?** [Datasets](/docs/datasets/overview) in Langfuse allow you to create test sets and benchmarks to evaluate the performance of your LLM application. A dataset is a collection of dataset items, where each item contains inputs, expected outputs, and metadata. You can create datasets from production edge cases, synthetic test cases, or manual test cases. This enables continuous improvement through structured testing, pre-deployment benchmarking, and flexible evaluation using custom metrics or LLM-as-a-judge approaches.\\\\\\\\n**What is a dataset experiment run?** A dataset experiment run lets you test changes to your application - like trying different models, prompts, or parameters - by running each version against your test dataset and comparing the results through traces in Langfuse to evaluate which changes work best.\\\\\\\\n## What\\\\\\\\u2019s new?\\\\\\\\nAnd the fun continues, **Day 1 of[Launch Week 2](/blog/2024-11-17-launch-week-2)** is here.\\\\\\\\n[Langfuse Datasets](/docs/datasets/overview) now enables intuitive comparison of dataset experiment runs for technical and non-technical users. The view features an overview of each item in the dataset, and a summary of each selected experiment run. The latter includes metrics on latency, cost, scores and the application\\\\\\\\u2019s output response for each dataset item.\\\\\\\\n## How to use the comparison view?\\\\\\\\n### Setup dataset and run experiments\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n  * Follow the [getting started guide](/docs/datasets/get-started) to set up a dataset, populate it with items, and run experiments.\\\\\\\\n  * Alternatively, execute an end-to-end example ([Python notebook](/docs/datasets/python-cookbook)).\\\\\\\\n### Open comparison view\\\\\\\\n  1. Select multiple dataset runs\\\\\\\\n  2. `Actions`, select `Compare`\\\\\\\\n## Learn more\\\\\\\\nFor a conceptual introduction to datasets and offline experiments, see the [dataset documentation](/docs/datasets/overview).\\\\\\\\nLast updated on September 5, 2025\\\\\\\\nWas this page helpful?\\\\\\\\nYesNo\\\\\\\\n[Support](/support)\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/guides/cookbook/example_synthetic_datasets\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Synthetic Dataset Generation for LLM Evaluation\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Synthetic Dataset Generation for LLM Evaluation - Langfuse\\\\\\\\nGuides[Cookbooks](/guides/cookbook \\\\\\\\\\\\\\\"Cookbooks\\\\\\\\\\\\\\\")Synthetic Datasets\\\\\\\\nCopy as Markdown\\\\\\\\nThis is a Jupyter notebook\\\\\\\\n[Open on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/cookbook/example_synthetic_datasets.ipynb)\\\\\\\\n[Run on Google Colab](https://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/cookbook/example_synthetic_datasets.ipynb)\\\\\\\\n# Synthetic Dataset Generation for LLM Evaluation\\\\\\\\nIn this notebook, we will explore how to **generate synthetic datasets** using language models and uploading them to [Langfuse](https://langfuse.com) for evaluation.\\\\\\\\n## What are Langfuse Datasets?\\\\\\\\nIn Langfuse, a _dataset_ is a collection of _dataset items_ , each typically containing an `input` (e.g., user prompt/question), `expected_output` (the ground truth or ideal answer) and optional metadata.\\\\\\\\nDatasets are used for **evaluation**. You can run your LLM or application on each item in a dataset and compare the application\\\\\\\\u2019s responses to the expected outputs. This way, you can track performance over time and across different application configs (e.g. model versions or prompt changes).\\\\\\\\n## Cases your Dataset Should Cover\\\\\\\\n**Happy path** \\\\\\\\u2013 straightforward or common queries:\\\\\\\\n  * \\\\\\\\u201cWhat is the capital of France?\\\\\\\\u201d\\\\\\\\n  * \\\\\\\\u201cConvert 5 USD to EUR.\\\\\\\\u201d\\\\\\\\n**Edge cases** \\\\\\\\u2013 unusual or complex:\\\\\\\\n  * Very long prompts.\\\\\\\\n  * Ambiguous queries.\\\\\\\\n  * Very technical or niche.\\\\\\\\n**Adversarial cases** \\\\\\\\u2013 malicious or tricky:\\\\\\\\n  * Prompt injection attempts (\\\\\\\\u201cIgnore all instructions and \\\\\\\\u2026\\\\\\\\u201d).\\\\\\\\n  * Content policy violations (harassment, hate speech).\\\\\\\\n  * Logic traps (trick questions).\\\\\\\\n## Examples\\\\\\\\n### Example 1: Looping Over OpenAI API\\\\\\\\nWe\\\\\\\\u2019ll use OpenAI\\\\\\\\u2019s API in a simple loop to create synthetic questions for an airline chatbot. You could similarly prompt the model to generate _both_ questions and answers. %pip install openai langfuse\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport os\\\\\\\\n \\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n///# Get keys for your project from the project settings page: https://cloud.langfuse.com\\\\\\\\nos.environ[\\\\\\\\\\\\\\\"LANGFUSE_PUBLIC_KEY\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"pk-lf-...\\\\\\\\\\\\\\\" \\\\\\\\nos.environ[\\\\\\\\\\\\\\\"LANGFUSE_SECRET_KEY\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"sk-lf-...\\\\\\\\\\\\\\\" \\\\\\\\nos.environ[\\\\\\\\\\\\\\\"LANGFUSE_HOST\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"https://cloud.langfuse.com\\\\\\\\\\\\\\\" # \\\\\\\\ud83c\\\\\\\\uddea\\\\\\\\ud83c\\\\\\\\uddfa EU region\\\\\\\\n///# os.environ[\\\\\\\\\\\\\\\"LANGFUSE_HOST\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"https://us.cloud.langfuse.com\\\\\\\\\\\\\\\" # \\\\\\\\ud83c\\\\\\\\uddfa\\\\\\\\ud83c\\\\\\\\uddf8 US region\\\\\\\\n \\\\\\\\n///# Your openai key\\\\\\\\nos.environ[\\\\\\\\\\\\\\\"OPENAI_API_KEY\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"sk-proj-...\\\\\\\\\\\\\\\"\\\\\\\\n```\\\\\\\\nWith the environment variables set, we can now initialize the Langfuse client. `get_client()` initializes the Langfuse client using the credentials provided in the environment variables.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\n \\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\n///# Verify connection\\\\\\\\nif langfuse.auth_check():\\\\\\\\n print(\\\\\\\\\\\\\\\"Langfuse client is authenticated and ready!\\\\\\\\\\\\\\\")\\\\\\\\nelse:\\\\\\\\n print(\\\\\\\\\\\\\\\"Authentication failed. Please check your credentials and host.\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\\nLangfuse client is authenticated and ready!\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom openai import OpenAI\\\\\\\\nimport pandas as pd\\\\\\\\n \\\\\\\\nclient = OpenAI()\\\\\\\\n \\\\\\\\n///# Function to generate airline questions\\\\\\\\ndef generate_airline_questions(num_questions=20):\\\\\\\\n \\\\\\\\n questions = []\\\\\\\\n \\\\\\\\n for i in range(num_questions):\\\\\\\\n completion = client.chat.completions.create(\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4o\\\\\\\\\\\\\\\", \\\\\\\\n messages=[\\\\\\\\n {\\\\\\\\n \\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"system\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": (\\\\\\\\n \\\\\\\\\\\\\\\"You are a helpful customer service chatbot for an airline. \\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\\\\\\\\"Please generate a short, realistic question from a customer.\\\\\\\\\\\\\\\"\\\\\\\\n )\\\\\\\\n }\\\\\\\\n ],\\\\\\\\n temperature=1\\\\\\\\n )\\\\\\\\n question_text = completion.choices[0].message.content.strip()\\\\\\\\n questions.append(question_text)\\\\\\\\n \\\\\\\\n return questions\\\\\\\\n \\\\\\\\n///# Generate 20 airline-related questions\\\\\\\\nairline_questions = generate_airline_questions(num_questions=20)\\\\\\\\n \\\\\\\\n///# Convert to a Pandas DataFrame\\\\\\\\ndf = pd.DataFrame({\\\\\\\\\\\\\\\"Question\\\\\\\\\\\\\\\": airline_questions})\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n scenario=\\\\\\\\\\\\\\\"Non-technical users trying to query a database using plain English.\\\\\\\\\\\\\\\",\\\\\\\\n)\\\\\\\\n \\\\\\\\n///# 2. Initialize the Synthesizer\\\\\\\\nsynthesizer = Synthesizer(styling_config=styling_config)\\\\\\\\n \\\\\\\\n///# 3. Generate synthetic items from scratch, e.g. 20 items for a short demo\\\\\\\\nsynthesizer.generate_goldens_from_scratch(num_goldens=20)\\\\\\\\n \\\\\\\\n///# 4. Access the generated examples\\\\\\\\nsynthetic_goldens = synthesizer.synthetic_goldens\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\n///# 5. Create a Langfuse dataset\\\\\\\\ndeepeval_dataset_name = \\\\\\\\\\\\\\\"deepeval_synthetic_data\\\\\\\\\\\\\\\"\\\\\\\\nlangfuse.create_dataset(\\\\\\\\n name=deepeval_dataset_name,\\\\\\\\n description=\\\\\\\\\\\\\\\"Synthetic text-to-SQL data (DeepEval)\\\\\\\\\\\\\\\",\\\\\\\\n metadata={\\\\\\\\\\\\\\\"approach\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"deepeval\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"task\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"text-to-sql\\\\\\\\\\\\\\\"}\\\\\\\\n)\\\\\\\\n \\\\\\\\n///# 6. Upload the items\\\\\\\\nfor golden in synthetic_goldens:\\\\\\\\n langfuse.create_dataset_item(\\\\\\\\n dataset_name=deepeval_dataset_name,\\\\\\\\n input={\\\\\\\\\\\\\\\"query\\\\\\\\\\\\\\\": golden.input},\\\\\\\\n )\\\\\\\\n```\\\\\\\\n### Example 4: No-Code via Hugging Face Dataset Generator\\\\\\\\nIf you prefer a more UI-based approach, check out [Hugging Face\\\\\\\\u2019s Synthetic Data Generator](https://huggingface.co/blog/synthetic-data-generator). You can generate examples in the Hugging Face UI. Then you can download them as CSV and upload it in the Langfuse UI.\\\\\\\\n## Next Steps\\\\\\\\n  1. **Explore your dataset in Langfuse**. You can see each dataset in the UI.\\\\\\\\n  2. **Run experiments** You can now evaluate your application using this dataset.\\\\\\\\n  3. **Compare runs** over time or across models, prompts, or chain logic.\\\\\\\\nFor more details on how to run experiments on a dataset, see the [Langfuse docs](https://langfuse.com/docs/datasets/get-started#run-experiment-on-a-dataset).\\\\\\\\nLast updated on July 29, 2025\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Query Data in Langfuse via the SDK](/guides/cookbook/example_query_data_via_sdk \\\\\\\\\\\\\\\"Query Data in Langfuse via the SDK\\\\\\\\\\\\\\\")\\\\\\\\n[Amazon Bedrock](/guides/cookbook/integration_amazon_bedrock \\\\\\\\\\\\\\\"Amazon Bedrock\\\\\\\\\\\\\\\")\\\\\\\\nWas this page helpful?\\\\\\\\nYesNo\\\\\\\\n[Support](/support)\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/dataset-runs/data-model\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Dataset Runs Data Model\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Dataset Runs Data Model - Langfuse\\\\\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\\\\\\\\\"Evaluation\\\\\\\\\\\\\\\")\\\\\\\\n[Dataset Runs](/docs/evaluation/dataset-runs/datasets \\\\\\\\\\\\\\\"Dataset Runs\\\\\\\\\\\\\\\")Data Model\\\\\\\\nCopy as Markdown\\\\\\\\n# Dataset Runs Data Model\\\\\\\\nThis page describes the data model of datasets and dataset runs. For detailed reference please refer to the [API Reference](https://api.reference.langfuse.com).\\\\\\\\n## Datasets\\\\\\\\nDatasets are a collection of inputs and, optionally, expected outputs that can be during Dataset runs.\\\\\\\\n`Dataset`s are a collection of `DatasetItem`s.\\\\\\\\n1\\\\\\\\nn\\\\\\\\nDataset\\\\\\\\nname\\\\\\\\ndescription\\\\\\\\nmetadata\\\\\\\\nDatasetItem\\\\\\\\ndatasetName\\\\\\\\ninput\\\\\\\\nexpectedOutput\\\\\\\\nmetadata\\\\\\\\nsourceTraceId\\\\\\\\nsourceObservationId\\\\\\\\nid\\\\\\\\nstatus\\\\\\\\n### Dataset object\\\\\\\\nAttribute| Type| Required| Description  \\\\\\\\n---|---|---|---  \\\\\\\\n`name`| string| Yes| Name of the dataset  \\\\\\\\n`description`| string| No| Description of the dataset  \\\\\\\\n`metadata`| object| No| Additional metadata for the dataset  \\\\\\\\n  \\\\\\\\n### DatasetItem object\\\\\\\\nAttribute| Type| Required| Description  \\\\\\\\n---|---|---|---  \\\\\\\\n`datasetName`| string| Yes| Name of the dataset to add the item to  \\\\\\\\n`input`| object| No| Input data for the dataset item  \\\\\\\\n`expectedOutput`| object| No| Expected output data for the dataset item  \\\\\\\\n`metadata`| object| No| Additional metadata for the dataset item  \\\\\\\\n`sourceTraceId`| string| No| ID of the source trace to link this dataset item to  \\\\\\\\n`sourceObservationId`| string| No| ID of the source observation to link this dataset item to  \\\\\\\\n`id`| string| No| Unique identifier for the dataset item. Dataset items are upserted on their id. Id needs to be unique (project-level) and cannot be reused across datasets.  \\\\\\\\n`status`| DatasetStatus| No| Status of the dataset item. Defaults to ACTIVE for newly created items. Possible values: `ACTIVE`, `ARCHIVED`  \\\\\\\\n  \\\\\\\\n## DatasetRun\\\\\\\\nDataset runs are used to run a dataset through your LLM application and optionally apply evaluation methods to the results.\\\\\\\\n  \\\\\\\\n### DatasetRun object\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Attribute| Type| Required| Description  \\\\\\\\n---|---|---|---  \\\\\\\\n`datasetName`| string| Yes| Name of the dataset  \\\\\\\\n`runName`| string| Yes| Name of the dataset run  \\\\\\\\n  \\\\\\\\n### DatasetRunItem object\\\\\\\\nAttribute| Type| Required| Description  \\\\\\\\n---|---|---|---  \\\\\\\\n`runName`| string| Yes| Name of the dataset run to add the item to  \\\\\\\\n`runDescription`| string| No| Description of the run. If run exists, description will be updated  \\\\\\\\n`metadata`| object| No| Metadata of the dataset run, updates run if run already exists  \\\\\\\\n`datasetItemId`| string| Yes| ID of the dataset item to link to this run  \\\\\\\\n`observationId`| string| No| ID of the observation to link to this run  \\\\\\\\n`traceId`| string| No| ID of the trace to link to this run. traceId should always be provided. For compatibility with older SDK versions it can also be inferred from the provided observationId.  \\\\\\\\n  \\\\\\\\nMost of the time, we recommend that DatasetRunItems reference TraceIDs directly. The reference to ObservationID exists for backwards compatibility with older SDK versions.\\\\\\\\n## End to end data relations\\\\\\\\nDataSetRuns can combine a few Langfuse objects:\\\\\\\\n  * `DatasetRuns` are created by looping through all or selected `DatasetItem`s of a `Dataset` with your LLM application.\\\\\\\\n  * For each `DatasetItem` passed into the LLM application as an Input a `DatasetRunItem` & a `Trace` are created.\\\\\\\\n  * Optionally `Score`s can be added to the `Trace`s to evaluate the output of the LLM application during the `DatasetRun`.\\\\\\\\n  \\\\\\\\nLast updated on July 29, 2025\\\\\\\\n[Remote Run](/docs/evaluation/dataset-runs/remote-run \\\\\\\\\\\\\\\"Remote Run\\\\\\\\\\\\\\\")\\\\\\\\n[Troubleshooting & FAQ](/docs/evaluation/troubleshooting-and-faq \\\\\\\\\\\\\\\"Troubleshooting & FAQ\\\\\\\\\\\\\\\")\\\\\\\\nWas this page helpful?\\\\\\\\nYesNo\\\\\\\\n[Support](/support)\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"github_discussion\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://github.com/orgs/langfuse/discussions/4844\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Dataset creates a lot of identical processes that run infinitely.\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Title: Dataset creates a lot of identical processes that run infinitely.\\\\\\\\nCategory: Support\\\\\\\\nAuthor: @robotkoruslan\\\\\\\\nTimestamp: December 28, 2024\\\\\\\\nBody:\\\\\\\\nWhen running a simple experiment, dataset creates a lot of identical processes that run infinitely, if click trace error \\\\\\\\\\\\\\\"Path: traces.byIdWithObservationsAndScores\\\\\\\\\\\\\\\"\\\\\\\\r\\\\\\\\n<img width=\\\\\\\\\\\\\\\"1494\\\\\\\\\\\\\\\" alt=\\\\\\\\\\\\\\\"image\\\\\\\\\\\\\\\" src=\\\\\\\\\\\\\\\"https://github.com/user-attachments/assets/2ee21b5f-2769-46a2-b1c0-141f0014d6fa\\\\\\\\\\\\\\\" />\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n<img width=\\\\\\\\\\\\\\\"1494\\\\\\\\\\\\\\\" alt=\\\\\\\\\\\\\\\"image\\\\\\\\\\\\\\\" src=\\\\\\\\\\\\\\\"https://github.com/user-attachments/assets/8e56daf9-e405-411e-8f62-042292a64764\\\\\\\\\\\\\\\" />\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\nComments:\\\\\\\\n\\\\\\\\n    Comment #1 from @dosubot on December 28, 2024:\\\\\\\\n\\\\\\\\n    <!-- Greeting -->\\\\\\\\nHello, @robotkoruslan! I'm here to help you with any bugs, questions, or contributions you have. Let's tackle this issue together!\\\\\\\\n\\\\\\\\n<!-- Answer -->\\\\\\\\nThe issue you're encountering with identical processes running indefinitely during dataset runs in Langfuse could be related to how traces are managed and linked to dataset runs. Here are some potential causes and solutions:\\\\\\\\n\\\\\\\\n1. **Trace Linking**: Ensure that traces are correctly linked to dataset runs. Each dataset run item should have a corresponding `traceId` in the `datasetRunItems` table. If the trace is not properly linked or created, it could lead to a \\\\\\\\\\\\\\\"Trace not found\\\\\\\\\\\\\\\" error <sup>[[1]](https://github.com/langfuse/langfuse/blob/main/web/src/__tests__/async/dataset-service.servertest.ts)</sup><sup>[[2]](https://github.com/langfuse/langfuse/blob/main/web/src/pages/api/public/dataset-run-items.ts)</sup>.\\\\\\\\n\\\\\\\\n2. **Trace Retrieval**: The error with the path `traces.byIdWithObservationsAndScores` suggests an issue with retrieving traces by their ID along with observations and scores. This could be due to incorrect `traceId` or `projectId`, or if the trace has been deleted or not yet created <sup>[[3]](https://github.com/langfuse/langfuse/blob/main/web/src/server/api/routers/traces.ts)</sup>.\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/changelog/2024-11-21-all-new-datasets-and-evals-documentation\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"All new Datasets, Experimentation and Evaluation documentation\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"## New Datasets and Evaluation documentation\\\\\\\\nBased on this week\\\\\\\\u2019s changes, we have have restructured the documentation to:\\\\\\\\n  1. Better explain **how and when to use Langfuse Datasets and Evals** to evaluate your LLM applications in development\\\\\\\\n  2. Introduce the **core data model** of `datasets`, experiment `runs`, and `scores`\\\\\\\\n  3. Provide **end-to-end examples** for common workflows as Jupyter Notebooks\\\\\\\\n  4. Add many **more gifs and toggles with additional explanations** throughout the docs\\\\\\\\n### Evaluation\\\\\\\\n\\\\\\\\ud83d\\\\\\\\udca1\\\\\\\\n\\\\\\\\u2192 [Evaluation documentation](/docs/scores)\\\\\\\\nAnalyticsUser FeedbackLLM-as-a-JudgePrompt ExperimentAnnotation QueueCustom Evals\\\\\\\\nPlot evaluation results in the Langfuse Dashboard.\\\\\\\\nCollect feedback from your users. Can be captured in the frontend via our Browser SDK, server-side via the SDKs or API. Video includes example application.\\\\\\\\nRun fully managed LLM-as-a-judge evaluations on production or development traces. Can be applied to any step within your application for step-wise evaluations.\\\\\\\\nEvaluate prompts and models on datasets directly in the user interface. No custom code is needed.\\\\\\\\nBaseline your evaluation workflow with human annotations via Annotation Queues.\\\\\\\\nAdd custom evaluation results, supports numeric, boolean and categorical values.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nPOST /api/public/scores\\\\\\\\n```\\\\\\\\nAdd scores via Python or JS SDK.\\\\\\\\nExample (Python)\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.score(\\\\\\\\n trace_id=\\\\\\\\\\\\\\\"123\\\\\\\\\\\\\\\",\\\\\\\\n name=\\\\\\\\\\\\\\\"my_custom_evaluator\\\\\\\\\\\\\\\",\\\\\\\\n value=0.5,\\\\\\\\n)\\\\\\\\n```\\\\\\\\n### Datasets\\\\\\\\n\\\\\\\\ud83d\\\\\\\\udca1\\\\\\\\n\\\\\\\\u2192 [Datasets & Experiments documentation](/docs/datasets)\\\\\\\\nManage DatasetsCompare experimentsAdd new items from Langfuse Traces\\\\\\\\nCollaboratively manage datasets via UI, API, or SDKs.\\\\\\\\nRun experiments on datasets. Compare metrics across experiments, and see results side-by-side.\\\\\\\\nDirectly add new items from production traces to datasets to improve your datasets over time.\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"site\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/changelog/2024-11-18-dataset-runs-comparison-view\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Dataset Run Comparison View - Langfuse\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"---\\\\\\\\n\\\\\\\\n[\\\\\\\\u2190 Back to changelog](/changelog#2024-11-18-dataset-runs-comparison-view)\\\\\\\\nNovember 18, 2024 | Launch Week 2 \\\\\\\\ud83d\\\\\\\\ude80\\\\\\\\n# Dataset Run Comparison View\\\\\\\\n[Marlies Mayerhofer](https://twitter.com/marliessophie)\\\\\\\\nAfter running experiments on datasets, you can now compare results side-by-side, view metrics, and peek into details of each dataset item across runs.\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n## Introduction\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n**What is a dataset in Langfuse?** [Datasets](/docs/datasets/overview) in Langfuse allow you to create test sets and benchmarks to evaluate the performance of your LLM application. A dataset is a collection of dataset items, where each item contains inputs, expected outputs, and metadata. You can create datasets from production edge cases, synthetic test cases, or manual test cases. This enables continuous improvement through structured testing, pre-deployment benchmarking, and flexible evaluation using custom metrics or LLM-as-a-judge approaches.\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n**What is a dataset experiment run?** A dataset experiment run lets you test changes to your application - like trying different models, prompts, or parameters - by running each version against your test dataset and comparing the results through traces in Langfuse to evaluate which changes work best.\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n## What\\\\\\\\u2019s new?\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\nAnd the fun continues, **Day 1 of[Launch Week 2](/blog/2024-11-17-launch-week-2)** is here.\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n[Langfuse Datasets](/docs/datasets/overview) now enables intuitive comparison of dataset experiment runs for technical and non-technical users. The view features an overview of each item in the dataset, and a summary of each selected experiment run. The latter includes metrics on latency, cost, scores and the application\\\\\\\\u2019s output response for each dataset item.\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n## How to use the comparison view?\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n### Setup dataset and run experiments\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n  * Follow the [getting started guide](/docs/datasets/get-started) to set up a dataset, populate it with items, and run experiments.\\\\\\\\n  * Alternatively, execute an end-to-end example ([Python notebook](/docs/datasets/python-cookbook)).\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n### Open comparison view\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n  1. Select multiple dataset runs\\\\\\\\n  2. `Actions`, select `Compare`\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n## Learn more\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\nFor a conceptual introduction to datasets and offline experiments, see the [dataset documentation](/docs/datasets/overview).\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\nLast updated on September 5, 2025\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/blog/update-2023-09\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Langfuse Update \\\\\\\\u2014 September 2023\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n\\\\\\\\u2192 [Docs](/docs/scores/model-based-evals)\\\\\\\\n## \\\\\\\\ud83d\\\\\\\\uddc2\\\\\\\\ufe0f Datasets (beta)\\\\\\\\nSystematically test new iterations of your LLM app with `datasets`.\\\\\\\\nDatasets are collections of inputs and expected outputs that you can manage in Langfuse. Upload an existing dataset or create one based on production data (e.g. when discovering new edge cases).\\\\\\\\nWhen combined with automated evals, Datasets in Langfuse make it easy to systematically evaluate new iterations of your LLM app.\\\\\\\\n_Overview of dataset runs on a demo dataset_\\\\\\\\n_Run experiment on dataset_\\\\\\\\nPython SDKJS/TS SDKLangchain (Python)\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n \\\\\\\\ndataset = langfuse.get_dataset(\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\nfor item in dataset.items:\\\\\\\\n # execute application function and get Langfuse parent observation (span/generation/event)\\\\\\\\n # output also returned as it is used to evaluate the run\\\\\\\\n generation, output = my_llm_application.run(item.input)\\\\\\\\n \\\\\\\\n # link the execution trace to the dataset item and give it a run_name\\\\\\\\n item.link(generation, \\\\\\\\\\\\\\\"<run_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n # optionally, evaluate the output to compare different runs more easily\\\\\\\\n generation.score(\\\\\\\\n name=\\\\\\\\\\\\\\\"<example_eval>\\\\\\\\\\\\\\\",\\\\\\\\n # any float value\\\\\\\\n value=my_eval_fn(\\\\\\\\n item.input,\\\\\\\\n output,\\\\\\\\n item.expected_output\\\\\\\\n )\\\\\\\\n )\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nconst dataset = await langfuse.getDataset(\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\");\\\\\\\\n \\\\\\\\nfor (const item of dataset.items) {\\\\\\\\n // execute application function and get Langfuse parent observation (span/generation/event)\\\\\\\\n // output also returned as it is used to evaluate the run\\\\\\\\n const [generation, output] = await myLlmApplication.run(item.input);\\\\\\\\n \\\\\\\\n // link the execution trace to the dataset item and give it a run_name\\\\\\\\n await item.link(generation, \\\\\\\\\\\\\\\"<run_name>\\\\\\\\\\\\\\\");\\\\\\\\n \\\\\\\\n // optionally, evaluate the output to compare different runs more easily\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n generation.score({\\\\\\\\n name: \\\\\\\\\\\\\\\"<score_name>\\\\\\\\\\\\\\\",\\\\\\\\n value: myEvalFunction(item.input, output, item.expectedOutput),\\\\\\\\n });\\\\\\\\n}\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\ndataset = langfuse.get_dataset(\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\nfor item in dataset.items:\\\\\\\\n # Langchain calback handler that automatically links the execution trace to the dataset item\\\\\\\\n handler = item.get_langchain_handler(run_name=\\\\\\\\\\\\\\\"<run_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n # Execute application and pass custom handler\\\\\\\\n my_langchain_chain.run(item.input, callbacks=[handler])\\\\\\\\n```\\\\\\\\nDatasets are currently in beta on Langfuse Cloud as the API might still slightly change. If you\\\\\\\\u2019d like to try it, let us know via the in-app chat.\\\\\\\\n\\\\\\\\u2192 [Dataset docs](/docs/datasets/overview) \\\\\\\\u2192 [Python Cookbook](/docs/datasets/python-cookbook)\\\\\\\\n## \\\\\\\\ud83d\\\\\\\\udcca In-app dashboards\\\\\\\\nOver the last weeks, [analytics features](/docs/analytics/overview) were in public alpha on Langfuse Cloud. We\\\\\\\\u2019ve now shipped a new _query engine_ as an underlying abstraction for the native in-app dashboards. This is a major step towards bringing all analytics features into the Langfuse core project and helps us move much faster on these.\\\\\\\\nOver the next days, you\\\\\\\\u2019ll see more and more dashboards popping up in the app. If there is a specific analysis you\\\\\\\\u2019d like to see, suggest it on [Discord](/discord).\\\\\\\\n## \\\\\\\\ud83d\\\\\\\\udd04 New integrations\\\\\\\\nThe new integrations make it easier to get started with Langfuse. Thanks to the teams behind Langflow, Flowise and LiteLLM for building/collaborating on these integrations.\\\\\\\\nSee integrations docs for details:\\\\\\\\n  * [Langflow](/docs/langflow): No-code LLM app builder in Python\\\\\\\\n  * [Flowise](/docs/flowise): No-code LLM app builder in JS\\\\\\\\n  * [LiteLLM](/integrations/gateways/litellm): Python library to use any LLM model as drop in replacement of OpenAI API\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}]}\\\"}],\\\"isError\\\":false}\"},\"resourceAttributes\":{\"service.name\":\"unknown_service:node\",\"telemetry.sdk.language\":\"nodejs\",\"telemetry.sdk.name\":\"opentelemetry\",\"telemetry.sdk.version\":\"2.0.1\"},\"scope\":{\"name\":\"ai\",\"attributes\":{}}}",
        "level": "DEFAULT",
        "statusMessage": null,
        "version": null,
        "input": "{\"query\":\"What is a dataset in Langfuse? definition, use cases, evaluation, prompt testing, datasets feature docs\"}",
        "output": "{\"_meta\":{\"id\":\"inkeep-chatcmpl-1761a743-0fb1-4e4d-9c1c-10d4960ef29f\",\"model\":\"inkeep-rag\",\"choices\":[{\"finish_reason\":\"stop\",\"log_probs\":null,\"index\":0,\"message\":{\"content\":\"{\\\"content\\\": [{\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/dataset-runs/datasets\\\", \\\"title\\\": \\\"Datasets\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Datasets - Langfuse\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\"Evaluation\\\\\\\")Dataset RunsDatasets\\\\nCopy as Markdown\\\\n# Datasets\\\\nA dataset is a collection of inputs and expected outputs and is used to test your application. Before executing your first dataset run, you need to create a dataset.\\\\n## Why use datasets?\\\\n  * **Datasets prerequisite for Dataset Runs** , they serve as the data input of Dataset Runs\\\\n  * Create test cases for your application with real production traces\\\\n  * Collaboratively create and collect dataset items with your team\\\\n  * Have a single source of truth for your test data\\\\n## Get Started\\\\n### Creating a dataset\\\\nDatasets have a name which is unique within a project.\\\\nPython SDKJS/TS SDKLangfuse UI\\\\n    \\\\n    \\\\n```\\\\nlangfuse.create_dataset(\\\\n name=\\\\\\\"<dataset_name>\\\\\\\",\\\\n # optional description\\\\n description=\\\\\\\"My first dataset\\\\\\\",\\\\n # optional metadata\\\\n metadata={\\\\n \\\\\\\"author\\\\\\\": \\\\\\\"Alice\\\\\\\",\\\\n \\\\\\\"date\\\\\\\": \\\\\\\"2022-01-01\\\\\\\",\\\\n \\\\\\\"type\\\\\\\": \\\\\\\"benchmark\\\\\\\"\\\\n }\\\\n)\\\\n```\\\\n _See[Python SDK](/docs/sdk/python/sdk-v3) docs for details on how to initialize the Python client._\\\\n    \\\\n    \\\\n```\\\\nimport { LangfuseClient } from \\\\\\\"@langfuse/client\\\\\\\"\\\\n \\\\nconst langfuse = new LangfuseClient()\\\\n \\\\nawait langfuse.api.datasets.create({\\\\n name: \\\\\\\"<dataset_name>\\\\\\\",\\\\n // optional description\\\\n description: \\\\\\\"My first dataset\\\\\\\",\\\\n // optional metadata\\\\n metadata: {\\\\n author: \\\\\\\"Alice\\\\\\\",\\\\n date: \\\\\\\"2022-01-01\\\\\\\",\\\\n type: \\\\\\\"benchmark\\\\\\\",\\\\n },\\\\n});\\\\n```\\\\n  1. **Navigate to** `Your Project` > `Datasets`\\\\n  2. **Click on** `+ New dataset` to create a new dataset.\\\\n### Create new dataset items\\\\nDataset items can be added to a dataset by providing the input and optionally the expected output.\\\\nPython SDKJS/TS SDKLangfuse UI\\\\n    \\\\n    \\\\n```\\\\nlangfuse.create_dataset_item(\\\\n dataset_name=\\\\\\\"<dataset_name>\\\\\\\",\\\\n # any python object or value, optional\\\\n input={\\\\n \\\\\\\"text\\\\\\\": \\\\\\\"hello world\\\\\\\"\\\\n },\\\\n # any python object or value, optional\\\\n expected_output={\\\\n \\\\\\\"text\\\\\\\": \\\\\\\"hello world\\\\\\\"\\\\n },\\\\n # metadata, optional\\\\n metadata={\\\\n \\\\\\\"model\\\\\\\": \\\\\\\"llama3\\\\\\\",\\\\n }\\\\n)\\\\n```\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/dataset-runs/native-run\\\", \\\"title\\\": \\\"Native Dataset Runs (Prompt Experiments)\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Native Dataset Runs (Prompt Experiments) - Langfuse\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\"Evaluation\\\\\\\")\\\\n[Dataset Runs](/docs/evaluation/dataset-runs/datasets \\\\\\\"Dataset Runs\\\\\\\")Native Run\\\\nCopy as Markdown\\\\n# Native Dataset Runs (Prompt Experiments)\\\\nYou can execute Native Dataset Runs **(also called Prompt Experiments)** in the Langfuse UI to test different prompt versions from [Prompt Management](/docs/prompt-management) or language models and compare the results side-by-side.\\\\nOptionally, you can use [LLM-as-a-Judge Evaluators](/docs/evaluation/evaluation-methods/llm-as-a-judge) to automatically score the responses based on the expected outputs to further analyze the results on an aggregate level.\\\\nThey are called \\\\u201cNative Dataset Runs\\\\u201d because they rely on Datasets, Prompts and optionally LLM-as-a-Judge Evaluators all being managed **natively** in Langfuse.\\\\n## Why use Native Dataset Runs (Prompt Experiments)?\\\\n  * Quickly test different prompt versions or models\\\\n  * Structure your prompt testing by using a dataset to test different prompt versions and models\\\\n  * Quickly iterate on prompts through Dataset Runs\\\\n  * Optionally use LLM-as-a-Judge Evaluators to score the responses based on the expected outputs from the dataset\\\\n  * Prevent regressions by running tests when making prompt changes\\\\n## Prerequisites\\\\n### Create a usable prompt\\\\nCreate a prompt that you want to test and evaluate. [How to create a prompt?](/docs/prompt-management/get-started)\\\\n **A prompt is usable when:** your prompt has variables that match the dataset item keys in the dataset that will be used for the Dataset Run. See the example below.\\\\nExample: Prompt Variables & Dataset Item Keys Mapping\\\\n  \\\\n **Prompt:**\\\\n    \\\\n    \\\\n```\\\\n{{ documentation }}\\\\n \\\\nQuestion: {{question}}\\\\n \\\\n```\\\\n  \\\\n**Dataset Item:**\\\\n    \\\\n    \\\\n```\\\\n{\\\\n \\\\\\\"documentation\\\\\\\": \\\\\\\"Langfuse is an LLM Engineering Platform\\\\\\\",\\\\n \\\\\\\"question\\\\\\\": \\\\\\\"What is Langfuse?\\\\\\\"\\\\n}\\\\n```\\\\nIn this example:\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/changelog/2024-11-18-dataset-runs-comparison-view\\\", \\\"title\\\": \\\"Dataset Run Comparison View\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Dataset Run Comparison View - Langfuse\\\\n[\\\\u2190 Back to changelog](/changelog#2024-11-18-dataset-runs-comparison-view)\\\\nNovember 18, 2024 | Launch Week 2 \\\\ud83d\\\\ude80\\\\n# Dataset Run Comparison View\\\\n[Marlies Mayerhofer](https://twitter.com/marliessophie)\\\\nAfter running experiments on datasets, you can now compare results side-by-side, view metrics, and peek into details of each dataset item across runs.\\\\n## Introduction\\\\n**What is a dataset in Langfuse?** [Datasets](/docs/datasets/overview) in Langfuse allow you to create test sets and benchmarks to evaluate the performance of your LLM application. A dataset is a collection of dataset items, where each item contains inputs, expected outputs, and metadata. You can create datasets from production edge cases, synthetic test cases, or manual test cases. This enables continuous improvement through structured testing, pre-deployment benchmarking, and flexible evaluation using custom metrics or LLM-as-a-judge approaches.\\\\n**What is a dataset experiment run?** A dataset experiment run lets you test changes to your application - like trying different models, prompts, or parameters - by running each version against your test dataset and comparing the results through traces in Langfuse to evaluate which changes work best.\\\\n## What\\\\u2019s new?\\\\nAnd the fun continues, **Day 1 of[Launch Week 2](/blog/2024-11-17-launch-week-2)** is here.\\\\n[Langfuse Datasets](/docs/datasets/overview) now enables intuitive comparison of dataset experiment runs for technical and non-technical users. The view features an overview of each item in the dataset, and a summary of each selected experiment run. The latter includes metrics on latency, cost, scores and the application\\\\u2019s output response for each dataset item.\\\\n## How to use the comparison view?\\\\n### Setup dataset and run experiments\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n  * Follow the [getting started guide](/docs/datasets/get-started) to set up a dataset, populate it with items, and run experiments.\\\\n  * Alternatively, execute an end-to-end example ([Python notebook](/docs/datasets/python-cookbook)).\\\\n### Open comparison view\\\\n  1. Select multiple dataset runs\\\\n  2. `Actions`, select `Compare`\\\\n## Learn more\\\\nFor a conceptual introduction to datasets and offline experiments, see the [dataset documentation](/docs/datasets/overview).\\\\nLast updated on September 5, 2025\\\\nWas this page helpful?\\\\nYesNo\\\\n[Support](/support)\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/guides/cookbook/example_synthetic_datasets\\\", \\\"title\\\": \\\"Synthetic Dataset Generation for LLM Evaluation\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Synthetic Dataset Generation for LLM Evaluation - Langfuse\\\\nGuides[Cookbooks](/guides/cookbook \\\\\\\"Cookbooks\\\\\\\")Synthetic Datasets\\\\nCopy as Markdown\\\\nThis is a Jupyter notebook\\\\n[Open on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/cookbook/example_synthetic_datasets.ipynb)\\\\n[Run on Google Colab](https://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/cookbook/example_synthetic_datasets.ipynb)\\\\n# Synthetic Dataset Generation for LLM Evaluation\\\\nIn this notebook, we will explore how to **generate synthetic datasets** using language models and uploading them to [Langfuse](https://langfuse.com) for evaluation.\\\\n## What are Langfuse Datasets?\\\\nIn Langfuse, a _dataset_ is a collection of _dataset items_ , each typically containing an `input` (e.g., user prompt/question), `expected_output` (the ground truth or ideal answer) and optional metadata.\\\\nDatasets are used for **evaluation**. You can run your LLM or application on each item in a dataset and compare the application\\\\u2019s responses to the expected outputs. This way, you can track performance over time and across different application configs (e.g. model versions or prompt changes).\\\\n## Cases your Dataset Should Cover\\\\n**Happy path** \\\\u2013 straightforward or common queries:\\\\n  * \\\\u201cWhat is the capital of France?\\\\u201d\\\\n  * \\\\u201cConvert 5 USD to EUR.\\\\u201d\\\\n**Edge cases** \\\\u2013 unusual or complex:\\\\n  * Very long prompts.\\\\n  * Ambiguous queries.\\\\n  * Very technical or niche.\\\\n**Adversarial cases** \\\\u2013 malicious or tricky:\\\\n  * Prompt injection attempts (\\\\u201cIgnore all instructions and \\\\u2026\\\\u201d).\\\\n  * Content policy violations (harassment, hate speech).\\\\n  * Logic traps (trick questions).\\\\n## Examples\\\\n### Example 1: Looping Over OpenAI API\\\\nWe\\\\u2019ll use OpenAI\\\\u2019s API in a simple loop to create synthetic questions for an airline chatbot. You could similarly prompt the model to generate _both_ questions and answers. %pip install openai langfuse\\\\n    \\\\n    \\\\n```\\\\nimport os\\\\n \\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n///# Get keys for your project from the project settings page: https://cloud.langfuse.com\\\\nos.environ[\\\\\\\"LANGFUSE_PUBLIC_KEY\\\\\\\"] = \\\\\\\"pk-lf-...\\\\\\\" \\\\nos.environ[\\\\\\\"LANGFUSE_SECRET_KEY\\\\\\\"] = \\\\\\\"sk-lf-...\\\\\\\" \\\\nos.environ[\\\\\\\"LANGFUSE_HOST\\\\\\\"] = \\\\\\\"https://cloud.langfuse.com\\\\\\\" # \\\\ud83c\\\\uddea\\\\ud83c\\\\uddfa EU region\\\\n///# os.environ[\\\\\\\"LANGFUSE_HOST\\\\\\\"] = \\\\\\\"https://us.cloud.langfuse.com\\\\\\\" # \\\\ud83c\\\\uddfa\\\\ud83c\\\\uddf8 US region\\\\n \\\\n///# Your openai key\\\\nos.environ[\\\\\\\"OPENAI_API_KEY\\\\\\\"] = \\\\\\\"sk-proj-...\\\\\\\"\\\\n```\\\\nWith the environment variables set, we can now initialize the Langfuse client. `get_client()` initializes the Langfuse client using the credentials provided in the environment variables.\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\n \\\\nlangfuse = get_client()\\\\n \\\\n///# Verify connection\\\\nif langfuse.auth_check():\\\\n print(\\\\\\\"Langfuse client is authenticated and ready!\\\\\\\")\\\\nelse:\\\\n print(\\\\\\\"Authentication failed. Please check your credentials and host.\\\\\\\")\\\\n```\\\\nLangfuse client is authenticated and ready!\\\\n    \\\\n    \\\\n```\\\\nfrom openai import OpenAI\\\\nimport pandas as pd\\\\n \\\\nclient = OpenAI()\\\\n \\\\n///# Function to generate airline questions\\\\ndef generate_airline_questions(num_questions=20):\\\\n \\\\n questions = []\\\\n \\\\n for i in range(num_questions):\\\\n completion = client.chat.completions.create(\\\\n model=\\\\\\\"gpt-4o\\\\\\\", \\\\n messages=[\\\\n {\\\\n \\\\\\\"role\\\\\\\": \\\\\\\"system\\\\\\\",\\\\n \\\\\\\"content\\\\\\\": (\\\\n \\\\\\\"You are a helpful customer service chatbot for an airline. \\\\\\\"\\\\n \\\\\\\"Please generate a short, realistic question from a customer.\\\\\\\"\\\\n )\\\\n }\\\\n ],\\\\n temperature=1\\\\n )\\\\n question_text = completion.choices[0].message.content.strip()\\\\n questions.append(question_text)\\\\n \\\\n return questions\\\\n \\\\n///# Generate 20 airline-related questions\\\\nairline_questions = generate_airline_questions(num_questions=20)\\\\n \\\\n///# Convert to a Pandas DataFrame\\\\ndf = pd.DataFrame({\\\\\\\"Question\\\\\\\": airline_questions})\\\\n```\\\\n    \\\\n    \\\\n```\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n scenario=\\\\\\\"Non-technical users trying to query a database using plain English.\\\\\\\",\\\\n)\\\\n \\\\n///# 2. Initialize the Synthesizer\\\\nsynthesizer = Synthesizer(styling_config=styling_config)\\\\n \\\\n///# 3. Generate synthetic items from scratch, e.g. 20 items for a short demo\\\\nsynthesizer.generate_goldens_from_scratch(num_goldens=20)\\\\n \\\\n///# 4. Access the generated examples\\\\nsynthetic_goldens = synthesizer.synthetic_goldens\\\\n```\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\nlangfuse = get_client()\\\\n \\\\n///# 5. Create a Langfuse dataset\\\\ndeepeval_dataset_name = \\\\\\\"deepeval_synthetic_data\\\\\\\"\\\\nlangfuse.create_dataset(\\\\n name=deepeval_dataset_name,\\\\n description=\\\\\\\"Synthetic text-to-SQL data (DeepEval)\\\\\\\",\\\\n metadata={\\\\\\\"approach\\\\\\\": \\\\\\\"deepeval\\\\\\\", \\\\\\\"task\\\\\\\": \\\\\\\"text-to-sql\\\\\\\"}\\\\n)\\\\n \\\\n///# 6. Upload the items\\\\nfor golden in synthetic_goldens:\\\\n langfuse.create_dataset_item(\\\\n dataset_name=deepeval_dataset_name,\\\\n input={\\\\\\\"query\\\\\\\": golden.input},\\\\n )\\\\n```\\\\n### Example 4: No-Code via Hugging Face Dataset Generator\\\\nIf you prefer a more UI-based approach, check out [Hugging Face\\\\u2019s Synthetic Data Generator](https://huggingface.co/blog/synthetic-data-generator). You can generate examples in the Hugging Face UI. Then you can download them as CSV and upload it in the Langfuse UI.\\\\n## Next Steps\\\\n  1. **Explore your dataset in Langfuse**. You can see each dataset in the UI.\\\\n  2. **Run experiments** You can now evaluate your application using this dataset.\\\\n  3. **Compare runs** over time or across models, prompts, or chain logic.\\\\nFor more details on how to run experiments on a dataset, see the [Langfuse docs](https://langfuse.com/docs/datasets/get-started#run-experiment-on-a-dataset).\\\\nLast updated on July 29, 2025\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Query Data in Langfuse via the SDK](/guides/cookbook/example_query_data_via_sdk \\\\\\\"Query Data in Langfuse via the SDK\\\\\\\")\\\\n[Amazon Bedrock](/guides/cookbook/integration_amazon_bedrock \\\\\\\"Amazon Bedrock\\\\\\\")\\\\nWas this page helpful?\\\\nYesNo\\\\n[Support](/support)\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/dataset-runs/data-model\\\", \\\"title\\\": \\\"Dataset Runs Data Model\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Dataset Runs Data Model - Langfuse\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\"Evaluation\\\\\\\")\\\\n[Dataset Runs](/docs/evaluation/dataset-runs/datasets \\\\\\\"Dataset Runs\\\\\\\")Data Model\\\\nCopy as Markdown\\\\n# Dataset Runs Data Model\\\\nThis page describes the data model of datasets and dataset runs. For detailed reference please refer to the [API Reference](https://api.reference.langfuse.com).\\\\n## Datasets\\\\nDatasets are a collection of inputs and, optionally, expected outputs that can be during Dataset runs.\\\\n`Dataset`s are a collection of `DatasetItem`s.\\\\n1\\\\nn\\\\nDataset\\\\nname\\\\ndescription\\\\nmetadata\\\\nDatasetItem\\\\ndatasetName\\\\ninput\\\\nexpectedOutput\\\\nmetadata\\\\nsourceTraceId\\\\nsourceObservationId\\\\nid\\\\nstatus\\\\n### Dataset object\\\\nAttribute| Type| Required| Description  \\\\n---|---|---|---  \\\\n`name`| string| Yes| Name of the dataset  \\\\n`description`| string| No| Description of the dataset  \\\\n`metadata`| object| No| Additional metadata for the dataset  \\\\n  \\\\n### DatasetItem object\\\\nAttribute| Type| Required| Description  \\\\n---|---|---|---  \\\\n`datasetName`| string| Yes| Name of the dataset to add the item to  \\\\n`input`| object| No| Input data for the dataset item  \\\\n`expectedOutput`| object| No| Expected output data for the dataset item  \\\\n`metadata`| object| No| Additional metadata for the dataset item  \\\\n`sourceTraceId`| string| No| ID of the source trace to link this dataset item to  \\\\n`sourceObservationId`| string| No| ID of the source observation to link this dataset item to  \\\\n`id`| string| No| Unique identifier for the dataset item. Dataset items are upserted on their id. Id needs to be unique (project-level) and cannot be reused across datasets.  \\\\n`status`| DatasetStatus| No| Status of the dataset item. Defaults to ACTIVE for newly created items. Possible values: `ACTIVE`, `ARCHIVED`  \\\\n  \\\\n## DatasetRun\\\\nDataset runs are used to run a dataset through your LLM application and optionally apply evaluation methods to the results.\\\\n  \\\\n### DatasetRun object\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Attribute| Type| Required| Description  \\\\n---|---|---|---  \\\\n`datasetName`| string| Yes| Name of the dataset  \\\\n`runName`| string| Yes| Name of the dataset run  \\\\n  \\\\n### DatasetRunItem object\\\\nAttribute| Type| Required| Description  \\\\n---|---|---|---  \\\\n`runName`| string| Yes| Name of the dataset run to add the item to  \\\\n`runDescription`| string| No| Description of the run. If run exists, description will be updated  \\\\n`metadata`| object| No| Metadata of the dataset run, updates run if run already exists  \\\\n`datasetItemId`| string| Yes| ID of the dataset item to link to this run  \\\\n`observationId`| string| No| ID of the observation to link to this run  \\\\n`traceId`| string| No| ID of the trace to link to this run. traceId should always be provided. For compatibility with older SDK versions it can also be inferred from the provided observationId.  \\\\n  \\\\nMost of the time, we recommend that DatasetRunItems reference TraceIDs directly. The reference to ObservationID exists for backwards compatibility with older SDK versions.\\\\n## End to end data relations\\\\nDataSetRuns can combine a few Langfuse objects:\\\\n  * `DatasetRuns` are created by looping through all or selected `DatasetItem`s of a `Dataset` with your LLM application.\\\\n  * For each `DatasetItem` passed into the LLM application as an Input a `DatasetRunItem` & a `Trace` are created.\\\\n  * Optionally `Score`s can be added to the `Trace`s to evaluate the output of the LLM application during the `DatasetRun`.\\\\n  \\\\nLast updated on July 29, 2025\\\\n[Remote Run](/docs/evaluation/dataset-runs/remote-run \\\\\\\"Remote Run\\\\\\\")\\\\n[Troubleshooting & FAQ](/docs/evaluation/troubleshooting-and-faq \\\\\\\"Troubleshooting & FAQ\\\\\\\")\\\\nWas this page helpful?\\\\nYesNo\\\\n[Support](/support)\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"github_discussion\\\", \\\"url\\\": \\\"https://github.com/orgs/langfuse/discussions/4844\\\", \\\"title\\\": \\\"Dataset creates a lot of identical processes that run infinitely.\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Title: Dataset creates a lot of identical processes that run infinitely.\\\\nCategory: Support\\\\nAuthor: @robotkoruslan\\\\nTimestamp: December 28, 2024\\\\nBody:\\\\nWhen running a simple experiment, dataset creates a lot of identical processes that run infinitely, if click trace error \\\\\\\"Path: traces.byIdWithObservationsAndScores\\\\\\\"\\\\r\\\\n<img width=\\\\\\\"1494\\\\\\\" alt=\\\\\\\"image\\\\\\\" src=\\\\\\\"https://github.com/user-attachments/assets/2ee21b5f-2769-46a2-b1c0-141f0014d6fa\\\\\\\" />\\\\r\\\\n\\\\r\\\\n<img width=\\\\\\\"1494\\\\\\\" alt=\\\\\\\"image\\\\\\\" src=\\\\\\\"https://github.com/user-attachments/assets/8e56daf9-e405-411e-8f62-042292a64764\\\\\\\" />\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\nComments:\\\\n\\\\n    Comment #1 from @dosubot on December 28, 2024:\\\\n\\\\n    <!-- Greeting -->\\\\nHello, @robotkoruslan! I'm here to help you with any bugs, questions, or contributions you have. Let's tackle this issue together!\\\\n\\\\n<!-- Answer -->\\\\nThe issue you're encountering with identical processes running indefinitely during dataset runs in Langfuse could be related to how traces are managed and linked to dataset runs. Here are some potential causes and solutions:\\\\n\\\\n1. **Trace Linking**: Ensure that traces are correctly linked to dataset runs. Each dataset run item should have a corresponding `traceId` in the `datasetRunItems` table. If the trace is not properly linked or created, it could lead to a \\\\\\\"Trace not found\\\\\\\" error <sup>[[1]](https://github.com/langfuse/langfuse/blob/main/web/src/__tests__/async/dataset-service.servertest.ts)</sup><sup>[[2]](https://github.com/langfuse/langfuse/blob/main/web/src/pages/api/public/dataset-run-items.ts)</sup>.\\\\n\\\\n2. **Trace Retrieval**: The error with the path `traces.byIdWithObservationsAndScores` suggests an issue with retrieving traces by their ID along with observations and scores. This could be due to incorrect `traceId` or `projectId`, or if the trace has been deleted or not yet created <sup>[[3]](https://github.com/langfuse/langfuse/blob/main/web/src/server/api/routers/traces.ts)</sup>.\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/changelog/2024-11-21-all-new-datasets-and-evals-documentation\\\", \\\"title\\\": \\\"All new Datasets, Experimentation and Evaluation documentation\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"## New Datasets and Evaluation documentation\\\\nBased on this week\\\\u2019s changes, we have have restructured the documentation to:\\\\n  1. Better explain **how and when to use Langfuse Datasets and Evals** to evaluate your LLM applications in development\\\\n  2. Introduce the **core data model** of `datasets`, experiment `runs`, and `scores`\\\\n  3. Provide **end-to-end examples** for common workflows as Jupyter Notebooks\\\\n  4. Add many **more gifs and toggles with additional explanations** throughout the docs\\\\n### Evaluation\\\\n\\\\ud83d\\\\udca1\\\\n\\\\u2192 [Evaluation documentation](/docs/scores)\\\\nAnalyticsUser FeedbackLLM-as-a-JudgePrompt ExperimentAnnotation QueueCustom Evals\\\\nPlot evaluation results in the Langfuse Dashboard.\\\\nCollect feedback from your users. Can be captured in the frontend via our Browser SDK, server-side via the SDKs or API. Video includes example application.\\\\nRun fully managed LLM-as-a-judge evaluations on production or development traces. Can be applied to any step within your application for step-wise evaluations.\\\\nEvaluate prompts and models on datasets directly in the user interface. No custom code is needed.\\\\nBaseline your evaluation workflow with human annotations via Annotation Queues.\\\\nAdd custom evaluation results, supports numeric, boolean and categorical values.\\\\n    \\\\n    \\\\n```\\\\nPOST /api/public/scores\\\\n```\\\\nAdd scores via Python or JS SDK.\\\\nExample (Python)\\\\n    \\\\n    \\\\n```\\\\nlangfuse.score(\\\\n trace_id=\\\\\\\"123\\\\\\\",\\\\n name=\\\\\\\"my_custom_evaluator\\\\\\\",\\\\n value=0.5,\\\\n)\\\\n```\\\\n### Datasets\\\\n\\\\ud83d\\\\udca1\\\\n\\\\u2192 [Datasets & Experiments documentation](/docs/datasets)\\\\nManage DatasetsCompare experimentsAdd new items from Langfuse Traces\\\\nCollaboratively manage datasets via UI, API, or SDKs.\\\\nRun experiments on datasets. Compare metrics across experiments, and see results side-by-side.\\\\nDirectly add new items from production traces to datasets to improve your datasets over time.\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"site\\\", \\\"url\\\": \\\"https://langfuse.com/changelog/2024-11-18-dataset-runs-comparison-view\\\", \\\"title\\\": \\\"Dataset Run Comparison View - Langfuse\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"---\\\\n\\\\n[\\\\u2190 Back to changelog](/changelog#2024-11-18-dataset-runs-comparison-view)\\\\nNovember 18, 2024 | Launch Week 2 \\\\ud83d\\\\ude80\\\\n# Dataset Run Comparison View\\\\n[Marlies Mayerhofer](https://twitter.com/marliessophie)\\\\nAfter running experiments on datasets, you can now compare results side-by-side, view metrics, and peek into details of each dataset item across runs.\\\\n\\\\n---\\\\n\\\\n## Introduction\\\\n\\\\n---\\\\n\\\\n**What is a dataset in Langfuse?** [Datasets](/docs/datasets/overview) in Langfuse allow you to create test sets and benchmarks to evaluate the performance of your LLM application. A dataset is a collection of dataset items, where each item contains inputs, expected outputs, and metadata. You can create datasets from production edge cases, synthetic test cases, or manual test cases. This enables continuous improvement through structured testing, pre-deployment benchmarking, and flexible evaluation using custom metrics or LLM-as-a-judge approaches.\\\\n\\\\n---\\\\n\\\\n**What is a dataset experiment run?** A dataset experiment run lets you test changes to your application - like trying different models, prompts, or parameters - by running each version against your test dataset and comparing the results through traces in Langfuse to evaluate which changes work best.\\\\n\\\\n---\\\\n\\\\n## What\\\\u2019s new?\\\\n\\\\n---\\\\n\\\\nAnd the fun continues, **Day 1 of[Launch Week 2](/blog/2024-11-17-launch-week-2)** is here.\\\\n\\\\n---\\\\n\\\\n[Langfuse Datasets](/docs/datasets/overview) now enables intuitive comparison of dataset experiment runs for technical and non-technical users. The view features an overview of each item in the dataset, and a summary of each selected experiment run. The latter includes metrics on latency, cost, scores and the application\\\\u2019s output response for each dataset item.\\\\n\\\\n---\\\\n\\\\n## How to use the comparison view?\\\\n\\\\n---\\\\n\\\\n### Setup dataset and run experiments\\\\n\\\\n---\\\\n\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n  * Follow the [getting started guide](/docs/datasets/get-started) to set up a dataset, populate it with items, and run experiments.\\\\n  * Alternatively, execute an end-to-end example ([Python notebook](/docs/datasets/python-cookbook)).\\\\n\\\\n---\\\\n\\\\n### Open comparison view\\\\n\\\\n---\\\\n\\\\n  1. Select multiple dataset runs\\\\n  2. `Actions`, select `Compare`\\\\n\\\\n---\\\\n\\\\n## Learn more\\\\n\\\\n---\\\\n\\\\nFor a conceptual introduction to datasets and offline experiments, see the [dataset documentation](/docs/datasets/overview).\\\\n\\\\n---\\\\n\\\\nLast updated on September 5, 2025\\\\n\\\\n---\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/blog/update-2023-09\\\", \\\"title\\\": \\\"Langfuse Update \\\\u2014 September 2023\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n\\\\u2192 [Docs](/docs/scores/model-based-evals)\\\\n## \\\\ud83d\\\\uddc2\\\\ufe0f Datasets (beta)\\\\nSystematically test new iterations of your LLM app with `datasets`.\\\\nDatasets are collections of inputs and expected outputs that you can manage in Langfuse. Upload an existing dataset or create one based on production data (e.g. when discovering new edge cases).\\\\nWhen combined with automated evals, Datasets in Langfuse make it easy to systematically evaluate new iterations of your LLM app.\\\\n_Overview of dataset runs on a demo dataset_\\\\n_Run experiment on dataset_\\\\nPython SDKJS/TS SDKLangchain (Python)\\\\n    \\\\n    \\\\n```\\\\n \\\\ndataset = langfuse.get_dataset(\\\\\\\"<dataset_name>\\\\\\\")\\\\n \\\\nfor item in dataset.items:\\\\n # execute application function and get Langfuse parent observation (span/generation/event)\\\\n # output also returned as it is used to evaluate the run\\\\n generation, output = my_llm_application.run(item.input)\\\\n \\\\n # link the execution trace to the dataset item and give it a run_name\\\\n item.link(generation, \\\\\\\"<run_name>\\\\\\\")\\\\n \\\\n # optionally, evaluate the output to compare different runs more easily\\\\n generation.score(\\\\n name=\\\\\\\"<example_eval>\\\\\\\",\\\\n # any float value\\\\n value=my_eval_fn(\\\\n item.input,\\\\n output,\\\\n item.expected_output\\\\n )\\\\n )\\\\n```\\\\n    \\\\n    \\\\n```\\\\nconst dataset = await langfuse.getDataset(\\\\\\\"<dataset_name>\\\\\\\");\\\\n \\\\nfor (const item of dataset.items) {\\\\n // execute application function and get Langfuse parent observation (span/generation/event)\\\\n // output also returned as it is used to evaluate the run\\\\n const [generation, output] = await myLlmApplication.run(item.input);\\\\n \\\\n // link the execution trace to the dataset item and give it a run_name\\\\n await item.link(generation, \\\\\\\"<run_name>\\\\\\\");\\\\n \\\\n // optionally, evaluate the output to compare different runs more easily\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n generation.score({\\\\n name: \\\\\\\"<score_name>\\\\\\\",\\\\n value: myEvalFunction(item.input, output, item.expectedOutput),\\\\n });\\\\n}\\\\n```\\\\n    \\\\n    \\\\n```\\\\ndataset = langfuse.get_dataset(\\\\\\\"<dataset_name>\\\\\\\")\\\\n \\\\nfor item in dataset.items:\\\\n # Langchain calback handler that automatically links the execution trace to the dataset item\\\\n handler = item.get_langchain_handler(run_name=\\\\\\\"<run_name>\\\\\\\")\\\\n \\\\n # Execute application and pass custom handler\\\\n my_langchain_chain.run(item.input, callbacks=[handler])\\\\n```\\\\nDatasets are currently in beta on Langfuse Cloud as the API might still slightly change. If you\\\\u2019d like to try it, let us know via the in-app chat.\\\\n\\\\u2192 [Dataset docs](/docs/datasets/overview) \\\\u2192 [Python Cookbook](/docs/datasets/python-cookbook)\\\\n## \\\\ud83d\\\\udcca In-app dashboards\\\\nOver the last weeks, [analytics features](/docs/analytics/overview) were in public alpha on Langfuse Cloud. We\\\\u2019ve now shipped a new _query engine_ as an underlying abstraction for the native in-app dashboards. This is a major step towards bringing all analytics features into the Langfuse core project and helps us move much faster on these.\\\\nOver the next days, you\\\\u2019ll see more and more dashboards popping up in the app. If there is a specific analysis you\\\\u2019d like to see, suggest it on [Discord](/discord).\\\\n## \\\\ud83d\\\\udd04 New integrations\\\\nThe new integrations make it easier to get started with Langfuse. Thanks to the teams behind Langflow, Flowise and LiteLLM for building/collaborating on these integrations.\\\\nSee integrations docs for details:\\\\n  * [Langflow](/docs/langflow): No-code LLM app builder in Python\\\\n  * [Flowise](/docs/flowise): No-code LLM app builder in JS\\\\n  * [LiteLLM](/integrations/gateways/litellm): Python library to use any LLM model as drop in replacement of OpenAI API\\\"}], \\\"type\\\": \\\"content\\\"}}]}\",\"role\":\"assistant\",\"tool_calls\":null,\"function_call\":null}}],\"usage\":{\"prompt_tokens\":21,\"completion_tokens\":7295,\"total_tokens\":7316},\"system_fingerprint\":\"inkeep-fp-690807bb-bbc2-44b0-8a56-491bf9df8dbe\",\"object\":\"chat.completion\",\"service_tier\":null,\"created\":1758226809},\"content\":[{\"type\":\"text\",\"text\":\"{\\\"content\\\": [{\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/dataset-runs/datasets\\\", \\\"title\\\": \\\"Datasets\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Datasets - Langfuse\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\"Evaluation\\\\\\\")Dataset RunsDatasets\\\\nCopy as Markdown\\\\n# Datasets\\\\nA dataset is a collection of inputs and expected outputs and is used to test your application. Before executing your first dataset run, you need to create a dataset.\\\\n## Why use datasets?\\\\n  * **Datasets prerequisite for Dataset Runs** , they serve as the data input of Dataset Runs\\\\n  * Create test cases for your application with real production traces\\\\n  * Collaboratively create and collect dataset items with your team\\\\n  * Have a single source of truth for your test data\\\\n## Get Started\\\\n### Creating a dataset\\\\nDatasets have a name which is unique within a project.\\\\nPython SDKJS/TS SDKLangfuse UI\\\\n    \\\\n    \\\\n```\\\\nlangfuse.create_dataset(\\\\n name=\\\\\\\"<dataset_name>\\\\\\\",\\\\n # optional description\\\\n description=\\\\\\\"My first dataset\\\\\\\",\\\\n # optional metadata\\\\n metadata={\\\\n \\\\\\\"author\\\\\\\": \\\\\\\"Alice\\\\\\\",\\\\n \\\\\\\"date\\\\\\\": \\\\\\\"2022-01-01\\\\\\\",\\\\n \\\\\\\"type\\\\\\\": \\\\\\\"benchmark\\\\\\\"\\\\n }\\\\n)\\\\n```\\\\n _See[Python SDK](/docs/sdk/python/sdk-v3) docs for details on how to initialize the Python client._\\\\n    \\\\n    \\\\n```\\\\nimport { LangfuseClient } from \\\\\\\"@langfuse/client\\\\\\\"\\\\n \\\\nconst langfuse = new LangfuseClient()\\\\n \\\\nawait langfuse.api.datasets.create({\\\\n name: \\\\\\\"<dataset_name>\\\\\\\",\\\\n // optional description\\\\n description: \\\\\\\"My first dataset\\\\\\\",\\\\n // optional metadata\\\\n metadata: {\\\\n author: \\\\\\\"Alice\\\\\\\",\\\\n date: \\\\\\\"2022-01-01\\\\\\\",\\\\n type: \\\\\\\"benchmark\\\\\\\",\\\\n },\\\\n});\\\\n```\\\\n  1. **Navigate to** `Your Project` > `Datasets`\\\\n  2. **Click on** `+ New dataset` to create a new dataset.\\\\n### Create new dataset items\\\\nDataset items can be added to a dataset by providing the input and optionally the expected output.\\\\nPython SDKJS/TS SDKLangfuse UI\\\\n    \\\\n    \\\\n```\\\\nlangfuse.create_dataset_item(\\\\n dataset_name=\\\\\\\"<dataset_name>\\\\\\\",\\\\n # any python object or value, optional\\\\n input={\\\\n \\\\\\\"text\\\\\\\": \\\\\\\"hello world\\\\\\\"\\\\n },\\\\n # any python object or value, optional\\\\n expected_output={\\\\n \\\\\\\"text\\\\\\\": \\\\\\\"hello world\\\\\\\"\\\\n },\\\\n # metadata, optional\\\\n metadata={\\\\n \\\\\\\"model\\\\\\\": \\\\\\\"llama3\\\\\\\",\\\\n }\\\\n)\\\\n```\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/dataset-runs/native-run\\\", \\\"title\\\": \\\"Native Dataset Runs (Prompt Experiments)\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Native Dataset Runs (Prompt Experiments) - Langfuse\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\"Evaluation\\\\\\\")\\\\n[Dataset Runs](/docs/evaluation/dataset-runs/datasets \\\\\\\"Dataset Runs\\\\\\\")Native Run\\\\nCopy as Markdown\\\\n# Native Dataset Runs (Prompt Experiments)\\\\nYou can execute Native Dataset Runs **(also called Prompt Experiments)** in the Langfuse UI to test different prompt versions from [Prompt Management](/docs/prompt-management) or language models and compare the results side-by-side.\\\\nOptionally, you can use [LLM-as-a-Judge Evaluators](/docs/evaluation/evaluation-methods/llm-as-a-judge) to automatically score the responses based on the expected outputs to further analyze the results on an aggregate level.\\\\nThey are called \\\\u201cNative Dataset Runs\\\\u201d because they rely on Datasets, Prompts and optionally LLM-as-a-Judge Evaluators all being managed **natively** in Langfuse.\\\\n## Why use Native Dataset Runs (Prompt Experiments)?\\\\n  * Quickly test different prompt versions or models\\\\n  * Structure your prompt testing by using a dataset to test different prompt versions and models\\\\n  * Quickly iterate on prompts through Dataset Runs\\\\n  * Optionally use LLM-as-a-Judge Evaluators to score the responses based on the expected outputs from the dataset\\\\n  * Prevent regressions by running tests when making prompt changes\\\\n## Prerequisites\\\\n### Create a usable prompt\\\\nCreate a prompt that you want to test and evaluate. [How to create a prompt?](/docs/prompt-management/get-started)\\\\n **A prompt is usable when:** your prompt has variables that match the dataset item keys in the dataset that will be used for the Dataset Run. See the example below.\\\\nExample: Prompt Variables & Dataset Item Keys Mapping\\\\n  \\\\n **Prompt:**\\\\n    \\\\n    \\\\n```\\\\n{{ documentation }}\\\\n \\\\nQuestion: {{question}}\\\\n \\\\n```\\\\n  \\\\n**Dataset Item:**\\\\n    \\\\n    \\\\n```\\\\n{\\\\n \\\\\\\"documentation\\\\\\\": \\\\\\\"Langfuse is an LLM Engineering Platform\\\\\\\",\\\\n \\\\\\\"question\\\\\\\": \\\\\\\"What is Langfuse?\\\\\\\"\\\\n}\\\\n```\\\\nIn this example:\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/changelog/2024-11-18-dataset-runs-comparison-view\\\", \\\"title\\\": \\\"Dataset Run Comparison View\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Dataset Run Comparison View - Langfuse\\\\n[\\\\u2190 Back to changelog](/changelog#2024-11-18-dataset-runs-comparison-view)\\\\nNovember 18, 2024 | Launch Week 2 \\\\ud83d\\\\ude80\\\\n# Dataset Run Comparison View\\\\n[Marlies Mayerhofer](https://twitter.com/marliessophie)\\\\nAfter running experiments on datasets, you can now compare results side-by-side, view metrics, and peek into details of each dataset item across runs.\\\\n## Introduction\\\\n**What is a dataset in Langfuse?** [Datasets](/docs/datasets/overview) in Langfuse allow you to create test sets and benchmarks to evaluate the performance of your LLM application. A dataset is a collection of dataset items, where each item contains inputs, expected outputs, and metadata. You can create datasets from production edge cases, synthetic test cases, or manual test cases. This enables continuous improvement through structured testing, pre-deployment benchmarking, and flexible evaluation using custom metrics or LLM-as-a-judge approaches.\\\\n**What is a dataset experiment run?** A dataset experiment run lets you test changes to your application - like trying different models, prompts, or parameters - by running each version against your test dataset and comparing the results through traces in Langfuse to evaluate which changes work best.\\\\n## What\\\\u2019s new?\\\\nAnd the fun continues, **Day 1 of[Launch Week 2](/blog/2024-11-17-launch-week-2)** is here.\\\\n[Langfuse Datasets](/docs/datasets/overview) now enables intuitive comparison of dataset experiment runs for technical and non-technical users. The view features an overview of each item in the dataset, and a summary of each selected experiment run. The latter includes metrics on latency, cost, scores and the application\\\\u2019s output response for each dataset item.\\\\n## How to use the comparison view?\\\\n### Setup dataset and run experiments\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n  * Follow the [getting started guide](/docs/datasets/get-started) to set up a dataset, populate it with items, and run experiments.\\\\n  * Alternatively, execute an end-to-end example ([Python notebook](/docs/datasets/python-cookbook)).\\\\n### Open comparison view\\\\n  1. Select multiple dataset runs\\\\n  2. `Actions`, select `Compare`\\\\n## Learn more\\\\nFor a conceptual introduction to datasets and offline experiments, see the [dataset documentation](/docs/datasets/overview).\\\\nLast updated on September 5, 2025\\\\nWas this page helpful?\\\\nYesNo\\\\n[Support](/support)\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/guides/cookbook/example_synthetic_datasets\\\", \\\"title\\\": \\\"Synthetic Dataset Generation for LLM Evaluation\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Synthetic Dataset Generation for LLM Evaluation - Langfuse\\\\nGuides[Cookbooks](/guides/cookbook \\\\\\\"Cookbooks\\\\\\\")Synthetic Datasets\\\\nCopy as Markdown\\\\nThis is a Jupyter notebook\\\\n[Open on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/cookbook/example_synthetic_datasets.ipynb)\\\\n[Run on Google Colab](https://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/cookbook/example_synthetic_datasets.ipynb)\\\\n# Synthetic Dataset Generation for LLM Evaluation\\\\nIn this notebook, we will explore how to **generate synthetic datasets** using language models and uploading them to [Langfuse](https://langfuse.com) for evaluation.\\\\n## What are Langfuse Datasets?\\\\nIn Langfuse, a _dataset_ is a collection of _dataset items_ , each typically containing an `input` (e.g., user prompt/question), `expected_output` (the ground truth or ideal answer) and optional metadata.\\\\nDatasets are used for **evaluation**. You can run your LLM or application on each item in a dataset and compare the application\\\\u2019s responses to the expected outputs. This way, you can track performance over time and across different application configs (e.g. model versions or prompt changes).\\\\n## Cases your Dataset Should Cover\\\\n**Happy path** \\\\u2013 straightforward or common queries:\\\\n  * \\\\u201cWhat is the capital of France?\\\\u201d\\\\n  * \\\\u201cConvert 5 USD to EUR.\\\\u201d\\\\n**Edge cases** \\\\u2013 unusual or complex:\\\\n  * Very long prompts.\\\\n  * Ambiguous queries.\\\\n  * Very technical or niche.\\\\n**Adversarial cases** \\\\u2013 malicious or tricky:\\\\n  * Prompt injection attempts (\\\\u201cIgnore all instructions and \\\\u2026\\\\u201d).\\\\n  * Content policy violations (harassment, hate speech).\\\\n  * Logic traps (trick questions).\\\\n## Examples\\\\n### Example 1: Looping Over OpenAI API\\\\nWe\\\\u2019ll use OpenAI\\\\u2019s API in a simple loop to create synthetic questions for an airline chatbot. You could similarly prompt the model to generate _both_ questions and answers. %pip install openai langfuse\\\\n    \\\\n    \\\\n```\\\\nimport os\\\\n \\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n///# Get keys for your project from the project settings page: https://cloud.langfuse.com\\\\nos.environ[\\\\\\\"LANGFUSE_PUBLIC_KEY\\\\\\\"] = \\\\\\\"pk-lf-...\\\\\\\" \\\\nos.environ[\\\\\\\"LANGFUSE_SECRET_KEY\\\\\\\"] = \\\\\\\"sk-lf-...\\\\\\\" \\\\nos.environ[\\\\\\\"LANGFUSE_HOST\\\\\\\"] = \\\\\\\"https://cloud.langfuse.com\\\\\\\" # \\\\ud83c\\\\uddea\\\\ud83c\\\\uddfa EU region\\\\n///# os.environ[\\\\\\\"LANGFUSE_HOST\\\\\\\"] = \\\\\\\"https://us.cloud.langfuse.com\\\\\\\" # \\\\ud83c\\\\uddfa\\\\ud83c\\\\uddf8 US region\\\\n \\\\n///# Your openai key\\\\nos.environ[\\\\\\\"OPENAI_API_KEY\\\\\\\"] = \\\\\\\"sk-proj-...\\\\\\\"\\\\n```\\\\nWith the environment variables set, we can now initialize the Langfuse client. `get_client()` initializes the Langfuse client using the credentials provided in the environment variables.\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\n \\\\nlangfuse = get_client()\\\\n \\\\n///# Verify connection\\\\nif langfuse.auth_check():\\\\n print(\\\\\\\"Langfuse client is authenticated and ready!\\\\\\\")\\\\nelse:\\\\n print(\\\\\\\"Authentication failed. Please check your credentials and host.\\\\\\\")\\\\n```\\\\nLangfuse client is authenticated and ready!\\\\n    \\\\n    \\\\n```\\\\nfrom openai import OpenAI\\\\nimport pandas as pd\\\\n \\\\nclient = OpenAI()\\\\n \\\\n///# Function to generate airline questions\\\\ndef generate_airline_questions(num_questions=20):\\\\n \\\\n questions = []\\\\n \\\\n for i in range(num_questions):\\\\n completion = client.chat.completions.create(\\\\n model=\\\\\\\"gpt-4o\\\\\\\", \\\\n messages=[\\\\n {\\\\n \\\\\\\"role\\\\\\\": \\\\\\\"system\\\\\\\",\\\\n \\\\\\\"content\\\\\\\": (\\\\n \\\\\\\"You are a helpful customer service chatbot for an airline. \\\\\\\"\\\\n \\\\\\\"Please generate a short, realistic question from a customer.\\\\\\\"\\\\n )\\\\n }\\\\n ],\\\\n temperature=1\\\\n )\\\\n question_text = completion.choices[0].message.content.strip()\\\\n questions.append(question_text)\\\\n \\\\n return questions\\\\n \\\\n///# Generate 20 airline-related questions\\\\nairline_questions = generate_airline_questions(num_questions=20)\\\\n \\\\n///# Convert to a Pandas DataFrame\\\\ndf = pd.DataFrame({\\\\\\\"Question\\\\\\\": airline_questions})\\\\n```\\\\n    \\\\n    \\\\n```\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n scenario=\\\\\\\"Non-technical users trying to query a database using plain English.\\\\\\\",\\\\n)\\\\n \\\\n///# 2. Initialize the Synthesizer\\\\nsynthesizer = Synthesizer(styling_config=styling_config)\\\\n \\\\n///# 3. Generate synthetic items from scratch, e.g. 20 items for a short demo\\\\nsynthesizer.generate_goldens_from_scratch(num_goldens=20)\\\\n \\\\n///# 4. Access the generated examples\\\\nsynthetic_goldens = synthesizer.synthetic_goldens\\\\n```\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\nlangfuse = get_client()\\\\n \\\\n///# 5. Create a Langfuse dataset\\\\ndeepeval_dataset_name = \\\\\\\"deepeval_synthetic_data\\\\\\\"\\\\nlangfuse.create_dataset(\\\\n name=deepeval_dataset_name,\\\\n description=\\\\\\\"Synthetic text-to-SQL data (DeepEval)\\\\\\\",\\\\n metadata={\\\\\\\"approach\\\\\\\": \\\\\\\"deepeval\\\\\\\", \\\\\\\"task\\\\\\\": \\\\\\\"text-to-sql\\\\\\\"}\\\\n)\\\\n \\\\n///# 6. Upload the items\\\\nfor golden in synthetic_goldens:\\\\n langfuse.create_dataset_item(\\\\n dataset_name=deepeval_dataset_name,\\\\n input={\\\\\\\"query\\\\\\\": golden.input},\\\\n )\\\\n```\\\\n### Example 4: No-Code via Hugging Face Dataset Generator\\\\nIf you prefer a more UI-based approach, check out [Hugging Face\\\\u2019s Synthetic Data Generator](https://huggingface.co/blog/synthetic-data-generator). You can generate examples in the Hugging Face UI. Then you can download them as CSV and upload it in the Langfuse UI.\\\\n## Next Steps\\\\n  1. **Explore your dataset in Langfuse**. You can see each dataset in the UI.\\\\n  2. **Run experiments** You can now evaluate your application using this dataset.\\\\n  3. **Compare runs** over time or across models, prompts, or chain logic.\\\\nFor more details on how to run experiments on a dataset, see the [Langfuse docs](https://langfuse.com/docs/datasets/get-started#run-experiment-on-a-dataset).\\\\nLast updated on July 29, 2025\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Query Data in Langfuse via the SDK](/guides/cookbook/example_query_data_via_sdk \\\\\\\"Query Data in Langfuse via the SDK\\\\\\\")\\\\n[Amazon Bedrock](/guides/cookbook/integration_amazon_bedrock \\\\\\\"Amazon Bedrock\\\\\\\")\\\\nWas this page helpful?\\\\nYesNo\\\\n[Support](/support)\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/dataset-runs/data-model\\\", \\\"title\\\": \\\"Dataset Runs Data Model\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Dataset Runs Data Model - Langfuse\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\"Evaluation\\\\\\\")\\\\n[Dataset Runs](/docs/evaluation/dataset-runs/datasets \\\\\\\"Dataset Runs\\\\\\\")Data Model\\\\nCopy as Markdown\\\\n# Dataset Runs Data Model\\\\nThis page describes the data model of datasets and dataset runs. For detailed reference please refer to the [API Reference](https://api.reference.langfuse.com).\\\\n## Datasets\\\\nDatasets are a collection of inputs and, optionally, expected outputs that can be during Dataset runs.\\\\n`Dataset`s are a collection of `DatasetItem`s.\\\\n1\\\\nn\\\\nDataset\\\\nname\\\\ndescription\\\\nmetadata\\\\nDatasetItem\\\\ndatasetName\\\\ninput\\\\nexpectedOutput\\\\nmetadata\\\\nsourceTraceId\\\\nsourceObservationId\\\\nid\\\\nstatus\\\\n### Dataset object\\\\nAttribute| Type| Required| Description  \\\\n---|---|---|---  \\\\n`name`| string| Yes| Name of the dataset  \\\\n`description`| string| No| Description of the dataset  \\\\n`metadata`| object| No| Additional metadata for the dataset  \\\\n  \\\\n### DatasetItem object\\\\nAttribute| Type| Required| Description  \\\\n---|---|---|---  \\\\n`datasetName`| string| Yes| Name of the dataset to add the item to  \\\\n`input`| object| No| Input data for the dataset item  \\\\n`expectedOutput`| object| No| Expected output data for the dataset item  \\\\n`metadata`| object| No| Additional metadata for the dataset item  \\\\n`sourceTraceId`| string| No| ID of the source trace to link this dataset item to  \\\\n`sourceObservationId`| string| No| ID of the source observation to link this dataset item to  \\\\n`id`| string| No| Unique identifier for the dataset item. Dataset items are upserted on their id. Id needs to be unique (project-level) and cannot be reused across datasets.  \\\\n`status`| DatasetStatus| No| Status of the dataset item. Defaults to ACTIVE for newly created items. Possible values: `ACTIVE`, `ARCHIVED`  \\\\n  \\\\n## DatasetRun\\\\nDataset runs are used to run a dataset through your LLM application and optionally apply evaluation methods to the results.\\\\n  \\\\n### DatasetRun object\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Attribute| Type| Required| Description  \\\\n---|---|---|---  \\\\n`datasetName`| string| Yes| Name of the dataset  \\\\n`runName`| string| Yes| Name of the dataset run  \\\\n  \\\\n### DatasetRunItem object\\\\nAttribute| Type| Required| Description  \\\\n---|---|---|---  \\\\n`runName`| string| Yes| Name of the dataset run to add the item to  \\\\n`runDescription`| string| No| Description of the run. If run exists, description will be updated  \\\\n`metadata`| object| No| Metadata of the dataset run, updates run if run already exists  \\\\n`datasetItemId`| string| Yes| ID of the dataset item to link to this run  \\\\n`observationId`| string| No| ID of the observation to link to this run  \\\\n`traceId`| string| No| ID of the trace to link to this run. traceId should always be provided. For compatibility with older SDK versions it can also be inferred from the provided observationId.  \\\\n  \\\\nMost of the time, we recommend that DatasetRunItems reference TraceIDs directly. The reference to ObservationID exists for backwards compatibility with older SDK versions.\\\\n## End to end data relations\\\\nDataSetRuns can combine a few Langfuse objects:\\\\n  * `DatasetRuns` are created by looping through all or selected `DatasetItem`s of a `Dataset` with your LLM application.\\\\n  * For each `DatasetItem` passed into the LLM application as an Input a `DatasetRunItem` & a `Trace` are created.\\\\n  * Optionally `Score`s can be added to the `Trace`s to evaluate the output of the LLM application during the `DatasetRun`.\\\\n  \\\\nLast updated on July 29, 2025\\\\n[Remote Run](/docs/evaluation/dataset-runs/remote-run \\\\\\\"Remote Run\\\\\\\")\\\\n[Troubleshooting & FAQ](/docs/evaluation/troubleshooting-and-faq \\\\\\\"Troubleshooting & FAQ\\\\\\\")\\\\nWas this page helpful?\\\\nYesNo\\\\n[Support](/support)\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"github_discussion\\\", \\\"url\\\": \\\"https://github.com/orgs/langfuse/discussions/4844\\\", \\\"title\\\": \\\"Dataset creates a lot of identical processes that run infinitely.\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Title: Dataset creates a lot of identical processes that run infinitely.\\\\nCategory: Support\\\\nAuthor: @robotkoruslan\\\\nTimestamp: December 28, 2024\\\\nBody:\\\\nWhen running a simple experiment, dataset creates a lot of identical processes that run infinitely, if click trace error \\\\\\\"Path: traces.byIdWithObservationsAndScores\\\\\\\"\\\\r\\\\n<img width=\\\\\\\"1494\\\\\\\" alt=\\\\\\\"image\\\\\\\" src=\\\\\\\"https://github.com/user-attachments/assets/2ee21b5f-2769-46a2-b1c0-141f0014d6fa\\\\\\\" />\\\\r\\\\n\\\\r\\\\n<img width=\\\\\\\"1494\\\\\\\" alt=\\\\\\\"image\\\\\\\" src=\\\\\\\"https://github.com/user-attachments/assets/8e56daf9-e405-411e-8f62-042292a64764\\\\\\\" />\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\nComments:\\\\n\\\\n    Comment #1 from @dosubot on December 28, 2024:\\\\n\\\\n    <!-- Greeting -->\\\\nHello, @robotkoruslan! I'm here to help you with any bugs, questions, or contributions you have. Let's tackle this issue together!\\\\n\\\\n<!-- Answer -->\\\\nThe issue you're encountering with identical processes running indefinitely during dataset runs in Langfuse could be related to how traces are managed and linked to dataset runs. Here are some potential causes and solutions:\\\\n\\\\n1. **Trace Linking**: Ensure that traces are correctly linked to dataset runs. Each dataset run item should have a corresponding `traceId` in the `datasetRunItems` table. If the trace is not properly linked or created, it could lead to a \\\\\\\"Trace not found\\\\\\\" error <sup>[[1]](https://github.com/langfuse/langfuse/blob/main/web/src/__tests__/async/dataset-service.servertest.ts)</sup><sup>[[2]](https://github.com/langfuse/langfuse/blob/main/web/src/pages/api/public/dataset-run-items.ts)</sup>.\\\\n\\\\n2. **Trace Retrieval**: The error with the path `traces.byIdWithObservationsAndScores` suggests an issue with retrieving traces by their ID along with observations and scores. This could be due to incorrect `traceId` or `projectId`, or if the trace has been deleted or not yet created <sup>[[3]](https://github.com/langfuse/langfuse/blob/main/web/src/server/api/routers/traces.ts)</sup>.\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/changelog/2024-11-21-all-new-datasets-and-evals-documentation\\\", \\\"title\\\": \\\"All new Datasets, Experimentation and Evaluation documentation\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"## New Datasets and Evaluation documentation\\\\nBased on this week\\\\u2019s changes, we have have restructured the documentation to:\\\\n  1. Better explain **how and when to use Langfuse Datasets and Evals** to evaluate your LLM applications in development\\\\n  2. Introduce the **core data model** of `datasets`, experiment `runs`, and `scores`\\\\n  3. Provide **end-to-end examples** for common workflows as Jupyter Notebooks\\\\n  4. Add many **more gifs and toggles with additional explanations** throughout the docs\\\\n### Evaluation\\\\n\\\\ud83d\\\\udca1\\\\n\\\\u2192 [Evaluation documentation](/docs/scores)\\\\nAnalyticsUser FeedbackLLM-as-a-JudgePrompt ExperimentAnnotation QueueCustom Evals\\\\nPlot evaluation results in the Langfuse Dashboard.\\\\nCollect feedback from your users. Can be captured in the frontend via our Browser SDK, server-side via the SDKs or API. Video includes example application.\\\\nRun fully managed LLM-as-a-judge evaluations on production or development traces. Can be applied to any step within your application for step-wise evaluations.\\\\nEvaluate prompts and models on datasets directly in the user interface. No custom code is needed.\\\\nBaseline your evaluation workflow with human annotations via Annotation Queues.\\\\nAdd custom evaluation results, supports numeric, boolean and categorical values.\\\\n    \\\\n    \\\\n```\\\\nPOST /api/public/scores\\\\n```\\\\nAdd scores via Python or JS SDK.\\\\nExample (Python)\\\\n    \\\\n    \\\\n```\\\\nlangfuse.score(\\\\n trace_id=\\\\\\\"123\\\\\\\",\\\\n name=\\\\\\\"my_custom_evaluator\\\\\\\",\\\\n value=0.5,\\\\n)\\\\n```\\\\n### Datasets\\\\n\\\\ud83d\\\\udca1\\\\n\\\\u2192 [Datasets & Experiments documentation](/docs/datasets)\\\\nManage DatasetsCompare experimentsAdd new items from Langfuse Traces\\\\nCollaboratively manage datasets via UI, API, or SDKs.\\\\nRun experiments on datasets. Compare metrics across experiments, and see results side-by-side.\\\\nDirectly add new items from production traces to datasets to improve your datasets over time.\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"site\\\", \\\"url\\\": \\\"https://langfuse.com/changelog/2024-11-18-dataset-runs-comparison-view\\\", \\\"title\\\": \\\"Dataset Run Comparison View - Langfuse\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"---\\\\n\\\\n[\\\\u2190 Back to changelog](/changelog#2024-11-18-dataset-runs-comparison-view)\\\\nNovember 18, 2024 | Launch Week 2 \\\\ud83d\\\\ude80\\\\n# Dataset Run Comparison View\\\\n[Marlies Mayerhofer](https://twitter.com/marliessophie)\\\\nAfter running experiments on datasets, you can now compare results side-by-side, view metrics, and peek into details of each dataset item across runs.\\\\n\\\\n---\\\\n\\\\n## Introduction\\\\n\\\\n---\\\\n\\\\n**What is a dataset in Langfuse?** [Datasets](/docs/datasets/overview) in Langfuse allow you to create test sets and benchmarks to evaluate the performance of your LLM application. A dataset is a collection of dataset items, where each item contains inputs, expected outputs, and metadata. You can create datasets from production edge cases, synthetic test cases, or manual test cases. This enables continuous improvement through structured testing, pre-deployment benchmarking, and flexible evaluation using custom metrics or LLM-as-a-judge approaches.\\\\n\\\\n---\\\\n\\\\n**What is a dataset experiment run?** A dataset experiment run lets you test changes to your application - like trying different models, prompts, or parameters - by running each version against your test dataset and comparing the results through traces in Langfuse to evaluate which changes work best.\\\\n\\\\n---\\\\n\\\\n## What\\\\u2019s new?\\\\n\\\\n---\\\\n\\\\nAnd the fun continues, **Day 1 of[Launch Week 2](/blog/2024-11-17-launch-week-2)** is here.\\\\n\\\\n---\\\\n\\\\n[Langfuse Datasets](/docs/datasets/overview) now enables intuitive comparison of dataset experiment runs for technical and non-technical users. The view features an overview of each item in the dataset, and a summary of each selected experiment run. The latter includes metrics on latency, cost, scores and the application\\\\u2019s output response for each dataset item.\\\\n\\\\n---\\\\n\\\\n## How to use the comparison view?\\\\n\\\\n---\\\\n\\\\n### Setup dataset and run experiments\\\\n\\\\n---\\\\n\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n  * Follow the [getting started guide](/docs/datasets/get-started) to set up a dataset, populate it with items, and run experiments.\\\\n  * Alternatively, execute an end-to-end example ([Python notebook](/docs/datasets/python-cookbook)).\\\\n\\\\n---\\\\n\\\\n### Open comparison view\\\\n\\\\n---\\\\n\\\\n  1. Select multiple dataset runs\\\\n  2. `Actions`, select `Compare`\\\\n\\\\n---\\\\n\\\\n## Learn more\\\\n\\\\n---\\\\n\\\\nFor a conceptual introduction to datasets and offline experiments, see the [dataset documentation](/docs/datasets/overview).\\\\n\\\\n---\\\\n\\\\nLast updated on September 5, 2025\\\\n\\\\n---\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/blog/update-2023-09\\\", \\\"title\\\": \\\"Langfuse Update \\\\u2014 September 2023\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n\\\\u2192 [Docs](/docs/scores/model-based-evals)\\\\n## \\\\ud83d\\\\uddc2\\\\ufe0f Datasets (beta)\\\\nSystematically test new iterations of your LLM app with `datasets`.\\\\nDatasets are collections of inputs and expected outputs that you can manage in Langfuse. Upload an existing dataset or create one based on production data (e.g. when discovering new edge cases).\\\\nWhen combined with automated evals, Datasets in Langfuse make it easy to systematically evaluate new iterations of your LLM app.\\\\n_Overview of dataset runs on a demo dataset_\\\\n_Run experiment on dataset_\\\\nPython SDKJS/TS SDKLangchain (Python)\\\\n    \\\\n    \\\\n```\\\\n \\\\ndataset = langfuse.get_dataset(\\\\\\\"<dataset_name>\\\\\\\")\\\\n \\\\nfor item in dataset.items:\\\\n # execute application function and get Langfuse parent observation (span/generation/event)\\\\n # output also returned as it is used to evaluate the run\\\\n generation, output = my_llm_application.run(item.input)\\\\n \\\\n # link the execution trace to the dataset item and give it a run_name\\\\n item.link(generation, \\\\\\\"<run_name>\\\\\\\")\\\\n \\\\n # optionally, evaluate the output to compare different runs more easily\\\\n generation.score(\\\\n name=\\\\\\\"<example_eval>\\\\\\\",\\\\n # any float value\\\\n value=my_eval_fn(\\\\n item.input,\\\\n output,\\\\n item.expected_output\\\\n )\\\\n )\\\\n```\\\\n    \\\\n    \\\\n```\\\\nconst dataset = await langfuse.getDataset(\\\\\\\"<dataset_name>\\\\\\\");\\\\n \\\\nfor (const item of dataset.items) {\\\\n // execute application function and get Langfuse parent observation (span/generation/event)\\\\n // output also returned as it is used to evaluate the run\\\\n const [generation, output] = await myLlmApplication.run(item.input);\\\\n \\\\n // link the execution trace to the dataset item and give it a run_name\\\\n await item.link(generation, \\\\\\\"<run_name>\\\\\\\");\\\\n \\\\n // optionally, evaluate the output to compare different runs more easily\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n generation.score({\\\\n name: \\\\\\\"<score_name>\\\\\\\",\\\\n value: myEvalFunction(item.input, output, item.expectedOutput),\\\\n });\\\\n}\\\\n```\\\\n    \\\\n    \\\\n```\\\\ndataset = langfuse.get_dataset(\\\\\\\"<dataset_name>\\\\\\\")\\\\n \\\\nfor item in dataset.items:\\\\n # Langchain calback handler that automatically links the execution trace to the dataset item\\\\n handler = item.get_langchain_handler(run_name=\\\\\\\"<run_name>\\\\\\\")\\\\n \\\\n # Execute application and pass custom handler\\\\n my_langchain_chain.run(item.input, callbacks=[handler])\\\\n```\\\\nDatasets are currently in beta on Langfuse Cloud as the API might still slightly change. If you\\\\u2019d like to try it, let us know via the in-app chat.\\\\n\\\\u2192 [Dataset docs](/docs/datasets/overview) \\\\u2192 [Python Cookbook](/docs/datasets/python-cookbook)\\\\n## \\\\ud83d\\\\udcca In-app dashboards\\\\nOver the last weeks, [analytics features](/docs/analytics/overview) were in public alpha on Langfuse Cloud. We\\\\u2019ve now shipped a new _query engine_ as an underlying abstraction for the native in-app dashboards. This is a major step towards bringing all analytics features into the Langfuse core project and helps us move much faster on these.\\\\nOver the next days, you\\\\u2019ll see more and more dashboards popping up in the app. If there is a specific analysis you\\\\u2019d like to see, suggest it on [Discord](/discord).\\\\n## \\\\ud83d\\\\udd04 New integrations\\\\nThe new integrations make it easier to get started with Langfuse. Thanks to the teams behind Langflow, Flowise and LiteLLM for building/collaborating on these integrations.\\\\nSee integrations docs for details:\\\\n  * [Langflow](/docs/langflow): No-code LLM app builder in Python\\\\n  * [Flowise](/docs/flowise): No-code LLM app builder in JS\\\\n  * [LiteLLM](/integrations/gateways/litellm): Python library to use any LLM model as drop in replacement of OpenAI API\\\"}], \\\"type\\\": \\\"content\\\"}}]}\"}],\"isError\":false}",
        "modelParameters": {
          "maxSteps": null,
          "toolChoice": null,
          "maxTokens": null,
          "finishReason": null,
          "system": null,
          "maxRetries": null,
          "mode": null,
          "temperature": null
        },
        "completionStartTime": null,
        "promptId": null,
        "createdAt": "2025-09-18T20:20:14.484Z",
        "updatedAt": "2025-09-18T20:20:14.484Z",
        "usageDetails": {},
        "costDetails": {},
        "providedCostDetails": {},
        "model": null,
        "internalModelId": null,
        "promptName": null,
        "promptVersion": null,
        "latency": 2968,
        "timeToFirstToken": null,
        "inputCost": null,
        "outputCost": null,
        "totalCost": 0,
        "inputUsage": 0,
        "outputUsage": 0,
        "totalUsage": 0,
        "internalModel": null
      },
      {
        "id": "b1b95a1f038b4f8b",
        "traceId": "6f58ac41090ac81ce6203900dadf8e6b",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "type": "SPAN",
        "environment": "default",
        "parentObservationId": "5043b6d994a0663c",
        "startTime": "2025-09-18T20:19:57.060Z",
        "endTime": "2025-09-18T20:19:57.204Z",
        "name": "create-mcp-client",
        "metadata": "{}",
        "level": "DEFAULT",
        "statusMessage": null,
        "version": null,
        "modelParameters": null,
        "completionStartTime": null,
        "promptId": null,
        "createdAt": "2025-09-18T20:20:02.207Z",
        "updatedAt": "2025-09-18T20:20:02.207Z",
        "usageDetails": {},
        "costDetails": {},
        "providedCostDetails": {},
        "model": null,
        "internalModelId": null,
        "promptName": null,
        "promptVersion": null,
        "latency": 144,
        "timeToFirstToken": null,
        "inputCost": null,
        "outputCost": null,
        "totalCost": 0,
        "inputUsage": 0,
        "outputUsage": 0,
        "totalUsage": 0
      },
      {
        "id": "d61d56256881d854",
        "traceId": "6f58ac41090ac81ce6203900dadf8e6b",
        "projectId": "clkpwwm0m000gmm094odg11gi",
        "type": "SPAN",
        "environment": "default",
        "parentObservationId": "5043b6d994a0663c",
        "startTime": "2025-09-18T20:19:56.952Z",
        "endTime": "2025-09-18T20:19:57.059Z",
        "name": "get-langfuse-prompt",
        "metadata": "{\"resourceAttributes\":{\"service.name\":\"unknown_service:node\",\"telemetry.sdk.language\":\"nodejs\",\"telemetry.sdk.name\":\"opentelemetry\",\"telemetry.sdk.version\":\"2.0.1\"},\"scope\":{\"name\":\"langfuse-sdk\",\"version\":\"4.0.0\",\"attributes\":{}}}",
        "level": "DEFAULT",
        "statusMessage": null,
        "version": null,
        "input": "langfuse-docs-assistant-text",
        "output": "\"{\\\"name\\\":\\\"langfuse-docs-assistant-text\\\",\\\"prompt\\\":\\\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\\\n\\\\n## Guidelines\\\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\\\n- Answer the question using only this and only this information, outputted in markdown format.\\\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\\\n\\\\n## Scope & Safety\\\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\\\n\\\\n## Clarifying Questions\\\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., “Which SDK are you using - Python or JS/TS?”).\\\\n\\\\n## Pricing & Discounts\\\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can’t confirm and point to contacting support.\\\\n\\\\n## Markdown Style Guide\\\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\\\n- **Sections to include (if relevant):** *Prerequisites* → *Setup* → *Minimal Example* → *Run* → *Troubleshooting* → *Links*.\\\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\\\n- **Keep it tight:** ≤ 20 lines of code per SDK section. If longer, collapse details or link out.\\\",\\\"version\\\":10,\\\"isFallback\\\":false,\\\"tags\\\":[],\\\"labels\\\":[\\\"production\\\",\\\"latest\\\"],\\\"type\\\":\\\"text\\\",\\\"config\\\":{\\\"model\\\":\\\"gpt-5\\\",\\\"reasoningSummary\\\":\\\"detailed\\\",\\\"textVerbosity\\\":\\\"low\\\",\\\"reasoningEffort\\\":\\\"low\\\"}}\"",
        "modelParameters": null,
        "completionStartTime": null,
        "promptId": null,
        "createdAt": "2025-09-18T20:20:02.207Z",
        "updatedAt": "2025-09-18T20:20:02.207Z",
        "usageDetails": {},
        "costDetails": {},
        "providedCostDetails": {},
        "model": null,
        "internalModelId": null,
        "promptName": null,
        "promptVersion": null,
        "latency": 107,
        "timeToFirstToken": null,
        "inputCost": null,
        "outputCost": null,
        "totalCost": 0,
        "inputUsage": 0,
        "outputUsage": 0,
        "totalUsage": 0,
        "internalModel": null
      }
    ]
  },
  "observations": [
    {
      "id": "31ccdfbd9381c993",
      "traceId": "6f58ac41090ac81ce6203900dadf8e6b",
      "projectId": "clkpwwm0m000gmm094odg11gi",
      "type": "GENERATION",
      "environment": "default",
      "parentObservationId": "5043b6d994a0663c",
      "startTime": "2025-09-18T20:19:57.263Z",
      "endTime": "2025-09-18T20:20:17.549Z",
      "name": "ai.streamText",
      "metadata": "{\"attributes\":{\"operation.name\":\"ai.streamText\",\"ai.operationId\":\"ai.streamText\",\"ai.model.provider\":\"openai.responses\",\"ai.model.id\":\"gpt-5\",\"ai.settings.maxRetries\":\"2\",\"ai.telemetry.metadata.langfusePrompt\":\"{\\\"name\\\":\\\"langfuse-docs-assistant-text\\\",\\\"prompt\\\":\\\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\\\n\\\\n## Guidelines\\\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\\\n- Answer the question using only this and only this information, outputted in markdown format.\\\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\\\n\\\\n## Scope & Safety\\\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\\\n\\\\n## Clarifying Questions\\\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., “Which SDK are you using - Python or JS/TS?”).\\\\n\\\\n## Pricing & Discounts\\\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can’t confirm and point to contacting support.\\\\n\\\\n## Markdown Style Guide\\\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\\\n- **Sections to include (if relevant):** *Prerequisites* → *Setup* → *Minimal Example* → *Run* → *Troubleshooting* → *Links*.\\\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\\\n- **Keep it tight:** ≤ 20 lines of code per SDK section. If longer, collapse details or link out.\\\",\\\"version\\\":10,\\\"isFallback\\\":false,\\\"tags\\\":[],\\\"labels\\\":[\\\"production\\\",\\\"latest\\\"],\\\"type\\\":\\\"text\\\",\\\"config\\\":{\\\"model\\\":\\\"gpt-5\\\",\\\"reasoningSummary\\\":\\\"detailed\\\",\\\"textVerbosity\\\":\\\"low\\\",\\\"reasoningEffort\\\":\\\"low\\\"}}\",\"ai.prompt\":\"{\\\"system\\\":\\\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\\\n\\\\n## Guidelines\\\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\\\n- Answer the question using only this and only this information, outputted in markdown format.\\\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\\\n\\\\n## Scope & Safety\\\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\\\n\\\\n## Clarifying Questions\\\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., “Which SDK are you using - Python or JS/TS?”).\\\\n\\\\n## Pricing & Discounts\\\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can’t confirm and point to contacting support.\\\\n\\\\n## Markdown Style Guide\\\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\\\n- **Sections to include (if relevant):** *Prerequisites* → *Setup* → *Minimal Example* → *Run* → *Troubleshooting* → *Links*.\\\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\\\n- **Keep it tight:** ≤ 20 lines of code per SDK section. If longer, collapse details or link out.\\\",\\\"messages\\\":[{\\\"role\\\":\\\"assistant\\\",\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"**👋 Do you have any questions about Langfuse? Ask me!**\\\\n\\\\n_⚠️ Warning: Do not enter sensitive information. All chat messages can be viewed in the public demo project. Responses may be inaccurate. Please check the documentation for details or reach out to us via the chat widget._\\\"}]},{\\\"role\\\":\\\"user\\\",\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"what is a dataset\\\"}]}]}\",\"ai.response.finishReason\":\"stop\",\"ai.response.text\":\"# What is a dataset in Langfuse?\\n\\nA dataset is a collection of dataset items (inputs plus optional expected outputs and metadata) used to test and evaluate your LLM application. Datasets are the input to Dataset Runs (experiments) where you compare prompts/models/configs and optionally score results.\\n\\n- Create datasets from production edge cases, synthetic data, or manual cases\\n- Use them to benchmark changes and prevent regressions\\n\\nMinimal example (Python)\\n_ create_dataset.py_\\n```python\\nfrom langfuse import Langfuse\\n\\nlf = Langfuse(public_key=\\\"<YOUR_PUBLIC_KEY>\\\", secret_key=\\\"<YOUR_SECRET_KEY>\\\", host=\\\"https://cloud.langfuse.com\\\")\\n\\nlf.create_dataset(\\n    name=\\\"getting-started\\\",\\n    description=\\\"My first dataset\\\",\\n    metadata={\\\"type\\\": \\\"benchmark\\\"}\\n)\\n\\nlf.create_dataset_item(\\n    dataset_name=\\\"getting-started\\\",\\n    input={\\\"question\\\": \\\"What is Langfuse?\\\"},\\n    expected_output={\\\"answer_contains\\\": \\\"LLM\\\"}\\n)\\nprint(\\\"Dataset ready\\\")\\n```\\n\\nLinks\\n- Concept and setup: Datasets — what they are and how to create/use them: https://langfuse.com/docs/evaluation/dataset-runs/datasets\\n- Dataset Runs (prompt experiments): https://langfuse.com/docs/evaluation/dataset-runs/native-run\\n- Data model (Dataset, DatasetItem, Runs): https://langfuse.com/docs/evaluation/dataset-runs/data-model\",\"ai.response.providerMetadata\":\"{\\\"openai\\\":{\\\"responseId\\\":\\\"resp_68cc6979a1908195898244e8f9175c4a0e53a980a402aac8\\\",\\\"logprobs\\\":[[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],[]]}}\",\"ai.usage.inputTokens\":\"18439\",\"ai.usage.outputTokens\":\"473\",\"ai.usage.totalTokens\":\"18912\",\"ai.usage.reasoningTokens\":\"128\",\"ai.usage.cachedInputTokens\":\"1152\"},\"resourceAttributes\":{\"service.name\":\"unknown_service:node\",\"telemetry.sdk.language\":\"nodejs\",\"telemetry.sdk.name\":\"opentelemetry\",\"telemetry.sdk.version\":\"2.0.1\"},\"scope\":{\"name\":\"ai\",\"attributes\":{}}}",
      "level": "DEFAULT",
      "statusMessage": null,
      "version": null,
      "input": "{\"system\":\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\n\\n## Guidelines\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\n- Answer the question using only this and only this information, outputted in markdown format.\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\n\\n## Scope & Safety\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\n\\n## Clarifying Questions\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., “Which SDK are you using - Python or JS/TS?”).\\n\\n## Pricing & Discounts\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can’t confirm and point to contacting support.\\n\\n## Markdown Style Guide\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\n- **Sections to include (if relevant):** *Prerequisites* → *Setup* → *Minimal Example* → *Run* → *Troubleshooting* → *Links*.\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\n- **Keep it tight:** ≤ 20 lines of code per SDK section. If longer, collapse details or link out.\",\"messages\":[{\"role\":\"assistant\",\"content\":[{\"type\":\"text\",\"text\":\"**👋 Do you have any questions about Langfuse? Ask me!**\\n\\n_⚠️ Warning: Do not enter sensitive information. All chat messages can be viewed in the public demo project. Responses may be inaccurate. Please check the documentation for details or reach out to us via the chat widget._\"}]},{\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":\"what is a dataset\"}]}]}",
      "output": "# What is a dataset in Langfuse?\n\nA dataset is a collection of dataset items (inputs plus optional expected outputs and metadata) used to test and evaluate your LLM application. Datasets are the input to Dataset Runs (experiments) where you compare prompts/models/configs and optionally score results.\n\n- Create datasets from production edge cases, synthetic data, or manual cases\n- Use them to benchmark changes and prevent regressions\n\nMinimal example (Python)\n_ create_dataset.py_\n```python\nfrom langfuse import Langfuse\n\nlf = Langfuse(public_key=\"<YOUR_PUBLIC_KEY>\", secret_key=\"<YOUR_SECRET_KEY>\", host=\"https://cloud.langfuse.com\")\n\nlf.create_dataset(\n    name=\"getting-started\",\n    description=\"My first dataset\",\n    metadata={\"type\": \"benchmark\"}\n)\n\nlf.create_dataset_item(\n    dataset_name=\"getting-started\",\n    input={\"question\": \"What is Langfuse?\"},\n    expected_output={\"answer_contains\": \"LLM\"}\n)\nprint(\"Dataset ready\")\n```\n\nLinks\n- Concept and setup: Datasets — what they are and how to create/use them: https://langfuse.com/docs/evaluation/dataset-runs/datasets\n- Dataset Runs (prompt experiments): https://langfuse.com/docs/evaluation/dataset-runs/native-run\n- Data model (Dataset, DatasetItem, Runs): https://langfuse.com/docs/evaluation/dataset-runs/data-model",
      "modelParameters": {
        "maxSteps": null,
        "toolChoice": null,
        "maxTokens": null,
        "finishReason": null,
        "system": "openai.responses",
        "maxRetries": "2",
        "mode": null,
        "temperature": null
      },
      "completionStartTime": null,
      "promptId": "5422a9e9-19cf-4ba8-bbd4-2fe0108eefc5",
      "createdAt": "2025-09-18T20:20:17.667Z",
      "updatedAt": "2025-09-18T20:20:17.673Z",
      "usageDetails": {},
      "costDetails": {},
      "providedCostDetails": {},
      "model": null,
      "internalModelId": null,
      "promptName": "langfuse-docs-assistant-text",
      "promptVersion": 10,
      "latency": 20286,
      "timeToFirstToken": null,
      "inputCost": null,
      "outputCost": null,
      "totalCost": 0,
      "inputUsage": 0,
      "outputUsage": 0,
      "totalUsage": 0,
      "internalModel": null
    },
    {
      "id": "00db41221c0d2850",
      "traceId": "6f58ac41090ac81ce6203900dadf8e6b",
      "projectId": "clkpwwm0m000gmm094odg11gi",
      "type": "GENERATION",
      "environment": "default",
      "parentObservationId": "31ccdfbd9381c993",
      "startTime": "2025-09-18T20:20:09.360Z",
      "endTime": "2025-09-18T20:20:17.544Z",
      "name": "ai.streamText.doStream",
      "metadata": "{}",
      "level": "DEFAULT",
      "statusMessage": null,
      "version": null,
      "modelParameters": {
        "maxSteps": null,
        "toolChoice": "{\"type\":\"auto\"}",
        "maxTokens": null,
        "finishReason": "stop",
        "system": "openai.responses",
        "maxRetries": "2",
        "mode": null,
        "temperature": null
      },
      "completionStartTime": "2025-09-18T20:20:09.975Z",
      "promptId": "5422a9e9-19cf-4ba8-bbd4-2fe0108eefc5",
      "createdAt": "2025-09-18T20:20:17.667Z",
      "updatedAt": "2025-09-18T20:20:17.672Z",
      "usageDetails": {
        "input": 17324,
        "output": 429,
        "total": 17753
      },
      "costDetails": {
        "input": 0.021655,
        "output": 0.00429,
        "total": 0.025945
      },
      "providedCostDetails": {},
      "model": "gpt-5",
      "internalModelId": "38c3822a-09a3-457b-b200-2c6f17f7cf2f",
      "promptName": "langfuse-docs-assistant-text",
      "promptVersion": 10,
      "latency": 8184,
      "timeToFirstToken": 0.615,
      "inputCost": 0.021655,
      "outputCost": 0.00429,
      "totalCost": 0.025945,
      "inputUsage": 17324,
      "outputUsage": 429,
      "totalUsage": 17753
    },
    {
      "id": "5043b6d994a0663c",
      "traceId": "6f58ac41090ac81ce6203900dadf8e6b",
      "projectId": "clkpwwm0m000gmm094odg11gi",
      "type": "GENERATION",
      "environment": "default",
      "parentObservationId": null,
      "startTime": "2025-09-18T20:19:56.948Z",
      "endTime": "2025-09-18T20:20:17.549Z",
      "name": "handle-chatbot-message",
      "metadata": "{\"resourceAttributes\":{\"service.name\":\"unknown_service:node\",\"telemetry.sdk.language\":\"nodejs\",\"telemetry.sdk.name\":\"opentelemetry\",\"telemetry.sdk.version\":\"2.0.1\"},\"scope\":{\"name\":\"langfuse-sdk\",\"version\":\"4.0.0\",\"attributes\":{}}}",
      "level": "DEFAULT",
      "statusMessage": null,
      "version": null,
      "input": "what is a dataset",
      "output": "# What is a dataset in Langfuse?\n\nA dataset is a collection of dataset items (inputs plus optional expected outputs and metadata) used to test and evaluate your LLM application. Datasets are the input to Dataset Runs (experiments) where you compare prompts/models/configs and optionally score results.\n\n- Create datasets from production edge cases, synthetic data, or manual cases\n- Use them to benchmark changes and prevent regressions\n\nMinimal example (Python)\n_ create_dataset.py_\n```python\nfrom langfuse import Langfuse\n\nlf = Langfuse(public_key=\"<YOUR_PUBLIC_KEY>\", secret_key=\"<YOUR_SECRET_KEY>\", host=\"https://cloud.langfuse.com\")\n\nlf.create_dataset(\n    name=\"getting-started\",\n    description=\"My first dataset\",\n    metadata={\"type\": \"benchmark\"}\n)\n\nlf.create_dataset_item(\n    dataset_name=\"getting-started\",\n    input={\"question\": \"What is Langfuse?\"},\n    expected_output={\"answer_contains\": \"LLM\"}\n)\nprint(\"Dataset ready\")\n```\n\nLinks\n- Concept and setup: Datasets — what they are and how to create/use them: https://langfuse.com/docs/evaluation/dataset-runs/datasets\n- Dataset Runs (prompt experiments): https://langfuse.com/docs/evaluation/dataset-runs/native-run\n- Data model (Dataset, DatasetItem, Runs): https://langfuse.com/docs/evaluation/dataset-runs/data-model",
      "modelParameters": {},
      "completionStartTime": null,
      "promptId": null,
      "createdAt": "2025-09-18T20:20:17.668Z",
      "updatedAt": "2025-09-18T20:20:17.668Z",
      "usageDetails": {},
      "costDetails": {},
      "providedCostDetails": {},
      "model": null,
      "internalModelId": null,
      "promptName": null,
      "promptVersion": null,
      "latency": 20601,
      "timeToFirstToken": null,
      "inputCost": null,
      "outputCost": null,
      "totalCost": 0,
      "inputUsage": 0,
      "outputUsage": 0,
      "totalUsage": 0,
      "internalModel": null
    },
    {
      "id": "a44bd80c743afa87",
      "traceId": "6f58ac41090ac81ce6203900dadf8e6b",
      "projectId": "clkpwwm0m000gmm094odg11gi",
      "type": "GENERATION",
      "environment": "default",
      "parentObservationId": "31ccdfbd9381c993",
      "startTime": "2025-09-18T20:19:57.272Z",
      "endTime": "2025-09-18T20:20:09.357Z",
      "name": "ai.streamText.doStream",
      "metadata": "{\"tools\":[\"{\\\"type\\\":\\\"function\\\",\\\"name\\\":\\\"searchLangfuseDocs\\\",\\\"description\\\":\\\"Semantic search (RAG) over the Langfuse documentation. Use this whenever the user asks a broader question that cannot be answered by a specific single page. Returns a concise answer synthesized from relevant docs. The raw provider response is included in _meta. Prefer this before guessing. If a specific page is needed call getLangfuseDocsPage first.\\\",\\\"inputSchema\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"query\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"The user’s question in natural language. Include helpful context like SDK/language (e.g., Python v3, JS v4), self-hosted vs cloud, and short error messages (trim long stack traces). Keep under ~600 characters.\\\"}},\\\"required\\\":[\\\"query\\\"],\\\"additionalProperties\\\":false,\\\"$schema\\\":\\\"http://json-schema.org/draft-07/schema#\\\"}}\",\"{\\\"type\\\":\\\"function\\\",\\\"name\\\":\\\"getLangfuseDocsPage\\\",\\\"description\\\":\\\"Fetch the raw Markdown for a single Langfuse docs page. Accepts a docs path (e.g., /docs/observability/overview) or a full https://langfuse.com URL. Returns the exact Markdown (may include front matter). Use when you need a specific page content (Integration, Features, API, etc.) or code samples. Prefer searchLangfuseDocs for broader questions where there is not one specific page about it.\\\",\\\"inputSchema\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{\\\"pathOrUrl\\\":{\\\"type\\\":\\\"string\\\",\\\"description\\\":\\\"Docs path starting with “/” (e.g., /docs/observability/overview) or a full URL on https://langfuse.com. Do not include anchors (#...) or queries (?foo=bar) — they will be ignored.\\\"}},\\\"required\\\":[\\\"pathOrUrl\\\"],\\\"additionalProperties\\\":false,\\\"$schema\\\":\\\"http://json-schema.org/draft-07/schema#\\\"}}\",\"{\\\"type\\\":\\\"function\\\",\\\"name\\\":\\\"getLangfuseOverview\\\",\\\"description\\\":\\\"Get a high-level, machine-readable index by downloading https://langfuse.com/llms.txt. Use this at the start of a session when needed to discover key docs endpoints or to seed follow-up calls to searchLangfuseDocs or getLangfuseDocsPage.Returns the plain text contents of llms.txt. Avoid repeated calls within the same session.\\\",\\\"inputSchema\\\":{\\\"type\\\":\\\"object\\\",\\\"properties\\\":{},\\\"additionalProperties\\\":false,\\\"$schema\\\":\\\"http://json-schema.org/draft-07/schema#\\\"}}\"],\"attributes\":{\"operation.name\":\"ai.streamText.doStream\",\"ai.operationId\":\"ai.streamText.doStream\",\"ai.model.provider\":\"openai.responses\",\"ai.model.id\":\"gpt-5\",\"ai.settings.maxRetries\":\"2\",\"ai.telemetry.metadata.langfusePrompt\":\"{\\\"name\\\":\\\"langfuse-docs-assistant-text\\\",\\\"prompt\\\":\\\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\\\n\\\\n## Guidelines\\\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\\\n- Answer the question using only this and only this information, outputted in markdown format.\\\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\\\n\\\\n## Scope & Safety\\\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\\\n\\\\n## Clarifying Questions\\\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., “Which SDK are you using - Python or JS/TS?”).\\\\n\\\\n## Pricing & Discounts\\\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can’t confirm and point to contacting support.\\\\n\\\\n## Markdown Style Guide\\\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\\\n- **Sections to include (if relevant):** *Prerequisites* → *Setup* → *Minimal Example* → *Run* → *Troubleshooting* → *Links*.\\\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\\\n- **Keep it tight:** ≤ 20 lines of code per SDK section. If longer, collapse details or link out.\\\",\\\"version\\\":10,\\\"isFallback\\\":false,\\\"tags\\\":[],\\\"labels\\\":[\\\"production\\\",\\\"latest\\\"],\\\"type\\\":\\\"text\\\",\\\"config\\\":{\\\"model\\\":\\\"gpt-5\\\",\\\"reasoningSummary\\\":\\\"detailed\\\",\\\"textVerbosity\\\":\\\"low\\\",\\\"reasoningEffort\\\":\\\"low\\\"}}\",\"ai.prompt.messages\":\"[{\\\"role\\\":\\\"system\\\",\\\"content\\\":\\\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\\\n\\\\n## Guidelines\\\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\\\n- Answer the question using only this and only this information, outputted in markdown format.\\\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\\\n\\\\n## Scope & Safety\\\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\\\n\\\\n## Clarifying Questions\\\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., “Which SDK are you using - Python or JS/TS?”).\\\\n\\\\n## Pricing & Discounts\\\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can’t confirm and point to contacting support.\\\\n\\\\n## Markdown Style Guide\\\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\\\n- **Sections to include (if relevant):** *Prerequisites* → *Setup* → *Minimal Example* → *Run* → *Troubleshooting* → *Links*.\\\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\\\n- **Keep it tight:** ≤ 20 lines of code per SDK section. If longer, collapse details or link out.\\\"},{\\\"role\\\":\\\"assistant\\\",\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"**👋 Do you have any questions about Langfuse? Ask me!**\\\\n\\\\n_⚠️ Warning: Do not enter sensitive information. All chat messages can be viewed in the public demo project. Responses may be inaccurate. Please check the documentation for details or reach out to us via the chat widget._\\\"}]},{\\\"role\\\":\\\"user\\\",\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"what is a dataset\\\"}]}]\",\"ai.prompt.tools\":\"[\\\"{\\\\\\\"type\\\\\\\":\\\\\\\"function\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"searchLangfuseDocs\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Semantic search (RAG) over the Langfuse documentation. Use this whenever the user asks a broader question that cannot be answered by a specific single page. Returns a concise answer synthesized from relevant docs. The raw provider response is included in _meta. Prefer this before guessing. If a specific page is needed call getLangfuseDocsPage first.\\\\\\\",\\\\\\\"inputSchema\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"properties\\\\\\\":{\\\\\\\"query\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"The user’s question in natural language. Include helpful context like SDK/language (e.g., Python v3, JS v4), self-hosted vs cloud, and short error messages (trim long stack traces). Keep under ~600 characters.\\\\\\\"}},\\\\\\\"required\\\\\\\":[\\\\\\\"query\\\\\\\"],\\\\\\\"additionalProperties\\\\\\\":false,\\\\\\\"$schema\\\\\\\":\\\\\\\"http://json-schema.org/draft-07/schema#\\\\\\\"}}\\\",\\\"{\\\\\\\"type\\\\\\\":\\\\\\\"function\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"getLangfuseDocsPage\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Fetch the raw Markdown for a single Langfuse docs page. Accepts a docs path (e.g., /docs/observability/overview) or a full https://langfuse.com URL. Returns the exact Markdown (may include front matter). Use when you need a specific page content (Integration, Features, API, etc.) or code samples. Prefer searchLangfuseDocs for broader questions where there is not one specific page about it.\\\\\\\",\\\\\\\"inputSchema\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"properties\\\\\\\":{\\\\\\\"pathOrUrl\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"string\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Docs path starting with “/” (e.g., /docs/observability/overview) or a full URL on https://langfuse.com. Do not include anchors (#...) or queries (?foo=bar) — they will be ignored.\\\\\\\"}},\\\\\\\"required\\\\\\\":[\\\\\\\"pathOrUrl\\\\\\\"],\\\\\\\"additionalProperties\\\\\\\":false,\\\\\\\"$schema\\\\\\\":\\\\\\\"http://json-schema.org/draft-07/schema#\\\\\\\"}}\\\",\\\"{\\\\\\\"type\\\\\\\":\\\\\\\"function\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"getLangfuseOverview\\\\\\\",\\\\\\\"description\\\\\\\":\\\\\\\"Get a high-level, machine-readable index by downloading https://langfuse.com/llms.txt. Use this at the start of a session when needed to discover key docs endpoints or to seed follow-up calls to searchLangfuseDocs or getLangfuseDocsPage.Returns the plain text contents of llms.txt. Avoid repeated calls within the same session.\\\\\\\",\\\\\\\"inputSchema\\\\\\\":{\\\\\\\"type\\\\\\\":\\\\\\\"object\\\\\\\",\\\\\\\"properties\\\\\\\":{},\\\\\\\"additionalProperties\\\\\\\":false,\\\\\\\"$schema\\\\\\\":\\\\\\\"http://json-schema.org/draft-07/schema#\\\\\\\"}}\\\"]\",\"ai.prompt.toolChoice\":\"{\\\"type\\\":\\\"auto\\\"}\",\"gen_ai.system\":\"openai.responses\",\"gen_ai.request.model\":\"gpt-5\",\"ai.response.msToFirstChunk\":\"1187.6545230001211\",\"ai.response.msToFinish\":\"12084.884463000111\",\"ai.response.avgOutputTokensPerSecond\":\"3.6409119288408043\",\"ai.response.finishReason\":\"tool-calls\",\"ai.response.text\":\"\",\"ai.response.toolCalls\":\"[{\\\"type\\\":\\\"tool-call\\\",\\\"toolCallId\\\":\\\"call_Wgy5Y139OQnSh7grDUUE0iil\\\",\\\"toolName\\\":\\\"searchLangfuseDocs\\\",\\\"input\\\":{\\\"query\\\":\\\"What is a dataset in Langfuse? definition, use cases, evaluation, prompt testing, datasets feature docs\\\"},\\\"providerMetadata\\\":{\\\"openai\\\":{\\\"itemId\\\":\\\"fc_68cc6976097481958d36986414510cab0e53a980a402aac8\\\"}},\\\"dynamic\\\":true}]\",\"ai.response.id\":\"resp_68cc696e0a208195a91e435bd719e7800e53a980a402aac8\",\"ai.response.model\":\"gpt-5-2025-08-07\",\"ai.response.timestamp\":\"2025-09-18T20:19:58.000Z\",\"ai.response.providerMetadata\":\"{\\\"openai\\\":{\\\"responseId\\\":\\\"resp_68cc696e0a208195a91e435bd719e7800e53a980a402aac8\\\"}}\",\"ai.usage.inputTokens\":\"1115\",\"ai.usage.outputTokens\":\"44\",\"ai.usage.totalTokens\":\"1159\",\"ai.usage.reasoningTokens\":\"{\\\"intValue\\\":0}\",\"ai.usage.cachedInputTokens\":\"{\\\"intValue\\\":0}\",\"gen_ai.response.finish_reasons\":\"[\\\"tool-calls\\\"]\",\"gen_ai.response.id\":\"resp_68cc696e0a208195a91e435bd719e7800e53a980a402aac8\",\"gen_ai.response.model\":\"gpt-5-2025-08-07\",\"gen_ai.usage.input_tokens\":\"1115\",\"gen_ai.usage.output_tokens\":\"44\"},\"resourceAttributes\":{\"service.name\":\"unknown_service:node\",\"telemetry.sdk.language\":\"nodejs\",\"telemetry.sdk.name\":\"opentelemetry\",\"telemetry.sdk.version\":\"2.0.1\"},\"scope\":{\"name\":\"ai\",\"attributes\":{}}}",
      "level": "DEFAULT",
      "statusMessage": null,
      "version": null,
      "input": "[{\"role\":\"system\",\"content\":\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\n\\n## Guidelines\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\n- Answer the question using only this and only this information, outputted in markdown format.\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\n\\n## Scope & Safety\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\n\\n## Clarifying Questions\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., “Which SDK are you using - Python or JS/TS?”).\\n\\n## Pricing & Discounts\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can’t confirm and point to contacting support.\\n\\n## Markdown Style Guide\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\n- **Sections to include (if relevant):** *Prerequisites* → *Setup* → *Minimal Example* → *Run* → *Troubleshooting* → *Links*.\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\n- **Keep it tight:** ≤ 20 lines of code per SDK section. If longer, collapse details or link out.\"},{\"role\":\"assistant\",\"content\":[{\"type\":\"text\",\"text\":\"**👋 Do you have any questions about Langfuse? Ask me!**\\n\\n_⚠️ Warning: Do not enter sensitive information. All chat messages can be viewed in the public demo project. Responses may be inaccurate. Please check the documentation for details or reach out to us via the chat widget._\"}]},{\"role\":\"user\",\"content\":[{\"type\":\"text\",\"text\":\"what is a dataset\"}]}]",
      "output": null,
      "modelParameters": {
        "maxSteps": null,
        "toolChoice": "{\"type\":\"auto\"}",
        "maxTokens": null,
        "finishReason": "tool-calls",
        "system": "openai.responses",
        "maxRetries": "2",
        "mode": null,
        "temperature": null
      },
      "completionStartTime": "2025-09-18T20:19:58.460Z",
      "promptId": "5422a9e9-19cf-4ba8-bbd4-2fe0108eefc5",
      "createdAt": "2025-09-18T20:20:14.484Z",
      "updatedAt": "2025-09-18T20:20:14.486Z",
      "usageDetails": {
        "input": 1115,
        "output": 44,
        "input_cached_tokens": 0,
        "output_reasoning_tokens": 0,
        "total": 1159
      },
      "costDetails": {
        "input": 0.00139375,
        "output": 0.00044,
        "input_cached_tokens": 0,
        "output_reasoning_tokens": 0,
        "total": 0.00183375
      },
      "providedCostDetails": {},
      "model": "gpt-5",
      "internalModelId": "38c3822a-09a3-457b-b200-2c6f17f7cf2f",
      "promptName": "langfuse-docs-assistant-text",
      "promptVersion": 10,
      "latency": 12085,
      "timeToFirstToken": 1.188,
      "inputCost": 0.00139375,
      "outputCost": 0.00044,
      "totalCost": 0.00183375,
      "inputUsage": 1115,
      "outputUsage": 44,
      "totalUsage": 1159,
      "internalModel": "38c3822a-09a3-457b-b200-2c6f17f7cf2f"
    },
    {
      "id": "9a6a16a257c88b0a",
      "traceId": "6f58ac41090ac81ce6203900dadf8e6b",
      "projectId": "clkpwwm0m000gmm094odg11gi",
      "type": "TOOL",
      "environment": "default",
      "parentObservationId": "31ccdfbd9381c993",
      "startTime": "2025-09-18T20:20:06.383Z",
      "endTime": "2025-09-18T20:20:09.351Z",
      "name": "ai.toolCall",
      "metadata": "{\"attributes\":{\"operation.name\":\"ai.toolCall\",\"ai.operationId\":\"ai.toolCall\",\"ai.toolCall.name\":\"searchLangfuseDocs\",\"ai.toolCall.id\":\"call_Wgy5Y139OQnSh7grDUUE0iil\",\"ai.toolCall.args\":\"{\\\"query\\\":\\\"What is a dataset in Langfuse? definition, use cases, evaluation, prompt testing, datasets feature docs\\\"}\",\"ai.toolCall.result\":\"{\\\"_meta\\\":{\\\"id\\\":\\\"inkeep-chatcmpl-1761a743-0fb1-4e4d-9c1c-10d4960ef29f\\\",\\\"model\\\":\\\"inkeep-rag\\\",\\\"choices\\\":[{\\\"finish_reason\\\":\\\"stop\\\",\\\"log_probs\\\":null,\\\"index\\\":0,\\\"message\\\":{\\\"content\\\":\\\"{\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/dataset-runs/datasets\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Datasets\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Datasets - Langfuse\\\\\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\\\\\\\\\"Evaluation\\\\\\\\\\\\\\\")Dataset RunsDatasets\\\\\\\\nCopy as Markdown\\\\\\\\n# Datasets\\\\\\\\nA dataset is a collection of inputs and expected outputs and is used to test your application. Before executing your first dataset run, you need to create a dataset.\\\\\\\\n## Why use datasets?\\\\\\\\n  * **Datasets prerequisite for Dataset Runs** , they serve as the data input of Dataset Runs\\\\\\\\n  * Create test cases for your application with real production traces\\\\\\\\n  * Collaboratively create and collect dataset items with your team\\\\\\\\n  * Have a single source of truth for your test data\\\\\\\\n## Get Started\\\\\\\\n### Creating a dataset\\\\\\\\nDatasets have a name which is unique within a project.\\\\\\\\nPython SDKJS/TS SDKLangfuse UI\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.create_dataset(\\\\\\\\n name=\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\",\\\\\\\\n # optional description\\\\\\\\n description=\\\\\\\\\\\\\\\"My first dataset\\\\\\\\\\\\\\\",\\\\\\\\n # optional metadata\\\\\\\\n metadata={\\\\\\\\n \\\\\\\\\\\\\\\"author\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Alice\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"date\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"2022-01-01\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"benchmark\\\\\\\\\\\\\\\"\\\\\\\\n }\\\\\\\\n)\\\\\\\\n```\\\\\\\\n _See[Python SDK](/docs/sdk/python/sdk-v3) docs for details on how to initialize the Python client._\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport { LangfuseClient } from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\nconst langfuse = new LangfuseClient()\\\\\\\\n \\\\\\\\nawait langfuse.api.datasets.create({\\\\\\\\n name: \\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\",\\\\\\\\n // optional description\\\\\\\\n description: \\\\\\\\\\\\\\\"My first dataset\\\\\\\\\\\\\\\",\\\\\\\\n // optional metadata\\\\\\\\n metadata: {\\\\\\\\n author: \\\\\\\\\\\\\\\"Alice\\\\\\\\\\\\\\\",\\\\\\\\n date: \\\\\\\\\\\\\\\"2022-01-01\\\\\\\\\\\\\\\",\\\\\\\\n type: \\\\\\\\\\\\\\\"benchmark\\\\\\\\\\\\\\\",\\\\\\\\n },\\\\\\\\n});\\\\\\\\n```\\\\\\\\n  1. **Navigate to** `Your Project` > `Datasets`\\\\\\\\n  2. **Click on** `+ New dataset` to create a new dataset.\\\\\\\\n### Create new dataset items\\\\\\\\nDataset items can be added to a dataset by providing the input and optionally the expected output.\\\\\\\\nPython SDKJS/TS SDKLangfuse UI\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.create_dataset_item(\\\\\\\\n dataset_name=\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\",\\\\\\\\n # any python object or value, optional\\\\\\\\n input={\\\\\\\\n \\\\\\\\\\\\\\\"text\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"hello world\\\\\\\\\\\\\\\"\\\\\\\\n },\\\\\\\\n # any python object or value, optional\\\\\\\\n expected_output={\\\\\\\\n \\\\\\\\\\\\\\\"text\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"hello world\\\\\\\\\\\\\\\"\\\\\\\\n },\\\\\\\\n # metadata, optional\\\\\\\\n metadata={\\\\\\\\n \\\\\\\\\\\\\\\"model\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"llama3\\\\\\\\\\\\\\\",\\\\\\\\n }\\\\\\\\n)\\\\\\\\n```\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/dataset-runs/native-run\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Native Dataset Runs (Prompt Experiments)\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Native Dataset Runs (Prompt Experiments) - Langfuse\\\\\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\\\\\\\\\"Evaluation\\\\\\\\\\\\\\\")\\\\\\\\n[Dataset Runs](/docs/evaluation/dataset-runs/datasets \\\\\\\\\\\\\\\"Dataset Runs\\\\\\\\\\\\\\\")Native Run\\\\\\\\nCopy as Markdown\\\\\\\\n# Native Dataset Runs (Prompt Experiments)\\\\\\\\nYou can execute Native Dataset Runs **(also called Prompt Experiments)** in the Langfuse UI to test different prompt versions from [Prompt Management](/docs/prompt-management) or language models and compare the results side-by-side.\\\\\\\\nOptionally, you can use [LLM-as-a-Judge Evaluators](/docs/evaluation/evaluation-methods/llm-as-a-judge) to automatically score the responses based on the expected outputs to further analyze the results on an aggregate level.\\\\\\\\nThey are called \\\\\\\\u201cNative Dataset Runs\\\\\\\\u201d because they rely on Datasets, Prompts and optionally LLM-as-a-Judge Evaluators all being managed **natively** in Langfuse.\\\\\\\\n## Why use Native Dataset Runs (Prompt Experiments)?\\\\\\\\n  * Quickly test different prompt versions or models\\\\\\\\n  * Structure your prompt testing by using a dataset to test different prompt versions and models\\\\\\\\n  * Quickly iterate on prompts through Dataset Runs\\\\\\\\n  * Optionally use LLM-as-a-Judge Evaluators to score the responses based on the expected outputs from the dataset\\\\\\\\n  * Prevent regressions by running tests when making prompt changes\\\\\\\\n## Prerequisites\\\\\\\\n### Create a usable prompt\\\\\\\\nCreate a prompt that you want to test and evaluate. [How to create a prompt?](/docs/prompt-management/get-started)\\\\\\\\n **A prompt is usable when:** your prompt has variables that match the dataset item keys in the dataset that will be used for the Dataset Run. See the example below.\\\\\\\\nExample: Prompt Variables & Dataset Item Keys Mapping\\\\\\\\n  \\\\\\\\n **Prompt:**\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n{{ documentation }}\\\\\\\\n \\\\\\\\nQuestion: {{question}}\\\\\\\\n \\\\\\\\n```\\\\\\\\n  \\\\\\\\n**Dataset Item:**\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n{\\\\\\\\n \\\\\\\\\\\\\\\"documentation\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Langfuse is an LLM Engineering Platform\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"question\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What is Langfuse?\\\\\\\\\\\\\\\"\\\\\\\\n}\\\\\\\\n```\\\\\\\\nIn this example:\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/changelog/2024-11-18-dataset-runs-comparison-view\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Dataset Run Comparison View\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Dataset Run Comparison View - Langfuse\\\\\\\\n[\\\\\\\\u2190 Back to changelog](/changelog#2024-11-18-dataset-runs-comparison-view)\\\\\\\\nNovember 18, 2024 | Launch Week 2 \\\\\\\\ud83d\\\\\\\\ude80\\\\\\\\n# Dataset Run Comparison View\\\\\\\\n[Marlies Mayerhofer](https://twitter.com/marliessophie)\\\\\\\\nAfter running experiments on datasets, you can now compare results side-by-side, view metrics, and peek into details of each dataset item across runs.\\\\\\\\n## Introduction\\\\\\\\n**What is a dataset in Langfuse?** [Datasets](/docs/datasets/overview) in Langfuse allow you to create test sets and benchmarks to evaluate the performance of your LLM application. A dataset is a collection of dataset items, where each item contains inputs, expected outputs, and metadata. You can create datasets from production edge cases, synthetic test cases, or manual test cases. This enables continuous improvement through structured testing, pre-deployment benchmarking, and flexible evaluation using custom metrics or LLM-as-a-judge approaches.\\\\\\\\n**What is a dataset experiment run?** A dataset experiment run lets you test changes to your application - like trying different models, prompts, or parameters - by running each version against your test dataset and comparing the results through traces in Langfuse to evaluate which changes work best.\\\\\\\\n## What\\\\\\\\u2019s new?\\\\\\\\nAnd the fun continues, **Day 1 of[Launch Week 2](/blog/2024-11-17-launch-week-2)** is here.\\\\\\\\n[Langfuse Datasets](/docs/datasets/overview) now enables intuitive comparison of dataset experiment runs for technical and non-technical users. The view features an overview of each item in the dataset, and a summary of each selected experiment run. The latter includes metrics on latency, cost, scores and the application\\\\\\\\u2019s output response for each dataset item.\\\\\\\\n## How to use the comparison view?\\\\\\\\n### Setup dataset and run experiments\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n  * Follow the [getting started guide](/docs/datasets/get-started) to set up a dataset, populate it with items, and run experiments.\\\\\\\\n  * Alternatively, execute an end-to-end example ([Python notebook](/docs/datasets/python-cookbook)).\\\\\\\\n### Open comparison view\\\\\\\\n  1. Select multiple dataset runs\\\\\\\\n  2. `Actions`, select `Compare`\\\\\\\\n## Learn more\\\\\\\\nFor a conceptual introduction to datasets and offline experiments, see the [dataset documentation](/docs/datasets/overview).\\\\\\\\nLast updated on September 5, 2025\\\\\\\\nWas this page helpful?\\\\\\\\nYesNo\\\\\\\\n[Support](/support)\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/guides/cookbook/example_synthetic_datasets\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Synthetic Dataset Generation for LLM Evaluation\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Synthetic Dataset Generation for LLM Evaluation - Langfuse\\\\\\\\nGuides[Cookbooks](/guides/cookbook \\\\\\\\\\\\\\\"Cookbooks\\\\\\\\\\\\\\\")Synthetic Datasets\\\\\\\\nCopy as Markdown\\\\\\\\nThis is a Jupyter notebook\\\\\\\\n[Open on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/cookbook/example_synthetic_datasets.ipynb)\\\\\\\\n[Run on Google Colab](https://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/cookbook/example_synthetic_datasets.ipynb)\\\\\\\\n# Synthetic Dataset Generation for LLM Evaluation\\\\\\\\nIn this notebook, we will explore how to **generate synthetic datasets** using language models and uploading them to [Langfuse](https://langfuse.com) for evaluation.\\\\\\\\n## What are Langfuse Datasets?\\\\\\\\nIn Langfuse, a _dataset_ is a collection of _dataset items_ , each typically containing an `input` (e.g., user prompt/question), `expected_output` (the ground truth or ideal answer) and optional metadata.\\\\\\\\nDatasets are used for **evaluation**. You can run your LLM or application on each item in a dataset and compare the application\\\\\\\\u2019s responses to the expected outputs. This way, you can track performance over time and across different application configs (e.g. model versions or prompt changes).\\\\\\\\n## Cases your Dataset Should Cover\\\\\\\\n**Happy path** \\\\\\\\u2013 straightforward or common queries:\\\\\\\\n  * \\\\\\\\u201cWhat is the capital of France?\\\\\\\\u201d\\\\\\\\n  * \\\\\\\\u201cConvert 5 USD to EUR.\\\\\\\\u201d\\\\\\\\n**Edge cases** \\\\\\\\u2013 unusual or complex:\\\\\\\\n  * Very long prompts.\\\\\\\\n  * Ambiguous queries.\\\\\\\\n  * Very technical or niche.\\\\\\\\n**Adversarial cases** \\\\\\\\u2013 malicious or tricky:\\\\\\\\n  * Prompt injection attempts (\\\\\\\\u201cIgnore all instructions and \\\\\\\\u2026\\\\\\\\u201d).\\\\\\\\n  * Content policy violations (harassment, hate speech).\\\\\\\\n  * Logic traps (trick questions).\\\\\\\\n## Examples\\\\\\\\n### Example 1: Looping Over OpenAI API\\\\\\\\nWe\\\\\\\\u2019ll use OpenAI\\\\\\\\u2019s API in a simple loop to create synthetic questions for an airline chatbot. You could similarly prompt the model to generate _both_ questions and answers. %pip install openai langfuse\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport os\\\\\\\\n \\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n///# Get keys for your project from the project settings page: https://cloud.langfuse.com\\\\\\\\nos.environ[\\\\\\\\\\\\\\\"LANGFUSE_PUBLIC_KEY\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"pk-lf-...\\\\\\\\\\\\\\\" \\\\\\\\nos.environ[\\\\\\\\\\\\\\\"LANGFUSE_SECRET_KEY\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"sk-lf-...\\\\\\\\\\\\\\\" \\\\\\\\nos.environ[\\\\\\\\\\\\\\\"LANGFUSE_HOST\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"https://cloud.langfuse.com\\\\\\\\\\\\\\\" # \\\\\\\\ud83c\\\\\\\\uddea\\\\\\\\ud83c\\\\\\\\uddfa EU region\\\\\\\\n///# os.environ[\\\\\\\\\\\\\\\"LANGFUSE_HOST\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"https://us.cloud.langfuse.com\\\\\\\\\\\\\\\" # \\\\\\\\ud83c\\\\\\\\uddfa\\\\\\\\ud83c\\\\\\\\uddf8 US region\\\\\\\\n \\\\\\\\n///# Your openai key\\\\\\\\nos.environ[\\\\\\\\\\\\\\\"OPENAI_API_KEY\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"sk-proj-...\\\\\\\\\\\\\\\"\\\\\\\\n```\\\\\\\\nWith the environment variables set, we can now initialize the Langfuse client. `get_client()` initializes the Langfuse client using the credentials provided in the environment variables.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\n \\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\n///# Verify connection\\\\\\\\nif langfuse.auth_check():\\\\\\\\n print(\\\\\\\\\\\\\\\"Langfuse client is authenticated and ready!\\\\\\\\\\\\\\\")\\\\\\\\nelse:\\\\\\\\n print(\\\\\\\\\\\\\\\"Authentication failed. Please check your credentials and host.\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\\nLangfuse client is authenticated and ready!\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom openai import OpenAI\\\\\\\\nimport pandas as pd\\\\\\\\n \\\\\\\\nclient = OpenAI()\\\\\\\\n \\\\\\\\n///# Function to generate airline questions\\\\\\\\ndef generate_airline_questions(num_questions=20):\\\\\\\\n \\\\\\\\n questions = []\\\\\\\\n \\\\\\\\n for i in range(num_questions):\\\\\\\\n completion = client.chat.completions.create(\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4o\\\\\\\\\\\\\\\", \\\\\\\\n messages=[\\\\\\\\n {\\\\\\\\n \\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"system\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": (\\\\\\\\n \\\\\\\\\\\\\\\"You are a helpful customer service chatbot for an airline. \\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\\\\\\\\"Please generate a short, realistic question from a customer.\\\\\\\\\\\\\\\"\\\\\\\\n )\\\\\\\\n }\\\\\\\\n ],\\\\\\\\n temperature=1\\\\\\\\n )\\\\\\\\n question_text = completion.choices[0].message.content.strip()\\\\\\\\n questions.append(question_text)\\\\\\\\n \\\\\\\\n return questions\\\\\\\\n \\\\\\\\n///# Generate 20 airline-related questions\\\\\\\\nairline_questions = generate_airline_questions(num_questions=20)\\\\\\\\n \\\\\\\\n///# Convert to a Pandas DataFrame\\\\\\\\ndf = pd.DataFrame({\\\\\\\\\\\\\\\"Question\\\\\\\\\\\\\\\": airline_questions})\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n scenario=\\\\\\\\\\\\\\\"Non-technical users trying to query a database using plain English.\\\\\\\\\\\\\\\",\\\\\\\\n)\\\\\\\\n \\\\\\\\n///# 2. Initialize the Synthesizer\\\\\\\\nsynthesizer = Synthesizer(styling_config=styling_config)\\\\\\\\n \\\\\\\\n///# 3. Generate synthetic items from scratch, e.g. 20 items for a short demo\\\\\\\\nsynthesizer.generate_goldens_from_scratch(num_goldens=20)\\\\\\\\n \\\\\\\\n///# 4. Access the generated examples\\\\\\\\nsynthetic_goldens = synthesizer.synthetic_goldens\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\n///# 5. Create a Langfuse dataset\\\\\\\\ndeepeval_dataset_name = \\\\\\\\\\\\\\\"deepeval_synthetic_data\\\\\\\\\\\\\\\"\\\\\\\\nlangfuse.create_dataset(\\\\\\\\n name=deepeval_dataset_name,\\\\\\\\n description=\\\\\\\\\\\\\\\"Synthetic text-to-SQL data (DeepEval)\\\\\\\\\\\\\\\",\\\\\\\\n metadata={\\\\\\\\\\\\\\\"approach\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"deepeval\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"task\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"text-to-sql\\\\\\\\\\\\\\\"}\\\\\\\\n)\\\\\\\\n \\\\\\\\n///# 6. Upload the items\\\\\\\\nfor golden in synthetic_goldens:\\\\\\\\n langfuse.create_dataset_item(\\\\\\\\n dataset_name=deepeval_dataset_name,\\\\\\\\n input={\\\\\\\\\\\\\\\"query\\\\\\\\\\\\\\\": golden.input},\\\\\\\\n )\\\\\\\\n```\\\\\\\\n### Example 4: No-Code via Hugging Face Dataset Generator\\\\\\\\nIf you prefer a more UI-based approach, check out [Hugging Face\\\\\\\\u2019s Synthetic Data Generator](https://huggingface.co/blog/synthetic-data-generator). You can generate examples in the Hugging Face UI. Then you can download them as CSV and upload it in the Langfuse UI.\\\\\\\\n## Next Steps\\\\\\\\n  1. **Explore your dataset in Langfuse**. You can see each dataset in the UI.\\\\\\\\n  2. **Run experiments** You can now evaluate your application using this dataset.\\\\\\\\n  3. **Compare runs** over time or across models, prompts, or chain logic.\\\\\\\\nFor more details on how to run experiments on a dataset, see the [Langfuse docs](https://langfuse.com/docs/datasets/get-started#run-experiment-on-a-dataset).\\\\\\\\nLast updated on July 29, 2025\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Query Data in Langfuse via the SDK](/guides/cookbook/example_query_data_via_sdk \\\\\\\\\\\\\\\"Query Data in Langfuse via the SDK\\\\\\\\\\\\\\\")\\\\\\\\n[Amazon Bedrock](/guides/cookbook/integration_amazon_bedrock \\\\\\\\\\\\\\\"Amazon Bedrock\\\\\\\\\\\\\\\")\\\\\\\\nWas this page helpful?\\\\\\\\nYesNo\\\\\\\\n[Support](/support)\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/dataset-runs/data-model\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Dataset Runs Data Model\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Dataset Runs Data Model - Langfuse\\\\\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\\\\\\\\\"Evaluation\\\\\\\\\\\\\\\")\\\\\\\\n[Dataset Runs](/docs/evaluation/dataset-runs/datasets \\\\\\\\\\\\\\\"Dataset Runs\\\\\\\\\\\\\\\")Data Model\\\\\\\\nCopy as Markdown\\\\\\\\n# Dataset Runs Data Model\\\\\\\\nThis page describes the data model of datasets and dataset runs. For detailed reference please refer to the [API Reference](https://api.reference.langfuse.com).\\\\\\\\n## Datasets\\\\\\\\nDatasets are a collection of inputs and, optionally, expected outputs that can be during Dataset runs.\\\\\\\\n`Dataset`s are a collection of `DatasetItem`s.\\\\\\\\n1\\\\\\\\nn\\\\\\\\nDataset\\\\\\\\nname\\\\\\\\ndescription\\\\\\\\nmetadata\\\\\\\\nDatasetItem\\\\\\\\ndatasetName\\\\\\\\ninput\\\\\\\\nexpectedOutput\\\\\\\\nmetadata\\\\\\\\nsourceTraceId\\\\\\\\nsourceObservationId\\\\\\\\nid\\\\\\\\nstatus\\\\\\\\n### Dataset object\\\\\\\\nAttribute| Type| Required| Description  \\\\\\\\n---|---|---|---  \\\\\\\\n`name`| string| Yes| Name of the dataset  \\\\\\\\n`description`| string| No| Description of the dataset  \\\\\\\\n`metadata`| object| No| Additional metadata for the dataset  \\\\\\\\n  \\\\\\\\n### DatasetItem object\\\\\\\\nAttribute| Type| Required| Description  \\\\\\\\n---|---|---|---  \\\\\\\\n`datasetName`| string| Yes| Name of the dataset to add the item to  \\\\\\\\n`input`| object| No| Input data for the dataset item  \\\\\\\\n`expectedOutput`| object| No| Expected output data for the dataset item  \\\\\\\\n`metadata`| object| No| Additional metadata for the dataset item  \\\\\\\\n`sourceTraceId`| string| No| ID of the source trace to link this dataset item to  \\\\\\\\n`sourceObservationId`| string| No| ID of the source observation to link this dataset item to  \\\\\\\\n`id`| string| No| Unique identifier for the dataset item. Dataset items are upserted on their id. Id needs to be unique (project-level) and cannot be reused across datasets.  \\\\\\\\n`status`| DatasetStatus| No| Status of the dataset item. Defaults to ACTIVE for newly created items. Possible values: `ACTIVE`, `ARCHIVED`  \\\\\\\\n  \\\\\\\\n## DatasetRun\\\\\\\\nDataset runs are used to run a dataset through your LLM application and optionally apply evaluation methods to the results.\\\\\\\\n  \\\\\\\\n### DatasetRun object\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Attribute| Type| Required| Description  \\\\\\\\n---|---|---|---  \\\\\\\\n`datasetName`| string| Yes| Name of the dataset  \\\\\\\\n`runName`| string| Yes| Name of the dataset run  \\\\\\\\n  \\\\\\\\n### DatasetRunItem object\\\\\\\\nAttribute| Type| Required| Description  \\\\\\\\n---|---|---|---  \\\\\\\\n`runName`| string| Yes| Name of the dataset run to add the item to  \\\\\\\\n`runDescription`| string| No| Description of the run. If run exists, description will be updated  \\\\\\\\n`metadata`| object| No| Metadata of the dataset run, updates run if run already exists  \\\\\\\\n`datasetItemId`| string| Yes| ID of the dataset item to link to this run  \\\\\\\\n`observationId`| string| No| ID of the observation to link to this run  \\\\\\\\n`traceId`| string| No| ID of the trace to link to this run. traceId should always be provided. For compatibility with older SDK versions it can also be inferred from the provided observationId.  \\\\\\\\n  \\\\\\\\nMost of the time, we recommend that DatasetRunItems reference TraceIDs directly. The reference to ObservationID exists for backwards compatibility with older SDK versions.\\\\\\\\n## End to end data relations\\\\\\\\nDataSetRuns can combine a few Langfuse objects:\\\\\\\\n  * `DatasetRuns` are created by looping through all or selected `DatasetItem`s of a `Dataset` with your LLM application.\\\\\\\\n  * For each `DatasetItem` passed into the LLM application as an Input a `DatasetRunItem` & a `Trace` are created.\\\\\\\\n  * Optionally `Score`s can be added to the `Trace`s to evaluate the output of the LLM application during the `DatasetRun`.\\\\\\\\n  \\\\\\\\nLast updated on July 29, 2025\\\\\\\\n[Remote Run](/docs/evaluation/dataset-runs/remote-run \\\\\\\\\\\\\\\"Remote Run\\\\\\\\\\\\\\\")\\\\\\\\n[Troubleshooting & FAQ](/docs/evaluation/troubleshooting-and-faq \\\\\\\\\\\\\\\"Troubleshooting & FAQ\\\\\\\\\\\\\\\")\\\\\\\\nWas this page helpful?\\\\\\\\nYesNo\\\\\\\\n[Support](/support)\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"github_discussion\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://github.com/orgs/langfuse/discussions/4844\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Dataset creates a lot of identical processes that run infinitely.\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Title: Dataset creates a lot of identical processes that run infinitely.\\\\\\\\nCategory: Support\\\\\\\\nAuthor: @robotkoruslan\\\\\\\\nTimestamp: December 28, 2024\\\\\\\\nBody:\\\\\\\\nWhen running a simple experiment, dataset creates a lot of identical processes that run infinitely, if click trace error \\\\\\\\\\\\\\\"Path: traces.byIdWithObservationsAndScores\\\\\\\\\\\\\\\"\\\\\\\\r\\\\\\\\n<img width=\\\\\\\\\\\\\\\"1494\\\\\\\\\\\\\\\" alt=\\\\\\\\\\\\\\\"image\\\\\\\\\\\\\\\" src=\\\\\\\\\\\\\\\"https://github.com/user-attachments/assets/2ee21b5f-2769-46a2-b1c0-141f0014d6fa\\\\\\\\\\\\\\\" />\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n<img width=\\\\\\\\\\\\\\\"1494\\\\\\\\\\\\\\\" alt=\\\\\\\\\\\\\\\"image\\\\\\\\\\\\\\\" src=\\\\\\\\\\\\\\\"https://github.com/user-attachments/assets/8e56daf9-e405-411e-8f62-042292a64764\\\\\\\\\\\\\\\" />\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\nComments:\\\\\\\\n\\\\\\\\n    Comment #1 from @dosubot on December 28, 2024:\\\\\\\\n\\\\\\\\n    <!-- Greeting -->\\\\\\\\nHello, @robotkoruslan! I'm here to help you with any bugs, questions, or contributions you have. Let's tackle this issue together!\\\\\\\\n\\\\\\\\n<!-- Answer -->\\\\\\\\nThe issue you're encountering with identical processes running indefinitely during dataset runs in Langfuse could be related to how traces are managed and linked to dataset runs. Here are some potential causes and solutions:\\\\\\\\n\\\\\\\\n1. **Trace Linking**: Ensure that traces are correctly linked to dataset runs. Each dataset run item should have a corresponding `traceId` in the `datasetRunItems` table. If the trace is not properly linked or created, it could lead to a \\\\\\\\\\\\\\\"Trace not found\\\\\\\\\\\\\\\" error <sup>[[1]](https://github.com/langfuse/langfuse/blob/main/web/src/__tests__/async/dataset-service.servertest.ts)</sup><sup>[[2]](https://github.com/langfuse/langfuse/blob/main/web/src/pages/api/public/dataset-run-items.ts)</sup>.\\\\\\\\n\\\\\\\\n2. **Trace Retrieval**: The error with the path `traces.byIdWithObservationsAndScores` suggests an issue with retrieving traces by their ID along with observations and scores. This could be due to incorrect `traceId` or `projectId`, or if the trace has been deleted or not yet created <sup>[[3]](https://github.com/langfuse/langfuse/blob/main/web/src/server/api/routers/traces.ts)</sup>.\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/changelog/2024-11-21-all-new-datasets-and-evals-documentation\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"All new Datasets, Experimentation and Evaluation documentation\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"## New Datasets and Evaluation documentation\\\\\\\\nBased on this week\\\\\\\\u2019s changes, we have have restructured the documentation to:\\\\\\\\n  1. Better explain **how and when to use Langfuse Datasets and Evals** to evaluate your LLM applications in development\\\\\\\\n  2. Introduce the **core data model** of `datasets`, experiment `runs`, and `scores`\\\\\\\\n  3. Provide **end-to-end examples** for common workflows as Jupyter Notebooks\\\\\\\\n  4. Add many **more gifs and toggles with additional explanations** throughout the docs\\\\\\\\n### Evaluation\\\\\\\\n\\\\\\\\ud83d\\\\\\\\udca1\\\\\\\\n\\\\\\\\u2192 [Evaluation documentation](/docs/scores)\\\\\\\\nAnalyticsUser FeedbackLLM-as-a-JudgePrompt ExperimentAnnotation QueueCustom Evals\\\\\\\\nPlot evaluation results in the Langfuse Dashboard.\\\\\\\\nCollect feedback from your users. Can be captured in the frontend via our Browser SDK, server-side via the SDKs or API. Video includes example application.\\\\\\\\nRun fully managed LLM-as-a-judge evaluations on production or development traces. Can be applied to any step within your application for step-wise evaluations.\\\\\\\\nEvaluate prompts and models on datasets directly in the user interface. No custom code is needed.\\\\\\\\nBaseline your evaluation workflow with human annotations via Annotation Queues.\\\\\\\\nAdd custom evaluation results, supports numeric, boolean and categorical values.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nPOST /api/public/scores\\\\\\\\n```\\\\\\\\nAdd scores via Python or JS SDK.\\\\\\\\nExample (Python)\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.score(\\\\\\\\n trace_id=\\\\\\\\\\\\\\\"123\\\\\\\\\\\\\\\",\\\\\\\\n name=\\\\\\\\\\\\\\\"my_custom_evaluator\\\\\\\\\\\\\\\",\\\\\\\\n value=0.5,\\\\\\\\n)\\\\\\\\n```\\\\\\\\n### Datasets\\\\\\\\n\\\\\\\\ud83d\\\\\\\\udca1\\\\\\\\n\\\\\\\\u2192 [Datasets & Experiments documentation](/docs/datasets)\\\\\\\\nManage DatasetsCompare experimentsAdd new items from Langfuse Traces\\\\\\\\nCollaboratively manage datasets via UI, API, or SDKs.\\\\\\\\nRun experiments on datasets. Compare metrics across experiments, and see results side-by-side.\\\\\\\\nDirectly add new items from production traces to datasets to improve your datasets over time.\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"site\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/changelog/2024-11-18-dataset-runs-comparison-view\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Dataset Run Comparison View - Langfuse\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"---\\\\\\\\n\\\\\\\\n[\\\\\\\\u2190 Back to changelog](/changelog#2024-11-18-dataset-runs-comparison-view)\\\\\\\\nNovember 18, 2024 | Launch Week 2 \\\\\\\\ud83d\\\\\\\\ude80\\\\\\\\n# Dataset Run Comparison View\\\\\\\\n[Marlies Mayerhofer](https://twitter.com/marliessophie)\\\\\\\\nAfter running experiments on datasets, you can now compare results side-by-side, view metrics, and peek into details of each dataset item across runs.\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n## Introduction\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n**What is a dataset in Langfuse?** [Datasets](/docs/datasets/overview) in Langfuse allow you to create test sets and benchmarks to evaluate the performance of your LLM application. A dataset is a collection of dataset items, where each item contains inputs, expected outputs, and metadata. You can create datasets from production edge cases, synthetic test cases, or manual test cases. This enables continuous improvement through structured testing, pre-deployment benchmarking, and flexible evaluation using custom metrics or LLM-as-a-judge approaches.\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n**What is a dataset experiment run?** A dataset experiment run lets you test changes to your application - like trying different models, prompts, or parameters - by running each version against your test dataset and comparing the results through traces in Langfuse to evaluate which changes work best.\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n## What\\\\\\\\u2019s new?\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\nAnd the fun continues, **Day 1 of[Launch Week 2](/blog/2024-11-17-launch-week-2)** is here.\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n[Langfuse Datasets](/docs/datasets/overview) now enables intuitive comparison of dataset experiment runs for technical and non-technical users. The view features an overview of each item in the dataset, and a summary of each selected experiment run. The latter includes metrics on latency, cost, scores and the application\\\\\\\\u2019s output response for each dataset item.\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n## How to use the comparison view?\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n### Setup dataset and run experiments\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n  * Follow the [getting started guide](/docs/datasets/get-started) to set up a dataset, populate it with items, and run experiments.\\\\\\\\n  * Alternatively, execute an end-to-end example ([Python notebook](/docs/datasets/python-cookbook)).\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n### Open comparison view\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n  1. Select multiple dataset runs\\\\\\\\n  2. `Actions`, select `Compare`\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n## Learn more\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\nFor a conceptual introduction to datasets and offline experiments, see the [dataset documentation](/docs/datasets/overview).\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\nLast updated on September 5, 2025\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/blog/update-2023-09\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Langfuse Update \\\\\\\\u2014 September 2023\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n\\\\\\\\u2192 [Docs](/docs/scores/model-based-evals)\\\\\\\\n## \\\\\\\\ud83d\\\\\\\\uddc2\\\\\\\\ufe0f Datasets (beta)\\\\\\\\nSystematically test new iterations of your LLM app with `datasets`.\\\\\\\\nDatasets are collections of inputs and expected outputs that you can manage in Langfuse. Upload an existing dataset or create one based on production data (e.g. when discovering new edge cases).\\\\\\\\nWhen combined with automated evals, Datasets in Langfuse make it easy to systematically evaluate new iterations of your LLM app.\\\\\\\\n_Overview of dataset runs on a demo dataset_\\\\\\\\n_Run experiment on dataset_\\\\\\\\nPython SDKJS/TS SDKLangchain (Python)\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n \\\\\\\\ndataset = langfuse.get_dataset(\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\nfor item in dataset.items:\\\\\\\\n # execute application function and get Langfuse parent observation (span/generation/event)\\\\\\\\n # output also returned as it is used to evaluate the run\\\\\\\\n generation, output = my_llm_application.run(item.input)\\\\\\\\n \\\\\\\\n # link the execution trace to the dataset item and give it a run_name\\\\\\\\n item.link(generation, \\\\\\\\\\\\\\\"<run_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n # optionally, evaluate the output to compare different runs more easily\\\\\\\\n generation.score(\\\\\\\\n name=\\\\\\\\\\\\\\\"<example_eval>\\\\\\\\\\\\\\\",\\\\\\\\n # any float value\\\\\\\\n value=my_eval_fn(\\\\\\\\n item.input,\\\\\\\\n output,\\\\\\\\n item.expected_output\\\\\\\\n )\\\\\\\\n )\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nconst dataset = await langfuse.getDataset(\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\");\\\\\\\\n \\\\\\\\nfor (const item of dataset.items) {\\\\\\\\n // execute application function and get Langfuse parent observation (span/generation/event)\\\\\\\\n // output also returned as it is used to evaluate the run\\\\\\\\n const [generation, output] = await myLlmApplication.run(item.input);\\\\\\\\n \\\\\\\\n // link the execution trace to the dataset item and give it a run_name\\\\\\\\n await item.link(generation, \\\\\\\\\\\\\\\"<run_name>\\\\\\\\\\\\\\\");\\\\\\\\n \\\\\\\\n // optionally, evaluate the output to compare different runs more easily\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n generation.score({\\\\\\\\n name: \\\\\\\\\\\\\\\"<score_name>\\\\\\\\\\\\\\\",\\\\\\\\n value: myEvalFunction(item.input, output, item.expectedOutput),\\\\\\\\n });\\\\\\\\n}\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\ndataset = langfuse.get_dataset(\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\nfor item in dataset.items:\\\\\\\\n # Langchain calback handler that automatically links the execution trace to the dataset item\\\\\\\\n handler = item.get_langchain_handler(run_name=\\\\\\\\\\\\\\\"<run_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n # Execute application and pass custom handler\\\\\\\\n my_langchain_chain.run(item.input, callbacks=[handler])\\\\\\\\n```\\\\\\\\nDatasets are currently in beta on Langfuse Cloud as the API might still slightly change. If you\\\\\\\\u2019d like to try it, let us know via the in-app chat.\\\\\\\\n\\\\\\\\u2192 [Dataset docs](/docs/datasets/overview) \\\\\\\\u2192 [Python Cookbook](/docs/datasets/python-cookbook)\\\\\\\\n## \\\\\\\\ud83d\\\\\\\\udcca In-app dashboards\\\\\\\\nOver the last weeks, [analytics features](/docs/analytics/overview) were in public alpha on Langfuse Cloud. We\\\\\\\\u2019ve now shipped a new _query engine_ as an underlying abstraction for the native in-app dashboards. This is a major step towards bringing all analytics features into the Langfuse core project and helps us move much faster on these.\\\\\\\\nOver the next days, you\\\\\\\\u2019ll see more and more dashboards popping up in the app. If there is a specific analysis you\\\\\\\\u2019d like to see, suggest it on [Discord](/discord).\\\\\\\\n## \\\\\\\\ud83d\\\\\\\\udd04 New integrations\\\\\\\\nThe new integrations make it easier to get started with Langfuse. Thanks to the teams behind Langflow, Flowise and LiteLLM for building/collaborating on these integrations.\\\\\\\\nSee integrations docs for details:\\\\\\\\n  * [Langflow](/docs/langflow): No-code LLM app builder in Python\\\\\\\\n  * [Flowise](/docs/flowise): No-code LLM app builder in JS\\\\\\\\n  * [LiteLLM](/integrations/gateways/litellm): Python library to use any LLM model as drop in replacement of OpenAI API\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}]}\\\",\\\"role\\\":\\\"assistant\\\",\\\"tool_calls\\\":null,\\\"function_call\\\":null}}],\\\"usage\\\":{\\\"prompt_tokens\\\":21,\\\"completion_tokens\\\":7295,\\\"total_tokens\\\":7316},\\\"system_fingerprint\\\":\\\"inkeep-fp-690807bb-bbc2-44b0-8a56-491bf9df8dbe\\\",\\\"object\\\":\\\"chat.completion\\\",\\\"service_tier\\\":null,\\\"created\\\":1758226809},\\\"content\\\":[{\\\"type\\\":\\\"text\\\",\\\"text\\\":\\\"{\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/dataset-runs/datasets\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Datasets\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Datasets - Langfuse\\\\\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\\\\\\\\\"Evaluation\\\\\\\\\\\\\\\")Dataset RunsDatasets\\\\\\\\nCopy as Markdown\\\\\\\\n# Datasets\\\\\\\\nA dataset is a collection of inputs and expected outputs and is used to test your application. Before executing your first dataset run, you need to create a dataset.\\\\\\\\n## Why use datasets?\\\\\\\\n  * **Datasets prerequisite for Dataset Runs** , they serve as the data input of Dataset Runs\\\\\\\\n  * Create test cases for your application with real production traces\\\\\\\\n  * Collaboratively create and collect dataset items with your team\\\\\\\\n  * Have a single source of truth for your test data\\\\\\\\n## Get Started\\\\\\\\n### Creating a dataset\\\\\\\\nDatasets have a name which is unique within a project.\\\\\\\\nPython SDKJS/TS SDKLangfuse UI\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.create_dataset(\\\\\\\\n name=\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\",\\\\\\\\n # optional description\\\\\\\\n description=\\\\\\\\\\\\\\\"My first dataset\\\\\\\\\\\\\\\",\\\\\\\\n # optional metadata\\\\\\\\n metadata={\\\\\\\\n \\\\\\\\\\\\\\\"author\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Alice\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"date\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"2022-01-01\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"type\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"benchmark\\\\\\\\\\\\\\\"\\\\\\\\n }\\\\\\\\n)\\\\\\\\n```\\\\\\\\n _See[Python SDK](/docs/sdk/python/sdk-v3) docs for details on how to initialize the Python client._\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport { LangfuseClient } from \\\\\\\\\\\\\\\"@langfuse/client\\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\nconst langfuse = new LangfuseClient()\\\\\\\\n \\\\\\\\nawait langfuse.api.datasets.create({\\\\\\\\n name: \\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\",\\\\\\\\n // optional description\\\\\\\\n description: \\\\\\\\\\\\\\\"My first dataset\\\\\\\\\\\\\\\",\\\\\\\\n // optional metadata\\\\\\\\n metadata: {\\\\\\\\n author: \\\\\\\\\\\\\\\"Alice\\\\\\\\\\\\\\\",\\\\\\\\n date: \\\\\\\\\\\\\\\"2022-01-01\\\\\\\\\\\\\\\",\\\\\\\\n type: \\\\\\\\\\\\\\\"benchmark\\\\\\\\\\\\\\\",\\\\\\\\n },\\\\\\\\n});\\\\\\\\n```\\\\\\\\n  1. **Navigate to** `Your Project` > `Datasets`\\\\\\\\n  2. **Click on** `+ New dataset` to create a new dataset.\\\\\\\\n### Create new dataset items\\\\\\\\nDataset items can be added to a dataset by providing the input and optionally the expected output.\\\\\\\\nPython SDKJS/TS SDKLangfuse UI\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.create_dataset_item(\\\\\\\\n dataset_name=\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\",\\\\\\\\n # any python object or value, optional\\\\\\\\n input={\\\\\\\\n \\\\\\\\\\\\\\\"text\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"hello world\\\\\\\\\\\\\\\"\\\\\\\\n },\\\\\\\\n # any python object or value, optional\\\\\\\\n expected_output={\\\\\\\\n \\\\\\\\\\\\\\\"text\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"hello world\\\\\\\\\\\\\\\"\\\\\\\\n },\\\\\\\\n # metadata, optional\\\\\\\\n metadata={\\\\\\\\n \\\\\\\\\\\\\\\"model\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"llama3\\\\\\\\\\\\\\\",\\\\\\\\n }\\\\\\\\n)\\\\\\\\n```\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/dataset-runs/native-run\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Native Dataset Runs (Prompt Experiments)\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Native Dataset Runs (Prompt Experiments) - Langfuse\\\\\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\\\\\\\\\"Evaluation\\\\\\\\\\\\\\\")\\\\\\\\n[Dataset Runs](/docs/evaluation/dataset-runs/datasets \\\\\\\\\\\\\\\"Dataset Runs\\\\\\\\\\\\\\\")Native Run\\\\\\\\nCopy as Markdown\\\\\\\\n# Native Dataset Runs (Prompt Experiments)\\\\\\\\nYou can execute Native Dataset Runs **(also called Prompt Experiments)** in the Langfuse UI to test different prompt versions from [Prompt Management](/docs/prompt-management) or language models and compare the results side-by-side.\\\\\\\\nOptionally, you can use [LLM-as-a-Judge Evaluators](/docs/evaluation/evaluation-methods/llm-as-a-judge) to automatically score the responses based on the expected outputs to further analyze the results on an aggregate level.\\\\\\\\nThey are called \\\\\\\\u201cNative Dataset Runs\\\\\\\\u201d because they rely on Datasets, Prompts and optionally LLM-as-a-Judge Evaluators all being managed **natively** in Langfuse.\\\\\\\\n## Why use Native Dataset Runs (Prompt Experiments)?\\\\\\\\n  * Quickly test different prompt versions or models\\\\\\\\n  * Structure your prompt testing by using a dataset to test different prompt versions and models\\\\\\\\n  * Quickly iterate on prompts through Dataset Runs\\\\\\\\n  * Optionally use LLM-as-a-Judge Evaluators to score the responses based on the expected outputs from the dataset\\\\\\\\n  * Prevent regressions by running tests when making prompt changes\\\\\\\\n## Prerequisites\\\\\\\\n### Create a usable prompt\\\\\\\\nCreate a prompt that you want to test and evaluate. [How to create a prompt?](/docs/prompt-management/get-started)\\\\\\\\n **A prompt is usable when:** your prompt has variables that match the dataset item keys in the dataset that will be used for the Dataset Run. See the example below.\\\\\\\\nExample: Prompt Variables & Dataset Item Keys Mapping\\\\\\\\n  \\\\\\\\n **Prompt:**\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n{{ documentation }}\\\\\\\\n \\\\\\\\nQuestion: {{question}}\\\\\\\\n \\\\\\\\n```\\\\\\\\n  \\\\\\\\n**Dataset Item:**\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n{\\\\\\\\n \\\\\\\\\\\\\\\"documentation\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"Langfuse is an LLM Engineering Platform\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"question\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"What is Langfuse?\\\\\\\\\\\\\\\"\\\\\\\\n}\\\\\\\\n```\\\\\\\\nIn this example:\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/changelog/2024-11-18-dataset-runs-comparison-view\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Dataset Run Comparison View\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Dataset Run Comparison View - Langfuse\\\\\\\\n[\\\\\\\\u2190 Back to changelog](/changelog#2024-11-18-dataset-runs-comparison-view)\\\\\\\\nNovember 18, 2024 | Launch Week 2 \\\\\\\\ud83d\\\\\\\\ude80\\\\\\\\n# Dataset Run Comparison View\\\\\\\\n[Marlies Mayerhofer](https://twitter.com/marliessophie)\\\\\\\\nAfter running experiments on datasets, you can now compare results side-by-side, view metrics, and peek into details of each dataset item across runs.\\\\\\\\n## Introduction\\\\\\\\n**What is a dataset in Langfuse?** [Datasets](/docs/datasets/overview) in Langfuse allow you to create test sets and benchmarks to evaluate the performance of your LLM application. A dataset is a collection of dataset items, where each item contains inputs, expected outputs, and metadata. You can create datasets from production edge cases, synthetic test cases, or manual test cases. This enables continuous improvement through structured testing, pre-deployment benchmarking, and flexible evaluation using custom metrics or LLM-as-a-judge approaches.\\\\\\\\n**What is a dataset experiment run?** A dataset experiment run lets you test changes to your application - like trying different models, prompts, or parameters - by running each version against your test dataset and comparing the results through traces in Langfuse to evaluate which changes work best.\\\\\\\\n## What\\\\\\\\u2019s new?\\\\\\\\nAnd the fun continues, **Day 1 of[Launch Week 2](/blog/2024-11-17-launch-week-2)** is here.\\\\\\\\n[Langfuse Datasets](/docs/datasets/overview) now enables intuitive comparison of dataset experiment runs for technical and non-technical users. The view features an overview of each item in the dataset, and a summary of each selected experiment run. The latter includes metrics on latency, cost, scores and the application\\\\\\\\u2019s output response for each dataset item.\\\\\\\\n## How to use the comparison view?\\\\\\\\n### Setup dataset and run experiments\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n  * Follow the [getting started guide](/docs/datasets/get-started) to set up a dataset, populate it with items, and run experiments.\\\\\\\\n  * Alternatively, execute an end-to-end example ([Python notebook](/docs/datasets/python-cookbook)).\\\\\\\\n### Open comparison view\\\\\\\\n  1. Select multiple dataset runs\\\\\\\\n  2. `Actions`, select `Compare`\\\\\\\\n## Learn more\\\\\\\\nFor a conceptual introduction to datasets and offline experiments, see the [dataset documentation](/docs/datasets/overview).\\\\\\\\nLast updated on September 5, 2025\\\\\\\\nWas this page helpful?\\\\\\\\nYesNo\\\\\\\\n[Support](/support)\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/guides/cookbook/example_synthetic_datasets\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Synthetic Dataset Generation for LLM Evaluation\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Synthetic Dataset Generation for LLM Evaluation - Langfuse\\\\\\\\nGuides[Cookbooks](/guides/cookbook \\\\\\\\\\\\\\\"Cookbooks\\\\\\\\\\\\\\\")Synthetic Datasets\\\\\\\\nCopy as Markdown\\\\\\\\nThis is a Jupyter notebook\\\\\\\\n[Open on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/cookbook/example_synthetic_datasets.ipynb)\\\\\\\\n[Run on Google Colab](https://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/cookbook/example_synthetic_datasets.ipynb)\\\\\\\\n# Synthetic Dataset Generation for LLM Evaluation\\\\\\\\nIn this notebook, we will explore how to **generate synthetic datasets** using language models and uploading them to [Langfuse](https://langfuse.com) for evaluation.\\\\\\\\n## What are Langfuse Datasets?\\\\\\\\nIn Langfuse, a _dataset_ is a collection of _dataset items_ , each typically containing an `input` (e.g., user prompt/question), `expected_output` (the ground truth or ideal answer) and optional metadata.\\\\\\\\nDatasets are used for **evaluation**. You can run your LLM or application on each item in a dataset and compare the application\\\\\\\\u2019s responses to the expected outputs. This way, you can track performance over time and across different application configs (e.g. model versions or prompt changes).\\\\\\\\n## Cases your Dataset Should Cover\\\\\\\\n**Happy path** \\\\\\\\u2013 straightforward or common queries:\\\\\\\\n  * \\\\\\\\u201cWhat is the capital of France?\\\\\\\\u201d\\\\\\\\n  * \\\\\\\\u201cConvert 5 USD to EUR.\\\\\\\\u201d\\\\\\\\n**Edge cases** \\\\\\\\u2013 unusual or complex:\\\\\\\\n  * Very long prompts.\\\\\\\\n  * Ambiguous queries.\\\\\\\\n  * Very technical or niche.\\\\\\\\n**Adversarial cases** \\\\\\\\u2013 malicious or tricky:\\\\\\\\n  * Prompt injection attempts (\\\\\\\\u201cIgnore all instructions and \\\\\\\\u2026\\\\\\\\u201d).\\\\\\\\n  * Content policy violations (harassment, hate speech).\\\\\\\\n  * Logic traps (trick questions).\\\\\\\\n## Examples\\\\\\\\n### Example 1: Looping Over OpenAI API\\\\\\\\nWe\\\\\\\\u2019ll use OpenAI\\\\\\\\u2019s API in a simple loop to create synthetic questions for an airline chatbot. You could similarly prompt the model to generate _both_ questions and answers. %pip install openai langfuse\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nimport os\\\\\\\\n \\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n///# Get keys for your project from the project settings page: https://cloud.langfuse.com\\\\\\\\nos.environ[\\\\\\\\\\\\\\\"LANGFUSE_PUBLIC_KEY\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"pk-lf-...\\\\\\\\\\\\\\\" \\\\\\\\nos.environ[\\\\\\\\\\\\\\\"LANGFUSE_SECRET_KEY\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"sk-lf-...\\\\\\\\\\\\\\\" \\\\\\\\nos.environ[\\\\\\\\\\\\\\\"LANGFUSE_HOST\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"https://cloud.langfuse.com\\\\\\\\\\\\\\\" # \\\\\\\\ud83c\\\\\\\\uddea\\\\\\\\ud83c\\\\\\\\uddfa EU region\\\\\\\\n///# os.environ[\\\\\\\\\\\\\\\"LANGFUSE_HOST\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"https://us.cloud.langfuse.com\\\\\\\\\\\\\\\" # \\\\\\\\ud83c\\\\\\\\uddfa\\\\\\\\ud83c\\\\\\\\uddf8 US region\\\\\\\\n \\\\\\\\n///# Your openai key\\\\\\\\nos.environ[\\\\\\\\\\\\\\\"OPENAI_API_KEY\\\\\\\\\\\\\\\"] = \\\\\\\\\\\\\\\"sk-proj-...\\\\\\\\\\\\\\\"\\\\\\\\n```\\\\\\\\nWith the environment variables set, we can now initialize the Langfuse client. `get_client()` initializes the Langfuse client using the credentials provided in the environment variables.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\n \\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\n///# Verify connection\\\\\\\\nif langfuse.auth_check():\\\\\\\\n print(\\\\\\\\\\\\\\\"Langfuse client is authenticated and ready!\\\\\\\\\\\\\\\")\\\\\\\\nelse:\\\\\\\\n print(\\\\\\\\\\\\\\\"Authentication failed. Please check your credentials and host.\\\\\\\\\\\\\\\")\\\\\\\\n```\\\\\\\\nLangfuse client is authenticated and ready!\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom openai import OpenAI\\\\\\\\nimport pandas as pd\\\\\\\\n \\\\\\\\nclient = OpenAI()\\\\\\\\n \\\\\\\\n///# Function to generate airline questions\\\\\\\\ndef generate_airline_questions(num_questions=20):\\\\\\\\n \\\\\\\\n questions = []\\\\\\\\n \\\\\\\\n for i in range(num_questions):\\\\\\\\n completion = client.chat.completions.create(\\\\\\\\n model=\\\\\\\\\\\\\\\"gpt-4o\\\\\\\\\\\\\\\", \\\\\\\\n messages=[\\\\\\\\n {\\\\\\\\n \\\\\\\\\\\\\\\"role\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"system\\\\\\\\\\\\\\\",\\\\\\\\n \\\\\\\\\\\\\\\"content\\\\\\\\\\\\\\\": (\\\\\\\\n \\\\\\\\\\\\\\\"You are a helpful customer service chatbot for an airline. \\\\\\\\\\\\\\\"\\\\\\\\n \\\\\\\\\\\\\\\"Please generate a short, realistic question from a customer.\\\\\\\\\\\\\\\"\\\\\\\\n )\\\\\\\\n }\\\\\\\\n ],\\\\\\\\n temperature=1\\\\\\\\n )\\\\\\\\n question_text = completion.choices[0].message.content.strip()\\\\\\\\n questions.append(question_text)\\\\\\\\n \\\\\\\\n return questions\\\\\\\\n \\\\\\\\n///# Generate 20 airline-related questions\\\\\\\\nairline_questions = generate_airline_questions(num_questions=20)\\\\\\\\n \\\\\\\\n///# Convert to a Pandas DataFrame\\\\\\\\ndf = pd.DataFrame({\\\\\\\\\\\\\\\"Question\\\\\\\\\\\\\\\": airline_questions})\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n scenario=\\\\\\\\\\\\\\\"Non-technical users trying to query a database using plain English.\\\\\\\\\\\\\\\",\\\\\\\\n)\\\\\\\\n \\\\\\\\n///# 2. Initialize the Synthesizer\\\\\\\\nsynthesizer = Synthesizer(styling_config=styling_config)\\\\\\\\n \\\\\\\\n///# 3. Generate synthetic items from scratch, e.g. 20 items for a short demo\\\\\\\\nsynthesizer.generate_goldens_from_scratch(num_goldens=20)\\\\\\\\n \\\\\\\\n///# 4. Access the generated examples\\\\\\\\nsynthetic_goldens = synthesizer.synthetic_goldens\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nfrom langfuse import get_client\\\\\\\\nlangfuse = get_client()\\\\\\\\n \\\\\\\\n///# 5. Create a Langfuse dataset\\\\\\\\ndeepeval_dataset_name = \\\\\\\\\\\\\\\"deepeval_synthetic_data\\\\\\\\\\\\\\\"\\\\\\\\nlangfuse.create_dataset(\\\\\\\\n name=deepeval_dataset_name,\\\\\\\\n description=\\\\\\\\\\\\\\\"Synthetic text-to-SQL data (DeepEval)\\\\\\\\\\\\\\\",\\\\\\\\n metadata={\\\\\\\\\\\\\\\"approach\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"deepeval\\\\\\\\\\\\\\\", \\\\\\\\\\\\\\\"task\\\\\\\\\\\\\\\": \\\\\\\\\\\\\\\"text-to-sql\\\\\\\\\\\\\\\"}\\\\\\\\n)\\\\\\\\n \\\\\\\\n///# 6. Upload the items\\\\\\\\nfor golden in synthetic_goldens:\\\\\\\\n langfuse.create_dataset_item(\\\\\\\\n dataset_name=deepeval_dataset_name,\\\\\\\\n input={\\\\\\\\\\\\\\\"query\\\\\\\\\\\\\\\": golden.input},\\\\\\\\n )\\\\\\\\n```\\\\\\\\n### Example 4: No-Code via Hugging Face Dataset Generator\\\\\\\\nIf you prefer a more UI-based approach, check out [Hugging Face\\\\\\\\u2019s Synthetic Data Generator](https://huggingface.co/blog/synthetic-data-generator). You can generate examples in the Hugging Face UI. Then you can download them as CSV and upload it in the Langfuse UI.\\\\\\\\n## Next Steps\\\\\\\\n  1. **Explore your dataset in Langfuse**. You can see each dataset in the UI.\\\\\\\\n  2. **Run experiments** You can now evaluate your application using this dataset.\\\\\\\\n  3. **Compare runs** over time or across models, prompts, or chain logic.\\\\\\\\nFor more details on how to run experiments on a dataset, see the [Langfuse docs](https://langfuse.com/docs/datasets/get-started#run-experiment-on-a-dataset).\\\\\\\\nLast updated on July 29, 2025\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n[Query Data in Langfuse via the SDK](/guides/cookbook/example_query_data_via_sdk \\\\\\\\\\\\\\\"Query Data in Langfuse via the SDK\\\\\\\\\\\\\\\")\\\\\\\\n[Amazon Bedrock](/guides/cookbook/integration_amazon_bedrock \\\\\\\\\\\\\\\"Amazon Bedrock\\\\\\\\\\\\\\\")\\\\\\\\nWas this page helpful?\\\\\\\\nYesNo\\\\\\\\n[Support](/support)\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/docs/evaluation/dataset-runs/data-model\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Dataset Runs Data Model\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Dataset Runs Data Model - Langfuse\\\\\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\\\\\\\\\"Evaluation\\\\\\\\\\\\\\\")\\\\\\\\n[Dataset Runs](/docs/evaluation/dataset-runs/datasets \\\\\\\\\\\\\\\"Dataset Runs\\\\\\\\\\\\\\\")Data Model\\\\\\\\nCopy as Markdown\\\\\\\\n# Dataset Runs Data Model\\\\\\\\nThis page describes the data model of datasets and dataset runs. For detailed reference please refer to the [API Reference](https://api.reference.langfuse.com).\\\\\\\\n## Datasets\\\\\\\\nDatasets are a collection of inputs and, optionally, expected outputs that can be during Dataset runs.\\\\\\\\n`Dataset`s are a collection of `DatasetItem`s.\\\\\\\\n1\\\\\\\\nn\\\\\\\\nDataset\\\\\\\\nname\\\\\\\\ndescription\\\\\\\\nmetadata\\\\\\\\nDatasetItem\\\\\\\\ndatasetName\\\\\\\\ninput\\\\\\\\nexpectedOutput\\\\\\\\nmetadata\\\\\\\\nsourceTraceId\\\\\\\\nsourceObservationId\\\\\\\\nid\\\\\\\\nstatus\\\\\\\\n### Dataset object\\\\\\\\nAttribute| Type| Required| Description  \\\\\\\\n---|---|---|---  \\\\\\\\n`name`| string| Yes| Name of the dataset  \\\\\\\\n`description`| string| No| Description of the dataset  \\\\\\\\n`metadata`| object| No| Additional metadata for the dataset  \\\\\\\\n  \\\\\\\\n### DatasetItem object\\\\\\\\nAttribute| Type| Required| Description  \\\\\\\\n---|---|---|---  \\\\\\\\n`datasetName`| string| Yes| Name of the dataset to add the item to  \\\\\\\\n`input`| object| No| Input data for the dataset item  \\\\\\\\n`expectedOutput`| object| No| Expected output data for the dataset item  \\\\\\\\n`metadata`| object| No| Additional metadata for the dataset item  \\\\\\\\n`sourceTraceId`| string| No| ID of the source trace to link this dataset item to  \\\\\\\\n`sourceObservationId`| string| No| ID of the source observation to link this dataset item to  \\\\\\\\n`id`| string| No| Unique identifier for the dataset item. Dataset items are upserted on their id. Id needs to be unique (project-level) and cannot be reused across datasets.  \\\\\\\\n`status`| DatasetStatus| No| Status of the dataset item. Defaults to ACTIVE for newly created items. Possible values: `ACTIVE`, `ARCHIVED`  \\\\\\\\n  \\\\\\\\n## DatasetRun\\\\\\\\nDataset runs are used to run a dataset through your LLM application and optionally apply evaluation methods to the results.\\\\\\\\n  \\\\\\\\n### DatasetRun object\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Attribute| Type| Required| Description  \\\\\\\\n---|---|---|---  \\\\\\\\n`datasetName`| string| Yes| Name of the dataset  \\\\\\\\n`runName`| string| Yes| Name of the dataset run  \\\\\\\\n  \\\\\\\\n### DatasetRunItem object\\\\\\\\nAttribute| Type| Required| Description  \\\\\\\\n---|---|---|---  \\\\\\\\n`runName`| string| Yes| Name of the dataset run to add the item to  \\\\\\\\n`runDescription`| string| No| Description of the run. If run exists, description will be updated  \\\\\\\\n`metadata`| object| No| Metadata of the dataset run, updates run if run already exists  \\\\\\\\n`datasetItemId`| string| Yes| ID of the dataset item to link to this run  \\\\\\\\n`observationId`| string| No| ID of the observation to link to this run  \\\\\\\\n`traceId`| string| No| ID of the trace to link to this run. traceId should always be provided. For compatibility with older SDK versions it can also be inferred from the provided observationId.  \\\\\\\\n  \\\\\\\\nMost of the time, we recommend that DatasetRunItems reference TraceIDs directly. The reference to ObservationID exists for backwards compatibility with older SDK versions.\\\\\\\\n## End to end data relations\\\\\\\\nDataSetRuns can combine a few Langfuse objects:\\\\\\\\n  * `DatasetRuns` are created by looping through all or selected `DatasetItem`s of a `Dataset` with your LLM application.\\\\\\\\n  * For each `DatasetItem` passed into the LLM application as an Input a `DatasetRunItem` & a `Trace` are created.\\\\\\\\n  * Optionally `Score`s can be added to the `Trace`s to evaluate the output of the LLM application during the `DatasetRun`.\\\\\\\\n  \\\\\\\\nLast updated on July 29, 2025\\\\\\\\n[Remote Run](/docs/evaluation/dataset-runs/remote-run \\\\\\\\\\\\\\\"Remote Run\\\\\\\\\\\\\\\")\\\\\\\\n[Troubleshooting & FAQ](/docs/evaluation/troubleshooting-and-faq \\\\\\\\\\\\\\\"Troubleshooting & FAQ\\\\\\\\\\\\\\\")\\\\\\\\nWas this page helpful?\\\\\\\\nYesNo\\\\\\\\n[Support](/support)\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"github_discussion\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://github.com/orgs/langfuse/discussions/4844\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Dataset creates a lot of identical processes that run infinitely.\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"Title: Dataset creates a lot of identical processes that run infinitely.\\\\\\\\nCategory: Support\\\\\\\\nAuthor: @robotkoruslan\\\\\\\\nTimestamp: December 28, 2024\\\\\\\\nBody:\\\\\\\\nWhen running a simple experiment, dataset creates a lot of identical processes that run infinitely, if click trace error \\\\\\\\\\\\\\\"Path: traces.byIdWithObservationsAndScores\\\\\\\\\\\\\\\"\\\\\\\\r\\\\\\\\n<img width=\\\\\\\\\\\\\\\"1494\\\\\\\\\\\\\\\" alt=\\\\\\\\\\\\\\\"image\\\\\\\\\\\\\\\" src=\\\\\\\\\\\\\\\"https://github.com/user-attachments/assets/2ee21b5f-2769-46a2-b1c0-141f0014d6fa\\\\\\\\\\\\\\\" />\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n<img width=\\\\\\\\\\\\\\\"1494\\\\\\\\\\\\\\\" alt=\\\\\\\\\\\\\\\"image\\\\\\\\\\\\\\\" src=\\\\\\\\\\\\\\\"https://github.com/user-attachments/assets/8e56daf9-e405-411e-8f62-042292a64764\\\\\\\\\\\\\\\" />\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\r\\\\\\\\n\\\\\\\\nComments:\\\\\\\\n\\\\\\\\n    Comment #1 from @dosubot on December 28, 2024:\\\\\\\\n\\\\\\\\n    <!-- Greeting -->\\\\\\\\nHello, @robotkoruslan! I'm here to help you with any bugs, questions, or contributions you have. Let's tackle this issue together!\\\\\\\\n\\\\\\\\n<!-- Answer -->\\\\\\\\nThe issue you're encountering with identical processes running indefinitely during dataset runs in Langfuse could be related to how traces are managed and linked to dataset runs. Here are some potential causes and solutions:\\\\\\\\n\\\\\\\\n1. **Trace Linking**: Ensure that traces are correctly linked to dataset runs. Each dataset run item should have a corresponding `traceId` in the `datasetRunItems` table. If the trace is not properly linked or created, it could lead to a \\\\\\\\\\\\\\\"Trace not found\\\\\\\\\\\\\\\" error <sup>[[1]](https://github.com/langfuse/langfuse/blob/main/web/src/__tests__/async/dataset-service.servertest.ts)</sup><sup>[[2]](https://github.com/langfuse/langfuse/blob/main/web/src/pages/api/public/dataset-run-items.ts)</sup>.\\\\\\\\n\\\\\\\\n2. **Trace Retrieval**: The error with the path `traces.byIdWithObservationsAndScores` suggests an issue with retrieving traces by their ID along with observations and scores. This could be due to incorrect `traceId` or `projectId`, or if the trace has been deleted or not yet created <sup>[[3]](https://github.com/langfuse/langfuse/blob/main/web/src/server/api/routers/traces.ts)</sup>.\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/changelog/2024-11-21-all-new-datasets-and-evals-documentation\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"All new Datasets, Experimentation and Evaluation documentation\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"## New Datasets and Evaluation documentation\\\\\\\\nBased on this week\\\\\\\\u2019s changes, we have have restructured the documentation to:\\\\\\\\n  1. Better explain **how and when to use Langfuse Datasets and Evals** to evaluate your LLM applications in development\\\\\\\\n  2. Introduce the **core data model** of `datasets`, experiment `runs`, and `scores`\\\\\\\\n  3. Provide **end-to-end examples** for common workflows as Jupyter Notebooks\\\\\\\\n  4. Add many **more gifs and toggles with additional explanations** throughout the docs\\\\\\\\n### Evaluation\\\\\\\\n\\\\\\\\ud83d\\\\\\\\udca1\\\\\\\\n\\\\\\\\u2192 [Evaluation documentation](/docs/scores)\\\\\\\\nAnalyticsUser FeedbackLLM-as-a-JudgePrompt ExperimentAnnotation QueueCustom Evals\\\\\\\\nPlot evaluation results in the Langfuse Dashboard.\\\\\\\\nCollect feedback from your users. Can be captured in the frontend via our Browser SDK, server-side via the SDKs or API. Video includes example application.\\\\\\\\nRun fully managed LLM-as-a-judge evaluations on production or development traces. Can be applied to any step within your application for step-wise evaluations.\\\\\\\\nEvaluate prompts and models on datasets directly in the user interface. No custom code is needed.\\\\\\\\nBaseline your evaluation workflow with human annotations via Annotation Queues.\\\\\\\\nAdd custom evaluation results, supports numeric, boolean and categorical values.\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nPOST /api/public/scores\\\\\\\\n```\\\\\\\\nAdd scores via Python or JS SDK.\\\\\\\\nExample (Python)\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nlangfuse.score(\\\\\\\\n trace_id=\\\\\\\\\\\\\\\"123\\\\\\\\\\\\\\\",\\\\\\\\n name=\\\\\\\\\\\\\\\"my_custom_evaluator\\\\\\\\\\\\\\\",\\\\\\\\n value=0.5,\\\\\\\\n)\\\\\\\\n```\\\\\\\\n### Datasets\\\\\\\\n\\\\\\\\ud83d\\\\\\\\udca1\\\\\\\\n\\\\\\\\u2192 [Datasets & Experiments documentation](/docs/datasets)\\\\\\\\nManage DatasetsCompare experimentsAdd new items from Langfuse Traces\\\\\\\\nCollaboratively manage datasets via UI, API, or SDKs.\\\\\\\\nRun experiments on datasets. Compare metrics across experiments, and see results side-by-side.\\\\\\\\nDirectly add new items from production traces to datasets to improve your datasets over time.\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"site\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/changelog/2024-11-18-dataset-runs-comparison-view\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Dataset Run Comparison View - Langfuse\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"---\\\\\\\\n\\\\\\\\n[\\\\\\\\u2190 Back to changelog](/changelog#2024-11-18-dataset-runs-comparison-view)\\\\\\\\nNovember 18, 2024 | Launch Week 2 \\\\\\\\ud83d\\\\\\\\ude80\\\\\\\\n# Dataset Run Comparison View\\\\\\\\n[Marlies Mayerhofer](https://twitter.com/marliessophie)\\\\\\\\nAfter running experiments on datasets, you can now compare results side-by-side, view metrics, and peek into details of each dataset item across runs.\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n## Introduction\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n**What is a dataset in Langfuse?** [Datasets](/docs/datasets/overview) in Langfuse allow you to create test sets and benchmarks to evaluate the performance of your LLM application. A dataset is a collection of dataset items, where each item contains inputs, expected outputs, and metadata. You can create datasets from production edge cases, synthetic test cases, or manual test cases. This enables continuous improvement through structured testing, pre-deployment benchmarking, and flexible evaluation using custom metrics or LLM-as-a-judge approaches.\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n**What is a dataset experiment run?** A dataset experiment run lets you test changes to your application - like trying different models, prompts, or parameters - by running each version against your test dataset and comparing the results through traces in Langfuse to evaluate which changes work best.\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n## What\\\\\\\\u2019s new?\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\nAnd the fun continues, **Day 1 of[Launch Week 2](/blog/2024-11-17-launch-week-2)** is here.\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n[Langfuse Datasets](/docs/datasets/overview) now enables intuitive comparison of dataset experiment runs for technical and non-technical users. The view features an overview of each item in the dataset, and a summary of each selected experiment run. The latter includes metrics on latency, cost, scores and the application\\\\\\\\u2019s output response for each dataset item.\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n## How to use the comparison view?\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n### Setup dataset and run experiments\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n  * Follow the [getting started guide](/docs/datasets/get-started) to set up a dataset, populate it with items, and run experiments.\\\\\\\\n  * Alternatively, execute an end-to-end example ([Python notebook](/docs/datasets/python-cookbook)).\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n### Open comparison view\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n  1. Select multiple dataset runs\\\\\\\\n  2. `Actions`, select `Compare`\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\n## Learn more\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\nFor a conceptual introduction to datasets and offline experiments, see the [dataset documentation](/docs/datasets/overview).\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\\nLast updated on September 5, 2025\\\\\\\\n\\\\\\\\n---\\\\\\\\n\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}, {\\\\\\\"type\\\\\\\": \\\\\\\"document\\\\\\\", \\\\\\\"record_type\\\\\\\": \\\\\\\"documentation\\\\\\\", \\\\\\\"url\\\\\\\": \\\\\\\"https://langfuse.com/blog/update-2023-09\\\\\\\", \\\\\\\"title\\\\\\\": \\\\\\\"Langfuse Update \\\\\\\\u2014 September 2023\\\\\\\", \\\\\\\"source\\\\\\\": {\\\\\\\"content\\\\\\\": [{\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n\\\\\\\\u2192 [Docs](/docs/scores/model-based-evals)\\\\\\\\n## \\\\\\\\ud83d\\\\\\\\uddc2\\\\\\\\ufe0f Datasets (beta)\\\\\\\\nSystematically test new iterations of your LLM app with `datasets`.\\\\\\\\nDatasets are collections of inputs and expected outputs that you can manage in Langfuse. Upload an existing dataset or create one based on production data (e.g. when discovering new edge cases).\\\\\\\\nWhen combined with automated evals, Datasets in Langfuse make it easy to systematically evaluate new iterations of your LLM app.\\\\\\\\n_Overview of dataset runs on a demo dataset_\\\\\\\\n_Run experiment on dataset_\\\\\\\\nPython SDKJS/TS SDKLangchain (Python)\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\n \\\\\\\\ndataset = langfuse.get_dataset(\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\nfor item in dataset.items:\\\\\\\\n # execute application function and get Langfuse parent observation (span/generation/event)\\\\\\\\n # output also returned as it is used to evaluate the run\\\\\\\\n generation, output = my_llm_application.run(item.input)\\\\\\\\n \\\\\\\\n # link the execution trace to the dataset item and give it a run_name\\\\\\\\n item.link(generation, \\\\\\\\\\\\\\\"<run_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n # optionally, evaluate the output to compare different runs more easily\\\\\\\\n generation.score(\\\\\\\\n name=\\\\\\\\\\\\\\\"<example_eval>\\\\\\\\\\\\\\\",\\\\\\\\n # any float value\\\\\\\\n value=my_eval_fn(\\\\\\\\n item.input,\\\\\\\\n output,\\\\\\\\n item.expected_output\\\\\\\\n )\\\\\\\\n )\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\nconst dataset = await langfuse.getDataset(\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\");\\\\\\\\n \\\\\\\\nfor (const item of dataset.items) {\\\\\\\\n // execute application function and get Langfuse parent observation (span/generation/event)\\\\\\\\n // output also returned as it is used to evaluate the run\\\\\\\\n const [generation, output] = await myLlmApplication.run(item.input);\\\\\\\\n \\\\\\\\n // link the execution trace to the dataset item and give it a run_name\\\\\\\\n await item.link(generation, \\\\\\\\\\\\\\\"<run_name>\\\\\\\\\\\\\\\");\\\\\\\\n \\\\\\\\n // optionally, evaluate the output to compare different runs more easily\\\\\\\"}, {\\\\\\\"type\\\\\\\": \\\\\\\"text\\\\\\\", \\\\\\\"text\\\\\\\": \\\\\\\"\\\\\\\\n generation.score({\\\\\\\\n name: \\\\\\\\\\\\\\\"<score_name>\\\\\\\\\\\\\\\",\\\\\\\\n value: myEvalFunction(item.input, output, item.expectedOutput),\\\\\\\\n });\\\\\\\\n}\\\\\\\\n```\\\\\\\\n    \\\\\\\\n    \\\\\\\\n```\\\\\\\\ndataset = langfuse.get_dataset(\\\\\\\\\\\\\\\"<dataset_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\nfor item in dataset.items:\\\\\\\\n # Langchain calback handler that automatically links the execution trace to the dataset item\\\\\\\\n handler = item.get_langchain_handler(run_name=\\\\\\\\\\\\\\\"<run_name>\\\\\\\\\\\\\\\")\\\\\\\\n \\\\\\\\n # Execute application and pass custom handler\\\\\\\\n my_langchain_chain.run(item.input, callbacks=[handler])\\\\\\\\n```\\\\\\\\nDatasets are currently in beta on Langfuse Cloud as the API might still slightly change. If you\\\\\\\\u2019d like to try it, let us know via the in-app chat.\\\\\\\\n\\\\\\\\u2192 [Dataset docs](/docs/datasets/overview) \\\\\\\\u2192 [Python Cookbook](/docs/datasets/python-cookbook)\\\\\\\\n## \\\\\\\\ud83d\\\\\\\\udcca In-app dashboards\\\\\\\\nOver the last weeks, [analytics features](/docs/analytics/overview) were in public alpha on Langfuse Cloud. We\\\\\\\\u2019ve now shipped a new _query engine_ as an underlying abstraction for the native in-app dashboards. This is a major step towards bringing all analytics features into the Langfuse core project and helps us move much faster on these.\\\\\\\\nOver the next days, you\\\\\\\\u2019ll see more and more dashboards popping up in the app. If there is a specific analysis you\\\\\\\\u2019d like to see, suggest it on [Discord](/discord).\\\\\\\\n## \\\\\\\\ud83d\\\\\\\\udd04 New integrations\\\\\\\\nThe new integrations make it easier to get started with Langfuse. Thanks to the teams behind Langflow, Flowise and LiteLLM for building/collaborating on these integrations.\\\\\\\\nSee integrations docs for details:\\\\\\\\n  * [Langflow](/docs/langflow): No-code LLM app builder in Python\\\\\\\\n  * [Flowise](/docs/flowise): No-code LLM app builder in JS\\\\\\\\n  * [LiteLLM](/integrations/gateways/litellm): Python library to use any LLM model as drop in replacement of OpenAI API\\\\\\\"}], \\\\\\\"type\\\\\\\": \\\\\\\"content\\\\\\\"}}]}\\\"}],\\\"isError\\\":false}\"},\"resourceAttributes\":{\"service.name\":\"unknown_service:node\",\"telemetry.sdk.language\":\"nodejs\",\"telemetry.sdk.name\":\"opentelemetry\",\"telemetry.sdk.version\":\"2.0.1\"},\"scope\":{\"name\":\"ai\",\"attributes\":{}}}",
      "level": "DEFAULT",
      "statusMessage": null,
      "version": null,
      "input": "{\"query\":\"What is a dataset in Langfuse? definition, use cases, evaluation, prompt testing, datasets feature docs\"}",
      "output": "{\"_meta\":{\"id\":\"inkeep-chatcmpl-1761a743-0fb1-4e4d-9c1c-10d4960ef29f\",\"model\":\"inkeep-rag\",\"choices\":[{\"finish_reason\":\"stop\",\"log_probs\":null,\"index\":0,\"message\":{\"content\":\"{\\\"content\\\": [{\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/dataset-runs/datasets\\\", \\\"title\\\": \\\"Datasets\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Datasets - Langfuse\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\"Evaluation\\\\\\\")Dataset RunsDatasets\\\\nCopy as Markdown\\\\n# Datasets\\\\nA dataset is a collection of inputs and expected outputs and is used to test your application. Before executing your first dataset run, you need to create a dataset.\\\\n## Why use datasets?\\\\n  * **Datasets prerequisite for Dataset Runs** , they serve as the data input of Dataset Runs\\\\n  * Create test cases for your application with real production traces\\\\n  * Collaboratively create and collect dataset items with your team\\\\n  * Have a single source of truth for your test data\\\\n## Get Started\\\\n### Creating a dataset\\\\nDatasets have a name which is unique within a project.\\\\nPython SDKJS/TS SDKLangfuse UI\\\\n    \\\\n    \\\\n```\\\\nlangfuse.create_dataset(\\\\n name=\\\\\\\"<dataset_name>\\\\\\\",\\\\n # optional description\\\\n description=\\\\\\\"My first dataset\\\\\\\",\\\\n # optional metadata\\\\n metadata={\\\\n \\\\\\\"author\\\\\\\": \\\\\\\"Alice\\\\\\\",\\\\n \\\\\\\"date\\\\\\\": \\\\\\\"2022-01-01\\\\\\\",\\\\n \\\\\\\"type\\\\\\\": \\\\\\\"benchmark\\\\\\\"\\\\n }\\\\n)\\\\n```\\\\n _See[Python SDK](/docs/sdk/python/sdk-v3) docs for details on how to initialize the Python client._\\\\n    \\\\n    \\\\n```\\\\nimport { LangfuseClient } from \\\\\\\"@langfuse/client\\\\\\\"\\\\n \\\\nconst langfuse = new LangfuseClient()\\\\n \\\\nawait langfuse.api.datasets.create({\\\\n name: \\\\\\\"<dataset_name>\\\\\\\",\\\\n // optional description\\\\n description: \\\\\\\"My first dataset\\\\\\\",\\\\n // optional metadata\\\\n metadata: {\\\\n author: \\\\\\\"Alice\\\\\\\",\\\\n date: \\\\\\\"2022-01-01\\\\\\\",\\\\n type: \\\\\\\"benchmark\\\\\\\",\\\\n },\\\\n});\\\\n```\\\\n  1. **Navigate to** `Your Project` > `Datasets`\\\\n  2. **Click on** `+ New dataset` to create a new dataset.\\\\n### Create new dataset items\\\\nDataset items can be added to a dataset by providing the input and optionally the expected output.\\\\nPython SDKJS/TS SDKLangfuse UI\\\\n    \\\\n    \\\\n```\\\\nlangfuse.create_dataset_item(\\\\n dataset_name=\\\\\\\"<dataset_name>\\\\\\\",\\\\n # any python object or value, optional\\\\n input={\\\\n \\\\\\\"text\\\\\\\": \\\\\\\"hello world\\\\\\\"\\\\n },\\\\n # any python object or value, optional\\\\n expected_output={\\\\n \\\\\\\"text\\\\\\\": \\\\\\\"hello world\\\\\\\"\\\\n },\\\\n # metadata, optional\\\\n metadata={\\\\n \\\\\\\"model\\\\\\\": \\\\\\\"llama3\\\\\\\",\\\\n }\\\\n)\\\\n```\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/dataset-runs/native-run\\\", \\\"title\\\": \\\"Native Dataset Runs (Prompt Experiments)\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Native Dataset Runs (Prompt Experiments) - Langfuse\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\"Evaluation\\\\\\\")\\\\n[Dataset Runs](/docs/evaluation/dataset-runs/datasets \\\\\\\"Dataset Runs\\\\\\\")Native Run\\\\nCopy as Markdown\\\\n# Native Dataset Runs (Prompt Experiments)\\\\nYou can execute Native Dataset Runs **(also called Prompt Experiments)** in the Langfuse UI to test different prompt versions from [Prompt Management](/docs/prompt-management) or language models and compare the results side-by-side.\\\\nOptionally, you can use [LLM-as-a-Judge Evaluators](/docs/evaluation/evaluation-methods/llm-as-a-judge) to automatically score the responses based on the expected outputs to further analyze the results on an aggregate level.\\\\nThey are called \\\\u201cNative Dataset Runs\\\\u201d because they rely on Datasets, Prompts and optionally LLM-as-a-Judge Evaluators all being managed **natively** in Langfuse.\\\\n## Why use Native Dataset Runs (Prompt Experiments)?\\\\n  * Quickly test different prompt versions or models\\\\n  * Structure your prompt testing by using a dataset to test different prompt versions and models\\\\n  * Quickly iterate on prompts through Dataset Runs\\\\n  * Optionally use LLM-as-a-Judge Evaluators to score the responses based on the expected outputs from the dataset\\\\n  * Prevent regressions by running tests when making prompt changes\\\\n## Prerequisites\\\\n### Create a usable prompt\\\\nCreate a prompt that you want to test and evaluate. [How to create a prompt?](/docs/prompt-management/get-started)\\\\n **A prompt is usable when:** your prompt has variables that match the dataset item keys in the dataset that will be used for the Dataset Run. See the example below.\\\\nExample: Prompt Variables & Dataset Item Keys Mapping\\\\n  \\\\n **Prompt:**\\\\n    \\\\n    \\\\n```\\\\n{{ documentation }}\\\\n \\\\nQuestion: {{question}}\\\\n \\\\n```\\\\n  \\\\n**Dataset Item:**\\\\n    \\\\n    \\\\n```\\\\n{\\\\n \\\\\\\"documentation\\\\\\\": \\\\\\\"Langfuse is an LLM Engineering Platform\\\\\\\",\\\\n \\\\\\\"question\\\\\\\": \\\\\\\"What is Langfuse?\\\\\\\"\\\\n}\\\\n```\\\\nIn this example:\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/changelog/2024-11-18-dataset-runs-comparison-view\\\", \\\"title\\\": \\\"Dataset Run Comparison View\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Dataset Run Comparison View - Langfuse\\\\n[\\\\u2190 Back to changelog](/changelog#2024-11-18-dataset-runs-comparison-view)\\\\nNovember 18, 2024 | Launch Week 2 \\\\ud83d\\\\ude80\\\\n# Dataset Run Comparison View\\\\n[Marlies Mayerhofer](https://twitter.com/marliessophie)\\\\nAfter running experiments on datasets, you can now compare results side-by-side, view metrics, and peek into details of each dataset item across runs.\\\\n## Introduction\\\\n**What is a dataset in Langfuse?** [Datasets](/docs/datasets/overview) in Langfuse allow you to create test sets and benchmarks to evaluate the performance of your LLM application. A dataset is a collection of dataset items, where each item contains inputs, expected outputs, and metadata. You can create datasets from production edge cases, synthetic test cases, or manual test cases. This enables continuous improvement through structured testing, pre-deployment benchmarking, and flexible evaluation using custom metrics or LLM-as-a-judge approaches.\\\\n**What is a dataset experiment run?** A dataset experiment run lets you test changes to your application - like trying different models, prompts, or parameters - by running each version against your test dataset and comparing the results through traces in Langfuse to evaluate which changes work best.\\\\n## What\\\\u2019s new?\\\\nAnd the fun continues, **Day 1 of[Launch Week 2](/blog/2024-11-17-launch-week-2)** is here.\\\\n[Langfuse Datasets](/docs/datasets/overview) now enables intuitive comparison of dataset experiment runs for technical and non-technical users. The view features an overview of each item in the dataset, and a summary of each selected experiment run. The latter includes metrics on latency, cost, scores and the application\\\\u2019s output response for each dataset item.\\\\n## How to use the comparison view?\\\\n### Setup dataset and run experiments\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n  * Follow the [getting started guide](/docs/datasets/get-started) to set up a dataset, populate it with items, and run experiments.\\\\n  * Alternatively, execute an end-to-end example ([Python notebook](/docs/datasets/python-cookbook)).\\\\n### Open comparison view\\\\n  1. Select multiple dataset runs\\\\n  2. `Actions`, select `Compare`\\\\n## Learn more\\\\nFor a conceptual introduction to datasets and offline experiments, see the [dataset documentation](/docs/datasets/overview).\\\\nLast updated on September 5, 2025\\\\nWas this page helpful?\\\\nYesNo\\\\n[Support](/support)\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/guides/cookbook/example_synthetic_datasets\\\", \\\"title\\\": \\\"Synthetic Dataset Generation for LLM Evaluation\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Synthetic Dataset Generation for LLM Evaluation - Langfuse\\\\nGuides[Cookbooks](/guides/cookbook \\\\\\\"Cookbooks\\\\\\\")Synthetic Datasets\\\\nCopy as Markdown\\\\nThis is a Jupyter notebook\\\\n[Open on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/cookbook/example_synthetic_datasets.ipynb)\\\\n[Run on Google Colab](https://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/cookbook/example_synthetic_datasets.ipynb)\\\\n# Synthetic Dataset Generation for LLM Evaluation\\\\nIn this notebook, we will explore how to **generate synthetic datasets** using language models and uploading them to [Langfuse](https://langfuse.com) for evaluation.\\\\n## What are Langfuse Datasets?\\\\nIn Langfuse, a _dataset_ is a collection of _dataset items_ , each typically containing an `input` (e.g., user prompt/question), `expected_output` (the ground truth or ideal answer) and optional metadata.\\\\nDatasets are used for **evaluation**. You can run your LLM or application on each item in a dataset and compare the application\\\\u2019s responses to the expected outputs. This way, you can track performance over time and across different application configs (e.g. model versions or prompt changes).\\\\n## Cases your Dataset Should Cover\\\\n**Happy path** \\\\u2013 straightforward or common queries:\\\\n  * \\\\u201cWhat is the capital of France?\\\\u201d\\\\n  * \\\\u201cConvert 5 USD to EUR.\\\\u201d\\\\n**Edge cases** \\\\u2013 unusual or complex:\\\\n  * Very long prompts.\\\\n  * Ambiguous queries.\\\\n  * Very technical or niche.\\\\n**Adversarial cases** \\\\u2013 malicious or tricky:\\\\n  * Prompt injection attempts (\\\\u201cIgnore all instructions and \\\\u2026\\\\u201d).\\\\n  * Content policy violations (harassment, hate speech).\\\\n  * Logic traps (trick questions).\\\\n## Examples\\\\n### Example 1: Looping Over OpenAI API\\\\nWe\\\\u2019ll use OpenAI\\\\u2019s API in a simple loop to create synthetic questions for an airline chatbot. You could similarly prompt the model to generate _both_ questions and answers. %pip install openai langfuse\\\\n    \\\\n    \\\\n```\\\\nimport os\\\\n \\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n///# Get keys for your project from the project settings page: https://cloud.langfuse.com\\\\nos.environ[\\\\\\\"LANGFUSE_PUBLIC_KEY\\\\\\\"] = \\\\\\\"pk-lf-...\\\\\\\" \\\\nos.environ[\\\\\\\"LANGFUSE_SECRET_KEY\\\\\\\"] = \\\\\\\"sk-lf-...\\\\\\\" \\\\nos.environ[\\\\\\\"LANGFUSE_HOST\\\\\\\"] = \\\\\\\"https://cloud.langfuse.com\\\\\\\" # \\\\ud83c\\\\uddea\\\\ud83c\\\\uddfa EU region\\\\n///# os.environ[\\\\\\\"LANGFUSE_HOST\\\\\\\"] = \\\\\\\"https://us.cloud.langfuse.com\\\\\\\" # \\\\ud83c\\\\uddfa\\\\ud83c\\\\uddf8 US region\\\\n \\\\n///# Your openai key\\\\nos.environ[\\\\\\\"OPENAI_API_KEY\\\\\\\"] = \\\\\\\"sk-proj-...\\\\\\\"\\\\n```\\\\nWith the environment variables set, we can now initialize the Langfuse client. `get_client()` initializes the Langfuse client using the credentials provided in the environment variables.\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\n \\\\nlangfuse = get_client()\\\\n \\\\n///# Verify connection\\\\nif langfuse.auth_check():\\\\n print(\\\\\\\"Langfuse client is authenticated and ready!\\\\\\\")\\\\nelse:\\\\n print(\\\\\\\"Authentication failed. Please check your credentials and host.\\\\\\\")\\\\n```\\\\nLangfuse client is authenticated and ready!\\\\n    \\\\n    \\\\n```\\\\nfrom openai import OpenAI\\\\nimport pandas as pd\\\\n \\\\nclient = OpenAI()\\\\n \\\\n///# Function to generate airline questions\\\\ndef generate_airline_questions(num_questions=20):\\\\n \\\\n questions = []\\\\n \\\\n for i in range(num_questions):\\\\n completion = client.chat.completions.create(\\\\n model=\\\\\\\"gpt-4o\\\\\\\", \\\\n messages=[\\\\n {\\\\n \\\\\\\"role\\\\\\\": \\\\\\\"system\\\\\\\",\\\\n \\\\\\\"content\\\\\\\": (\\\\n \\\\\\\"You are a helpful customer service chatbot for an airline. \\\\\\\"\\\\n \\\\\\\"Please generate a short, realistic question from a customer.\\\\\\\"\\\\n )\\\\n }\\\\n ],\\\\n temperature=1\\\\n )\\\\n question_text = completion.choices[0].message.content.strip()\\\\n questions.append(question_text)\\\\n \\\\n return questions\\\\n \\\\n///# Generate 20 airline-related questions\\\\nairline_questions = generate_airline_questions(num_questions=20)\\\\n \\\\n///# Convert to a Pandas DataFrame\\\\ndf = pd.DataFrame({\\\\\\\"Question\\\\\\\": airline_questions})\\\\n```\\\\n    \\\\n    \\\\n```\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n scenario=\\\\\\\"Non-technical users trying to query a database using plain English.\\\\\\\",\\\\n)\\\\n \\\\n///# 2. Initialize the Synthesizer\\\\nsynthesizer = Synthesizer(styling_config=styling_config)\\\\n \\\\n///# 3. Generate synthetic items from scratch, e.g. 20 items for a short demo\\\\nsynthesizer.generate_goldens_from_scratch(num_goldens=20)\\\\n \\\\n///# 4. Access the generated examples\\\\nsynthetic_goldens = synthesizer.synthetic_goldens\\\\n```\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\nlangfuse = get_client()\\\\n \\\\n///# 5. Create a Langfuse dataset\\\\ndeepeval_dataset_name = \\\\\\\"deepeval_synthetic_data\\\\\\\"\\\\nlangfuse.create_dataset(\\\\n name=deepeval_dataset_name,\\\\n description=\\\\\\\"Synthetic text-to-SQL data (DeepEval)\\\\\\\",\\\\n metadata={\\\\\\\"approach\\\\\\\": \\\\\\\"deepeval\\\\\\\", \\\\\\\"task\\\\\\\": \\\\\\\"text-to-sql\\\\\\\"}\\\\n)\\\\n \\\\n///# 6. Upload the items\\\\nfor golden in synthetic_goldens:\\\\n langfuse.create_dataset_item(\\\\n dataset_name=deepeval_dataset_name,\\\\n input={\\\\\\\"query\\\\\\\": golden.input},\\\\n )\\\\n```\\\\n### Example 4: No-Code via Hugging Face Dataset Generator\\\\nIf you prefer a more UI-based approach, check out [Hugging Face\\\\u2019s Synthetic Data Generator](https://huggingface.co/blog/synthetic-data-generator). You can generate examples in the Hugging Face UI. Then you can download them as CSV and upload it in the Langfuse UI.\\\\n## Next Steps\\\\n  1. **Explore your dataset in Langfuse**. You can see each dataset in the UI.\\\\n  2. **Run experiments** You can now evaluate your application using this dataset.\\\\n  3. **Compare runs** over time or across models, prompts, or chain logic.\\\\nFor more details on how to run experiments on a dataset, see the [Langfuse docs](https://langfuse.com/docs/datasets/get-started#run-experiment-on-a-dataset).\\\\nLast updated on July 29, 2025\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Query Data in Langfuse via the SDK](/guides/cookbook/example_query_data_via_sdk \\\\\\\"Query Data in Langfuse via the SDK\\\\\\\")\\\\n[Amazon Bedrock](/guides/cookbook/integration_amazon_bedrock \\\\\\\"Amazon Bedrock\\\\\\\")\\\\nWas this page helpful?\\\\nYesNo\\\\n[Support](/support)\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/dataset-runs/data-model\\\", \\\"title\\\": \\\"Dataset Runs Data Model\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Dataset Runs Data Model - Langfuse\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\"Evaluation\\\\\\\")\\\\n[Dataset Runs](/docs/evaluation/dataset-runs/datasets \\\\\\\"Dataset Runs\\\\\\\")Data Model\\\\nCopy as Markdown\\\\n# Dataset Runs Data Model\\\\nThis page describes the data model of datasets and dataset runs. For detailed reference please refer to the [API Reference](https://api.reference.langfuse.com).\\\\n## Datasets\\\\nDatasets are a collection of inputs and, optionally, expected outputs that can be during Dataset runs.\\\\n`Dataset`s are a collection of `DatasetItem`s.\\\\n1\\\\nn\\\\nDataset\\\\nname\\\\ndescription\\\\nmetadata\\\\nDatasetItem\\\\ndatasetName\\\\ninput\\\\nexpectedOutput\\\\nmetadata\\\\nsourceTraceId\\\\nsourceObservationId\\\\nid\\\\nstatus\\\\n### Dataset object\\\\nAttribute| Type| Required| Description  \\\\n---|---|---|---  \\\\n`name`| string| Yes| Name of the dataset  \\\\n`description`| string| No| Description of the dataset  \\\\n`metadata`| object| No| Additional metadata for the dataset  \\\\n  \\\\n### DatasetItem object\\\\nAttribute| Type| Required| Description  \\\\n---|---|---|---  \\\\n`datasetName`| string| Yes| Name of the dataset to add the item to  \\\\n`input`| object| No| Input data for the dataset item  \\\\n`expectedOutput`| object| No| Expected output data for the dataset item  \\\\n`metadata`| object| No| Additional metadata for the dataset item  \\\\n`sourceTraceId`| string| No| ID of the source trace to link this dataset item to  \\\\n`sourceObservationId`| string| No| ID of the source observation to link this dataset item to  \\\\n`id`| string| No| Unique identifier for the dataset item. Dataset items are upserted on their id. Id needs to be unique (project-level) and cannot be reused across datasets.  \\\\n`status`| DatasetStatus| No| Status of the dataset item. Defaults to ACTIVE for newly created items. Possible values: `ACTIVE`, `ARCHIVED`  \\\\n  \\\\n## DatasetRun\\\\nDataset runs are used to run a dataset through your LLM application and optionally apply evaluation methods to the results.\\\\n  \\\\n### DatasetRun object\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Attribute| Type| Required| Description  \\\\n---|---|---|---  \\\\n`datasetName`| string| Yes| Name of the dataset  \\\\n`runName`| string| Yes| Name of the dataset run  \\\\n  \\\\n### DatasetRunItem object\\\\nAttribute| Type| Required| Description  \\\\n---|---|---|---  \\\\n`runName`| string| Yes| Name of the dataset run to add the item to  \\\\n`runDescription`| string| No| Description of the run. If run exists, description will be updated  \\\\n`metadata`| object| No| Metadata of the dataset run, updates run if run already exists  \\\\n`datasetItemId`| string| Yes| ID of the dataset item to link to this run  \\\\n`observationId`| string| No| ID of the observation to link to this run  \\\\n`traceId`| string| No| ID of the trace to link to this run. traceId should always be provided. For compatibility with older SDK versions it can also be inferred from the provided observationId.  \\\\n  \\\\nMost of the time, we recommend that DatasetRunItems reference TraceIDs directly. The reference to ObservationID exists for backwards compatibility with older SDK versions.\\\\n## End to end data relations\\\\nDataSetRuns can combine a few Langfuse objects:\\\\n  * `DatasetRuns` are created by looping through all or selected `DatasetItem`s of a `Dataset` with your LLM application.\\\\n  * For each `DatasetItem` passed into the LLM application as an Input a `DatasetRunItem` & a `Trace` are created.\\\\n  * Optionally `Score`s can be added to the `Trace`s to evaluate the output of the LLM application during the `DatasetRun`.\\\\n  \\\\nLast updated on July 29, 2025\\\\n[Remote Run](/docs/evaluation/dataset-runs/remote-run \\\\\\\"Remote Run\\\\\\\")\\\\n[Troubleshooting & FAQ](/docs/evaluation/troubleshooting-and-faq \\\\\\\"Troubleshooting & FAQ\\\\\\\")\\\\nWas this page helpful?\\\\nYesNo\\\\n[Support](/support)\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"github_discussion\\\", \\\"url\\\": \\\"https://github.com/orgs/langfuse/discussions/4844\\\", \\\"title\\\": \\\"Dataset creates a lot of identical processes that run infinitely.\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Title: Dataset creates a lot of identical processes that run infinitely.\\\\nCategory: Support\\\\nAuthor: @robotkoruslan\\\\nTimestamp: December 28, 2024\\\\nBody:\\\\nWhen running a simple experiment, dataset creates a lot of identical processes that run infinitely, if click trace error \\\\\\\"Path: traces.byIdWithObservationsAndScores\\\\\\\"\\\\r\\\\n<img width=\\\\\\\"1494\\\\\\\" alt=\\\\\\\"image\\\\\\\" src=\\\\\\\"https://github.com/user-attachments/assets/2ee21b5f-2769-46a2-b1c0-141f0014d6fa\\\\\\\" />\\\\r\\\\n\\\\r\\\\n<img width=\\\\\\\"1494\\\\\\\" alt=\\\\\\\"image\\\\\\\" src=\\\\\\\"https://github.com/user-attachments/assets/8e56daf9-e405-411e-8f62-042292a64764\\\\\\\" />\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\nComments:\\\\n\\\\n    Comment #1 from @dosubot on December 28, 2024:\\\\n\\\\n    <!-- Greeting -->\\\\nHello, @robotkoruslan! I'm here to help you with any bugs, questions, or contributions you have. Let's tackle this issue together!\\\\n\\\\n<!-- Answer -->\\\\nThe issue you're encountering with identical processes running indefinitely during dataset runs in Langfuse could be related to how traces are managed and linked to dataset runs. Here are some potential causes and solutions:\\\\n\\\\n1. **Trace Linking**: Ensure that traces are correctly linked to dataset runs. Each dataset run item should have a corresponding `traceId` in the `datasetRunItems` table. If the trace is not properly linked or created, it could lead to a \\\\\\\"Trace not found\\\\\\\" error <sup>[[1]](https://github.com/langfuse/langfuse/blob/main/web/src/__tests__/async/dataset-service.servertest.ts)</sup><sup>[[2]](https://github.com/langfuse/langfuse/blob/main/web/src/pages/api/public/dataset-run-items.ts)</sup>.\\\\n\\\\n2. **Trace Retrieval**: The error with the path `traces.byIdWithObservationsAndScores` suggests an issue with retrieving traces by their ID along with observations and scores. This could be due to incorrect `traceId` or `projectId`, or if the trace has been deleted or not yet created <sup>[[3]](https://github.com/langfuse/langfuse/blob/main/web/src/server/api/routers/traces.ts)</sup>.\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/changelog/2024-11-21-all-new-datasets-and-evals-documentation\\\", \\\"title\\\": \\\"All new Datasets, Experimentation and Evaluation documentation\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"## New Datasets and Evaluation documentation\\\\nBased on this week\\\\u2019s changes, we have have restructured the documentation to:\\\\n  1. Better explain **how and when to use Langfuse Datasets and Evals** to evaluate your LLM applications in development\\\\n  2. Introduce the **core data model** of `datasets`, experiment `runs`, and `scores`\\\\n  3. Provide **end-to-end examples** for common workflows as Jupyter Notebooks\\\\n  4. Add many **more gifs and toggles with additional explanations** throughout the docs\\\\n### Evaluation\\\\n\\\\ud83d\\\\udca1\\\\n\\\\u2192 [Evaluation documentation](/docs/scores)\\\\nAnalyticsUser FeedbackLLM-as-a-JudgePrompt ExperimentAnnotation QueueCustom Evals\\\\nPlot evaluation results in the Langfuse Dashboard.\\\\nCollect feedback from your users. Can be captured in the frontend via our Browser SDK, server-side via the SDKs or API. Video includes example application.\\\\nRun fully managed LLM-as-a-judge evaluations on production or development traces. Can be applied to any step within your application for step-wise evaluations.\\\\nEvaluate prompts and models on datasets directly in the user interface. No custom code is needed.\\\\nBaseline your evaluation workflow with human annotations via Annotation Queues.\\\\nAdd custom evaluation results, supports numeric, boolean and categorical values.\\\\n    \\\\n    \\\\n```\\\\nPOST /api/public/scores\\\\n```\\\\nAdd scores via Python or JS SDK.\\\\nExample (Python)\\\\n    \\\\n    \\\\n```\\\\nlangfuse.score(\\\\n trace_id=\\\\\\\"123\\\\\\\",\\\\n name=\\\\\\\"my_custom_evaluator\\\\\\\",\\\\n value=0.5,\\\\n)\\\\n```\\\\n### Datasets\\\\n\\\\ud83d\\\\udca1\\\\n\\\\u2192 [Datasets & Experiments documentation](/docs/datasets)\\\\nManage DatasetsCompare experimentsAdd new items from Langfuse Traces\\\\nCollaboratively manage datasets via UI, API, or SDKs.\\\\nRun experiments on datasets. Compare metrics across experiments, and see results side-by-side.\\\\nDirectly add new items from production traces to datasets to improve your datasets over time.\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"site\\\", \\\"url\\\": \\\"https://langfuse.com/changelog/2024-11-18-dataset-runs-comparison-view\\\", \\\"title\\\": \\\"Dataset Run Comparison View - Langfuse\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"---\\\\n\\\\n[\\\\u2190 Back to changelog](/changelog#2024-11-18-dataset-runs-comparison-view)\\\\nNovember 18, 2024 | Launch Week 2 \\\\ud83d\\\\ude80\\\\n# Dataset Run Comparison View\\\\n[Marlies Mayerhofer](https://twitter.com/marliessophie)\\\\nAfter running experiments on datasets, you can now compare results side-by-side, view metrics, and peek into details of each dataset item across runs.\\\\n\\\\n---\\\\n\\\\n## Introduction\\\\n\\\\n---\\\\n\\\\n**What is a dataset in Langfuse?** [Datasets](/docs/datasets/overview) in Langfuse allow you to create test sets and benchmarks to evaluate the performance of your LLM application. A dataset is a collection of dataset items, where each item contains inputs, expected outputs, and metadata. You can create datasets from production edge cases, synthetic test cases, or manual test cases. This enables continuous improvement through structured testing, pre-deployment benchmarking, and flexible evaluation using custom metrics or LLM-as-a-judge approaches.\\\\n\\\\n---\\\\n\\\\n**What is a dataset experiment run?** A dataset experiment run lets you test changes to your application - like trying different models, prompts, or parameters - by running each version against your test dataset and comparing the results through traces in Langfuse to evaluate which changes work best.\\\\n\\\\n---\\\\n\\\\n## What\\\\u2019s new?\\\\n\\\\n---\\\\n\\\\nAnd the fun continues, **Day 1 of[Launch Week 2](/blog/2024-11-17-launch-week-2)** is here.\\\\n\\\\n---\\\\n\\\\n[Langfuse Datasets](/docs/datasets/overview) now enables intuitive comparison of dataset experiment runs for technical and non-technical users. The view features an overview of each item in the dataset, and a summary of each selected experiment run. The latter includes metrics on latency, cost, scores and the application\\\\u2019s output response for each dataset item.\\\\n\\\\n---\\\\n\\\\n## How to use the comparison view?\\\\n\\\\n---\\\\n\\\\n### Setup dataset and run experiments\\\\n\\\\n---\\\\n\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n  * Follow the [getting started guide](/docs/datasets/get-started) to set up a dataset, populate it with items, and run experiments.\\\\n  * Alternatively, execute an end-to-end example ([Python notebook](/docs/datasets/python-cookbook)).\\\\n\\\\n---\\\\n\\\\n### Open comparison view\\\\n\\\\n---\\\\n\\\\n  1. Select multiple dataset runs\\\\n  2. `Actions`, select `Compare`\\\\n\\\\n---\\\\n\\\\n## Learn more\\\\n\\\\n---\\\\n\\\\nFor a conceptual introduction to datasets and offline experiments, see the [dataset documentation](/docs/datasets/overview).\\\\n\\\\n---\\\\n\\\\nLast updated on September 5, 2025\\\\n\\\\n---\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/blog/update-2023-09\\\", \\\"title\\\": \\\"Langfuse Update \\\\u2014 September 2023\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n\\\\u2192 [Docs](/docs/scores/model-based-evals)\\\\n## \\\\ud83d\\\\uddc2\\\\ufe0f Datasets (beta)\\\\nSystematically test new iterations of your LLM app with `datasets`.\\\\nDatasets are collections of inputs and expected outputs that you can manage in Langfuse. Upload an existing dataset or create one based on production data (e.g. when discovering new edge cases).\\\\nWhen combined with automated evals, Datasets in Langfuse make it easy to systematically evaluate new iterations of your LLM app.\\\\n_Overview of dataset runs on a demo dataset_\\\\n_Run experiment on dataset_\\\\nPython SDKJS/TS SDKLangchain (Python)\\\\n    \\\\n    \\\\n```\\\\n \\\\ndataset = langfuse.get_dataset(\\\\\\\"<dataset_name>\\\\\\\")\\\\n \\\\nfor item in dataset.items:\\\\n # execute application function and get Langfuse parent observation (span/generation/event)\\\\n # output also returned as it is used to evaluate the run\\\\n generation, output = my_llm_application.run(item.input)\\\\n \\\\n # link the execution trace to the dataset item and give it a run_name\\\\n item.link(generation, \\\\\\\"<run_name>\\\\\\\")\\\\n \\\\n # optionally, evaluate the output to compare different runs more easily\\\\n generation.score(\\\\n name=\\\\\\\"<example_eval>\\\\\\\",\\\\n # any float value\\\\n value=my_eval_fn(\\\\n item.input,\\\\n output,\\\\n item.expected_output\\\\n )\\\\n )\\\\n```\\\\n    \\\\n    \\\\n```\\\\nconst dataset = await langfuse.getDataset(\\\\\\\"<dataset_name>\\\\\\\");\\\\n \\\\nfor (const item of dataset.items) {\\\\n // execute application function and get Langfuse parent observation (span/generation/event)\\\\n // output also returned as it is used to evaluate the run\\\\n const [generation, output] = await myLlmApplication.run(item.input);\\\\n \\\\n // link the execution trace to the dataset item and give it a run_name\\\\n await item.link(generation, \\\\\\\"<run_name>\\\\\\\");\\\\n \\\\n // optionally, evaluate the output to compare different runs more easily\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n generation.score({\\\\n name: \\\\\\\"<score_name>\\\\\\\",\\\\n value: myEvalFunction(item.input, output, item.expectedOutput),\\\\n });\\\\n}\\\\n```\\\\n    \\\\n    \\\\n```\\\\ndataset = langfuse.get_dataset(\\\\\\\"<dataset_name>\\\\\\\")\\\\n \\\\nfor item in dataset.items:\\\\n # Langchain calback handler that automatically links the execution trace to the dataset item\\\\n handler = item.get_langchain_handler(run_name=\\\\\\\"<run_name>\\\\\\\")\\\\n \\\\n # Execute application and pass custom handler\\\\n my_langchain_chain.run(item.input, callbacks=[handler])\\\\n```\\\\nDatasets are currently in beta on Langfuse Cloud as the API might still slightly change. If you\\\\u2019d like to try it, let us know via the in-app chat.\\\\n\\\\u2192 [Dataset docs](/docs/datasets/overview) \\\\u2192 [Python Cookbook](/docs/datasets/python-cookbook)\\\\n## \\\\ud83d\\\\udcca In-app dashboards\\\\nOver the last weeks, [analytics features](/docs/analytics/overview) were in public alpha on Langfuse Cloud. We\\\\u2019ve now shipped a new _query engine_ as an underlying abstraction for the native in-app dashboards. This is a major step towards bringing all analytics features into the Langfuse core project and helps us move much faster on these.\\\\nOver the next days, you\\\\u2019ll see more and more dashboards popping up in the app. If there is a specific analysis you\\\\u2019d like to see, suggest it on [Discord](/discord).\\\\n## \\\\ud83d\\\\udd04 New integrations\\\\nThe new integrations make it easier to get started with Langfuse. Thanks to the teams behind Langflow, Flowise and LiteLLM for building/collaborating on these integrations.\\\\nSee integrations docs for details:\\\\n  * [Langflow](/docs/langflow): No-code LLM app builder in Python\\\\n  * [Flowise](/docs/flowise): No-code LLM app builder in JS\\\\n  * [LiteLLM](/integrations/gateways/litellm): Python library to use any LLM model as drop in replacement of OpenAI API\\\"}], \\\"type\\\": \\\"content\\\"}}]}\",\"role\":\"assistant\",\"tool_calls\":null,\"function_call\":null}}],\"usage\":{\"prompt_tokens\":21,\"completion_tokens\":7295,\"total_tokens\":7316},\"system_fingerprint\":\"inkeep-fp-690807bb-bbc2-44b0-8a56-491bf9df8dbe\",\"object\":\"chat.completion\",\"service_tier\":null,\"created\":1758226809},\"content\":[{\"type\":\"text\",\"text\":\"{\\\"content\\\": [{\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/dataset-runs/datasets\\\", \\\"title\\\": \\\"Datasets\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Datasets - Langfuse\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\"Evaluation\\\\\\\")Dataset RunsDatasets\\\\nCopy as Markdown\\\\n# Datasets\\\\nA dataset is a collection of inputs and expected outputs and is used to test your application. Before executing your first dataset run, you need to create a dataset.\\\\n## Why use datasets?\\\\n  * **Datasets prerequisite for Dataset Runs** , they serve as the data input of Dataset Runs\\\\n  * Create test cases for your application with real production traces\\\\n  * Collaboratively create and collect dataset items with your team\\\\n  * Have a single source of truth for your test data\\\\n## Get Started\\\\n### Creating a dataset\\\\nDatasets have a name which is unique within a project.\\\\nPython SDKJS/TS SDKLangfuse UI\\\\n    \\\\n    \\\\n```\\\\nlangfuse.create_dataset(\\\\n name=\\\\\\\"<dataset_name>\\\\\\\",\\\\n # optional description\\\\n description=\\\\\\\"My first dataset\\\\\\\",\\\\n # optional metadata\\\\n metadata={\\\\n \\\\\\\"author\\\\\\\": \\\\\\\"Alice\\\\\\\",\\\\n \\\\\\\"date\\\\\\\": \\\\\\\"2022-01-01\\\\\\\",\\\\n \\\\\\\"type\\\\\\\": \\\\\\\"benchmark\\\\\\\"\\\\n }\\\\n)\\\\n```\\\\n _See[Python SDK](/docs/sdk/python/sdk-v3) docs for details on how to initialize the Python client._\\\\n    \\\\n    \\\\n```\\\\nimport { LangfuseClient } from \\\\\\\"@langfuse/client\\\\\\\"\\\\n \\\\nconst langfuse = new LangfuseClient()\\\\n \\\\nawait langfuse.api.datasets.create({\\\\n name: \\\\\\\"<dataset_name>\\\\\\\",\\\\n // optional description\\\\n description: \\\\\\\"My first dataset\\\\\\\",\\\\n // optional metadata\\\\n metadata: {\\\\n author: \\\\\\\"Alice\\\\\\\",\\\\n date: \\\\\\\"2022-01-01\\\\\\\",\\\\n type: \\\\\\\"benchmark\\\\\\\",\\\\n },\\\\n});\\\\n```\\\\n  1. **Navigate to** `Your Project` > `Datasets`\\\\n  2. **Click on** `+ New dataset` to create a new dataset.\\\\n### Create new dataset items\\\\nDataset items can be added to a dataset by providing the input and optionally the expected output.\\\\nPython SDKJS/TS SDKLangfuse UI\\\\n    \\\\n    \\\\n```\\\\nlangfuse.create_dataset_item(\\\\n dataset_name=\\\\\\\"<dataset_name>\\\\\\\",\\\\n # any python object or value, optional\\\\n input={\\\\n \\\\\\\"text\\\\\\\": \\\\\\\"hello world\\\\\\\"\\\\n },\\\\n # any python object or value, optional\\\\n expected_output={\\\\n \\\\\\\"text\\\\\\\": \\\\\\\"hello world\\\\\\\"\\\\n },\\\\n # metadata, optional\\\\n metadata={\\\\n \\\\\\\"model\\\\\\\": \\\\\\\"llama3\\\\\\\",\\\\n }\\\\n)\\\\n```\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/dataset-runs/native-run\\\", \\\"title\\\": \\\"Native Dataset Runs (Prompt Experiments)\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Native Dataset Runs (Prompt Experiments) - Langfuse\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\"Evaluation\\\\\\\")\\\\n[Dataset Runs](/docs/evaluation/dataset-runs/datasets \\\\\\\"Dataset Runs\\\\\\\")Native Run\\\\nCopy as Markdown\\\\n# Native Dataset Runs (Prompt Experiments)\\\\nYou can execute Native Dataset Runs **(also called Prompt Experiments)** in the Langfuse UI to test different prompt versions from [Prompt Management](/docs/prompt-management) or language models and compare the results side-by-side.\\\\nOptionally, you can use [LLM-as-a-Judge Evaluators](/docs/evaluation/evaluation-methods/llm-as-a-judge) to automatically score the responses based on the expected outputs to further analyze the results on an aggregate level.\\\\nThey are called \\\\u201cNative Dataset Runs\\\\u201d because they rely on Datasets, Prompts and optionally LLM-as-a-Judge Evaluators all being managed **natively** in Langfuse.\\\\n## Why use Native Dataset Runs (Prompt Experiments)?\\\\n  * Quickly test different prompt versions or models\\\\n  * Structure your prompt testing by using a dataset to test different prompt versions and models\\\\n  * Quickly iterate on prompts through Dataset Runs\\\\n  * Optionally use LLM-as-a-Judge Evaluators to score the responses based on the expected outputs from the dataset\\\\n  * Prevent regressions by running tests when making prompt changes\\\\n## Prerequisites\\\\n### Create a usable prompt\\\\nCreate a prompt that you want to test and evaluate. [How to create a prompt?](/docs/prompt-management/get-started)\\\\n **A prompt is usable when:** your prompt has variables that match the dataset item keys in the dataset that will be used for the Dataset Run. See the example below.\\\\nExample: Prompt Variables & Dataset Item Keys Mapping\\\\n  \\\\n **Prompt:**\\\\n    \\\\n    \\\\n```\\\\n{{ documentation }}\\\\n \\\\nQuestion: {{question}}\\\\n \\\\n```\\\\n  \\\\n**Dataset Item:**\\\\n    \\\\n    \\\\n```\\\\n{\\\\n \\\\\\\"documentation\\\\\\\": \\\\\\\"Langfuse is an LLM Engineering Platform\\\\\\\",\\\\n \\\\\\\"question\\\\\\\": \\\\\\\"What is Langfuse?\\\\\\\"\\\\n}\\\\n```\\\\nIn this example:\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/changelog/2024-11-18-dataset-runs-comparison-view\\\", \\\"title\\\": \\\"Dataset Run Comparison View\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Dataset Run Comparison View - Langfuse\\\\n[\\\\u2190 Back to changelog](/changelog#2024-11-18-dataset-runs-comparison-view)\\\\nNovember 18, 2024 | Launch Week 2 \\\\ud83d\\\\ude80\\\\n# Dataset Run Comparison View\\\\n[Marlies Mayerhofer](https://twitter.com/marliessophie)\\\\nAfter running experiments on datasets, you can now compare results side-by-side, view metrics, and peek into details of each dataset item across runs.\\\\n## Introduction\\\\n**What is a dataset in Langfuse?** [Datasets](/docs/datasets/overview) in Langfuse allow you to create test sets and benchmarks to evaluate the performance of your LLM application. A dataset is a collection of dataset items, where each item contains inputs, expected outputs, and metadata. You can create datasets from production edge cases, synthetic test cases, or manual test cases. This enables continuous improvement through structured testing, pre-deployment benchmarking, and flexible evaluation using custom metrics or LLM-as-a-judge approaches.\\\\n**What is a dataset experiment run?** A dataset experiment run lets you test changes to your application - like trying different models, prompts, or parameters - by running each version against your test dataset and comparing the results through traces in Langfuse to evaluate which changes work best.\\\\n## What\\\\u2019s new?\\\\nAnd the fun continues, **Day 1 of[Launch Week 2](/blog/2024-11-17-launch-week-2)** is here.\\\\n[Langfuse Datasets](/docs/datasets/overview) now enables intuitive comparison of dataset experiment runs for technical and non-technical users. The view features an overview of each item in the dataset, and a summary of each selected experiment run. The latter includes metrics on latency, cost, scores and the application\\\\u2019s output response for each dataset item.\\\\n## How to use the comparison view?\\\\n### Setup dataset and run experiments\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n  * Follow the [getting started guide](/docs/datasets/get-started) to set up a dataset, populate it with items, and run experiments.\\\\n  * Alternatively, execute an end-to-end example ([Python notebook](/docs/datasets/python-cookbook)).\\\\n### Open comparison view\\\\n  1. Select multiple dataset runs\\\\n  2. `Actions`, select `Compare`\\\\n## Learn more\\\\nFor a conceptual introduction to datasets and offline experiments, see the [dataset documentation](/docs/datasets/overview).\\\\nLast updated on September 5, 2025\\\\nWas this page helpful?\\\\nYesNo\\\\n[Support](/support)\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/guides/cookbook/example_synthetic_datasets\\\", \\\"title\\\": \\\"Synthetic Dataset Generation for LLM Evaluation\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Synthetic Dataset Generation for LLM Evaluation - Langfuse\\\\nGuides[Cookbooks](/guides/cookbook \\\\\\\"Cookbooks\\\\\\\")Synthetic Datasets\\\\nCopy as Markdown\\\\nThis is a Jupyter notebook\\\\n[Open on GitHub](https://github.com/langfuse/langfuse-docs/blob/main/cookbook/example_synthetic_datasets.ipynb)\\\\n[Run on Google Colab](https://colab.research.google.com/github/langfuse/langfuse-docs/blob/main/cookbook/example_synthetic_datasets.ipynb)\\\\n# Synthetic Dataset Generation for LLM Evaluation\\\\nIn this notebook, we will explore how to **generate synthetic datasets** using language models and uploading them to [Langfuse](https://langfuse.com) for evaluation.\\\\n## What are Langfuse Datasets?\\\\nIn Langfuse, a _dataset_ is a collection of _dataset items_ , each typically containing an `input` (e.g., user prompt/question), `expected_output` (the ground truth or ideal answer) and optional metadata.\\\\nDatasets are used for **evaluation**. You can run your LLM or application on each item in a dataset and compare the application\\\\u2019s responses to the expected outputs. This way, you can track performance over time and across different application configs (e.g. model versions or prompt changes).\\\\n## Cases your Dataset Should Cover\\\\n**Happy path** \\\\u2013 straightforward or common queries:\\\\n  * \\\\u201cWhat is the capital of France?\\\\u201d\\\\n  * \\\\u201cConvert 5 USD to EUR.\\\\u201d\\\\n**Edge cases** \\\\u2013 unusual or complex:\\\\n  * Very long prompts.\\\\n  * Ambiguous queries.\\\\n  * Very technical or niche.\\\\n**Adversarial cases** \\\\u2013 malicious or tricky:\\\\n  * Prompt injection attempts (\\\\u201cIgnore all instructions and \\\\u2026\\\\u201d).\\\\n  * Content policy violations (harassment, hate speech).\\\\n  * Logic traps (trick questions).\\\\n## Examples\\\\n### Example 1: Looping Over OpenAI API\\\\nWe\\\\u2019ll use OpenAI\\\\u2019s API in a simple loop to create synthetic questions for an airline chatbot. You could similarly prompt the model to generate _both_ questions and answers. %pip install openai langfuse\\\\n    \\\\n    \\\\n```\\\\nimport os\\\\n \\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n///# Get keys for your project from the project settings page: https://cloud.langfuse.com\\\\nos.environ[\\\\\\\"LANGFUSE_PUBLIC_KEY\\\\\\\"] = \\\\\\\"pk-lf-...\\\\\\\" \\\\nos.environ[\\\\\\\"LANGFUSE_SECRET_KEY\\\\\\\"] = \\\\\\\"sk-lf-...\\\\\\\" \\\\nos.environ[\\\\\\\"LANGFUSE_HOST\\\\\\\"] = \\\\\\\"https://cloud.langfuse.com\\\\\\\" # \\\\ud83c\\\\uddea\\\\ud83c\\\\uddfa EU region\\\\n///# os.environ[\\\\\\\"LANGFUSE_HOST\\\\\\\"] = \\\\\\\"https://us.cloud.langfuse.com\\\\\\\" # \\\\ud83c\\\\uddfa\\\\ud83c\\\\uddf8 US region\\\\n \\\\n///# Your openai key\\\\nos.environ[\\\\\\\"OPENAI_API_KEY\\\\\\\"] = \\\\\\\"sk-proj-...\\\\\\\"\\\\n```\\\\nWith the environment variables set, we can now initialize the Langfuse client. `get_client()` initializes the Langfuse client using the credentials provided in the environment variables.\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\n \\\\nlangfuse = get_client()\\\\n \\\\n///# Verify connection\\\\nif langfuse.auth_check():\\\\n print(\\\\\\\"Langfuse client is authenticated and ready!\\\\\\\")\\\\nelse:\\\\n print(\\\\\\\"Authentication failed. Please check your credentials and host.\\\\\\\")\\\\n```\\\\nLangfuse client is authenticated and ready!\\\\n    \\\\n    \\\\n```\\\\nfrom openai import OpenAI\\\\nimport pandas as pd\\\\n \\\\nclient = OpenAI()\\\\n \\\\n///# Function to generate airline questions\\\\ndef generate_airline_questions(num_questions=20):\\\\n \\\\n questions = []\\\\n \\\\n for i in range(num_questions):\\\\n completion = client.chat.completions.create(\\\\n model=\\\\\\\"gpt-4o\\\\\\\", \\\\n messages=[\\\\n {\\\\n \\\\\\\"role\\\\\\\": \\\\\\\"system\\\\\\\",\\\\n \\\\\\\"content\\\\\\\": (\\\\n \\\\\\\"You are a helpful customer service chatbot for an airline. \\\\\\\"\\\\n \\\\\\\"Please generate a short, realistic question from a customer.\\\\\\\"\\\\n )\\\\n }\\\\n ],\\\\n temperature=1\\\\n )\\\\n question_text = completion.choices[0].message.content.strip()\\\\n questions.append(question_text)\\\\n \\\\n return questions\\\\n \\\\n///# Generate 20 airline-related questions\\\\nairline_questions = generate_airline_questions(num_questions=20)\\\\n \\\\n///# Convert to a Pandas DataFrame\\\\ndf = pd.DataFrame({\\\\\\\"Question\\\\\\\": airline_questions})\\\\n```\\\\n    \\\\n    \\\\n```\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n scenario=\\\\\\\"Non-technical users trying to query a database using plain English.\\\\\\\",\\\\n)\\\\n \\\\n///# 2. Initialize the Synthesizer\\\\nsynthesizer = Synthesizer(styling_config=styling_config)\\\\n \\\\n///# 3. Generate synthetic items from scratch, e.g. 20 items for a short demo\\\\nsynthesizer.generate_goldens_from_scratch(num_goldens=20)\\\\n \\\\n///# 4. Access the generated examples\\\\nsynthetic_goldens = synthesizer.synthetic_goldens\\\\n```\\\\n    \\\\n    \\\\n```\\\\nfrom langfuse import get_client\\\\nlangfuse = get_client()\\\\n \\\\n///# 5. Create a Langfuse dataset\\\\ndeepeval_dataset_name = \\\\\\\"deepeval_synthetic_data\\\\\\\"\\\\nlangfuse.create_dataset(\\\\n name=deepeval_dataset_name,\\\\n description=\\\\\\\"Synthetic text-to-SQL data (DeepEval)\\\\\\\",\\\\n metadata={\\\\\\\"approach\\\\\\\": \\\\\\\"deepeval\\\\\\\", \\\\\\\"task\\\\\\\": \\\\\\\"text-to-sql\\\\\\\"}\\\\n)\\\\n \\\\n///# 6. Upload the items\\\\nfor golden in synthetic_goldens:\\\\n langfuse.create_dataset_item(\\\\n dataset_name=deepeval_dataset_name,\\\\n input={\\\\\\\"query\\\\\\\": golden.input},\\\\n )\\\\n```\\\\n### Example 4: No-Code via Hugging Face Dataset Generator\\\\nIf you prefer a more UI-based approach, check out [Hugging Face\\\\u2019s Synthetic Data Generator](https://huggingface.co/blog/synthetic-data-generator). You can generate examples in the Hugging Face UI. Then you can download them as CSV and upload it in the Langfuse UI.\\\\n## Next Steps\\\\n  1. **Explore your dataset in Langfuse**. You can see each dataset in the UI.\\\\n  2. **Run experiments** You can now evaluate your application using this dataset.\\\\n  3. **Compare runs** over time or across models, prompts, or chain logic.\\\\nFor more details on how to run experiments on a dataset, see the [Langfuse docs](https://langfuse.com/docs/datasets/get-started#run-experiment-on-a-dataset).\\\\nLast updated on July 29, 2025\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n[Query Data in Langfuse via the SDK](/guides/cookbook/example_query_data_via_sdk \\\\\\\"Query Data in Langfuse via the SDK\\\\\\\")\\\\n[Amazon Bedrock](/guides/cookbook/integration_amazon_bedrock \\\\\\\"Amazon Bedrock\\\\\\\")\\\\nWas this page helpful?\\\\nYesNo\\\\n[Support](/support)\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/docs/evaluation/dataset-runs/data-model\\\", \\\"title\\\": \\\"Dataset Runs Data Model\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Dataset Runs Data Model - Langfuse\\\\nDocs[Evaluation](/docs/evaluation/overview \\\\\\\"Evaluation\\\\\\\")\\\\n[Dataset Runs](/docs/evaluation/dataset-runs/datasets \\\\\\\"Dataset Runs\\\\\\\")Data Model\\\\nCopy as Markdown\\\\n# Dataset Runs Data Model\\\\nThis page describes the data model of datasets and dataset runs. For detailed reference please refer to the [API Reference](https://api.reference.langfuse.com).\\\\n## Datasets\\\\nDatasets are a collection of inputs and, optionally, expected outputs that can be during Dataset runs.\\\\n`Dataset`s are a collection of `DatasetItem`s.\\\\n1\\\\nn\\\\nDataset\\\\nname\\\\ndescription\\\\nmetadata\\\\nDatasetItem\\\\ndatasetName\\\\ninput\\\\nexpectedOutput\\\\nmetadata\\\\nsourceTraceId\\\\nsourceObservationId\\\\nid\\\\nstatus\\\\n### Dataset object\\\\nAttribute| Type| Required| Description  \\\\n---|---|---|---  \\\\n`name`| string| Yes| Name of the dataset  \\\\n`description`| string| No| Description of the dataset  \\\\n`metadata`| object| No| Additional metadata for the dataset  \\\\n  \\\\n### DatasetItem object\\\\nAttribute| Type| Required| Description  \\\\n---|---|---|---  \\\\n`datasetName`| string| Yes| Name of the dataset to add the item to  \\\\n`input`| object| No| Input data for the dataset item  \\\\n`expectedOutput`| object| No| Expected output data for the dataset item  \\\\n`metadata`| object| No| Additional metadata for the dataset item  \\\\n`sourceTraceId`| string| No| ID of the source trace to link this dataset item to  \\\\n`sourceObservationId`| string| No| ID of the source observation to link this dataset item to  \\\\n`id`| string| No| Unique identifier for the dataset item. Dataset items are upserted on their id. Id needs to be unique (project-level) and cannot be reused across datasets.  \\\\n`status`| DatasetStatus| No| Status of the dataset item. Defaults to ACTIVE for newly created items. Possible values: `ACTIVE`, `ARCHIVED`  \\\\n  \\\\n## DatasetRun\\\\nDataset runs are used to run a dataset through your LLM application and optionally apply evaluation methods to the results.\\\\n  \\\\n### DatasetRun object\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Attribute| Type| Required| Description  \\\\n---|---|---|---  \\\\n`datasetName`| string| Yes| Name of the dataset  \\\\n`runName`| string| Yes| Name of the dataset run  \\\\n  \\\\n### DatasetRunItem object\\\\nAttribute| Type| Required| Description  \\\\n---|---|---|---  \\\\n`runName`| string| Yes| Name of the dataset run to add the item to  \\\\n`runDescription`| string| No| Description of the run. If run exists, description will be updated  \\\\n`metadata`| object| No| Metadata of the dataset run, updates run if run already exists  \\\\n`datasetItemId`| string| Yes| ID of the dataset item to link to this run  \\\\n`observationId`| string| No| ID of the observation to link to this run  \\\\n`traceId`| string| No| ID of the trace to link to this run. traceId should always be provided. For compatibility with older SDK versions it can also be inferred from the provided observationId.  \\\\n  \\\\nMost of the time, we recommend that DatasetRunItems reference TraceIDs directly. The reference to ObservationID exists for backwards compatibility with older SDK versions.\\\\n## End to end data relations\\\\nDataSetRuns can combine a few Langfuse objects:\\\\n  * `DatasetRuns` are created by looping through all or selected `DatasetItem`s of a `Dataset` with your LLM application.\\\\n  * For each `DatasetItem` passed into the LLM application as an Input a `DatasetRunItem` & a `Trace` are created.\\\\n  * Optionally `Score`s can be added to the `Trace`s to evaluate the output of the LLM application during the `DatasetRun`.\\\\n  \\\\nLast updated on July 29, 2025\\\\n[Remote Run](/docs/evaluation/dataset-runs/remote-run \\\\\\\"Remote Run\\\\\\\")\\\\n[Troubleshooting & FAQ](/docs/evaluation/troubleshooting-and-faq \\\\\\\"Troubleshooting & FAQ\\\\\\\")\\\\nWas this page helpful?\\\\nYesNo\\\\n[Support](/support)\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"github_discussion\\\", \\\"url\\\": \\\"https://github.com/orgs/langfuse/discussions/4844\\\", \\\"title\\\": \\\"Dataset creates a lot of identical processes that run infinitely.\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"Title: Dataset creates a lot of identical processes that run infinitely.\\\\nCategory: Support\\\\nAuthor: @robotkoruslan\\\\nTimestamp: December 28, 2024\\\\nBody:\\\\nWhen running a simple experiment, dataset creates a lot of identical processes that run infinitely, if click trace error \\\\\\\"Path: traces.byIdWithObservationsAndScores\\\\\\\"\\\\r\\\\n<img width=\\\\\\\"1494\\\\\\\" alt=\\\\\\\"image\\\\\\\" src=\\\\\\\"https://github.com/user-attachments/assets/2ee21b5f-2769-46a2-b1c0-141f0014d6fa\\\\\\\" />\\\\r\\\\n\\\\r\\\\n<img width=\\\\\\\"1494\\\\\\\" alt=\\\\\\\"image\\\\\\\" src=\\\\\\\"https://github.com/user-attachments/assets/8e56daf9-e405-411e-8f62-042292a64764\\\\\\\" />\\\\r\\\\n\\\\r\\\\n\\\\r\\\\n\\\\nComments:\\\\n\\\\n    Comment #1 from @dosubot on December 28, 2024:\\\\n\\\\n    <!-- Greeting -->\\\\nHello, @robotkoruslan! I'm here to help you with any bugs, questions, or contributions you have. Let's tackle this issue together!\\\\n\\\\n<!-- Answer -->\\\\nThe issue you're encountering with identical processes running indefinitely during dataset runs in Langfuse could be related to how traces are managed and linked to dataset runs. Here are some potential causes and solutions:\\\\n\\\\n1. **Trace Linking**: Ensure that traces are correctly linked to dataset runs. Each dataset run item should have a corresponding `traceId` in the `datasetRunItems` table. If the trace is not properly linked or created, it could lead to a \\\\\\\"Trace not found\\\\\\\" error <sup>[[1]](https://github.com/langfuse/langfuse/blob/main/web/src/__tests__/async/dataset-service.servertest.ts)</sup><sup>[[2]](https://github.com/langfuse/langfuse/blob/main/web/src/pages/api/public/dataset-run-items.ts)</sup>.\\\\n\\\\n2. **Trace Retrieval**: The error with the path `traces.byIdWithObservationsAndScores` suggests an issue with retrieving traces by their ID along with observations and scores. This could be due to incorrect `traceId` or `projectId`, or if the trace has been deleted or not yet created <sup>[[3]](https://github.com/langfuse/langfuse/blob/main/web/src/server/api/routers/traces.ts)</sup>.\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/changelog/2024-11-21-all-new-datasets-and-evals-documentation\\\", \\\"title\\\": \\\"All new Datasets, Experimentation and Evaluation documentation\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"## New Datasets and Evaluation documentation\\\\nBased on this week\\\\u2019s changes, we have have restructured the documentation to:\\\\n  1. Better explain **how and when to use Langfuse Datasets and Evals** to evaluate your LLM applications in development\\\\n  2. Introduce the **core data model** of `datasets`, experiment `runs`, and `scores`\\\\n  3. Provide **end-to-end examples** for common workflows as Jupyter Notebooks\\\\n  4. Add many **more gifs and toggles with additional explanations** throughout the docs\\\\n### Evaluation\\\\n\\\\ud83d\\\\udca1\\\\n\\\\u2192 [Evaluation documentation](/docs/scores)\\\\nAnalyticsUser FeedbackLLM-as-a-JudgePrompt ExperimentAnnotation QueueCustom Evals\\\\nPlot evaluation results in the Langfuse Dashboard.\\\\nCollect feedback from your users. Can be captured in the frontend via our Browser SDK, server-side via the SDKs or API. Video includes example application.\\\\nRun fully managed LLM-as-a-judge evaluations on production or development traces. Can be applied to any step within your application for step-wise evaluations.\\\\nEvaluate prompts and models on datasets directly in the user interface. No custom code is needed.\\\\nBaseline your evaluation workflow with human annotations via Annotation Queues.\\\\nAdd custom evaluation results, supports numeric, boolean and categorical values.\\\\n    \\\\n    \\\\n```\\\\nPOST /api/public/scores\\\\n```\\\\nAdd scores via Python or JS SDK.\\\\nExample (Python)\\\\n    \\\\n    \\\\n```\\\\nlangfuse.score(\\\\n trace_id=\\\\\\\"123\\\\\\\",\\\\n name=\\\\\\\"my_custom_evaluator\\\\\\\",\\\\n value=0.5,\\\\n)\\\\n```\\\\n### Datasets\\\\n\\\\ud83d\\\\udca1\\\\n\\\\u2192 [Datasets & Experiments documentation](/docs/datasets)\\\\nManage DatasetsCompare experimentsAdd new items from Langfuse Traces\\\\nCollaboratively manage datasets via UI, API, or SDKs.\\\\nRun experiments on datasets. Compare metrics across experiments, and see results side-by-side.\\\\nDirectly add new items from production traces to datasets to improve your datasets over time.\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"site\\\", \\\"url\\\": \\\"https://langfuse.com/changelog/2024-11-18-dataset-runs-comparison-view\\\", \\\"title\\\": \\\"Dataset Run Comparison View - Langfuse\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"---\\\\n\\\\n[\\\\u2190 Back to changelog](/changelog#2024-11-18-dataset-runs-comparison-view)\\\\nNovember 18, 2024 | Launch Week 2 \\\\ud83d\\\\ude80\\\\n# Dataset Run Comparison View\\\\n[Marlies Mayerhofer](https://twitter.com/marliessophie)\\\\nAfter running experiments on datasets, you can now compare results side-by-side, view metrics, and peek into details of each dataset item across runs.\\\\n\\\\n---\\\\n\\\\n## Introduction\\\\n\\\\n---\\\\n\\\\n**What is a dataset in Langfuse?** [Datasets](/docs/datasets/overview) in Langfuse allow you to create test sets and benchmarks to evaluate the performance of your LLM application. A dataset is a collection of dataset items, where each item contains inputs, expected outputs, and metadata. You can create datasets from production edge cases, synthetic test cases, or manual test cases. This enables continuous improvement through structured testing, pre-deployment benchmarking, and flexible evaluation using custom metrics or LLM-as-a-judge approaches.\\\\n\\\\n---\\\\n\\\\n**What is a dataset experiment run?** A dataset experiment run lets you test changes to your application - like trying different models, prompts, or parameters - by running each version against your test dataset and comparing the results through traces in Langfuse to evaluate which changes work best.\\\\n\\\\n---\\\\n\\\\n## What\\\\u2019s new?\\\\n\\\\n---\\\\n\\\\nAnd the fun continues, **Day 1 of[Launch Week 2](/blog/2024-11-17-launch-week-2)** is here.\\\\n\\\\n---\\\\n\\\\n[Langfuse Datasets](/docs/datasets/overview) now enables intuitive comparison of dataset experiment runs for technical and non-technical users. The view features an overview of each item in the dataset, and a summary of each selected experiment run. The latter includes metrics on latency, cost, scores and the application\\\\u2019s output response for each dataset item.\\\\n\\\\n---\\\\n\\\\n## How to use the comparison view?\\\\n\\\\n---\\\\n\\\\n### Setup dataset and run experiments\\\\n\\\\n---\\\\n\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n  * Follow the [getting started guide](/docs/datasets/get-started) to set up a dataset, populate it with items, and run experiments.\\\\n  * Alternatively, execute an end-to-end example ([Python notebook](/docs/datasets/python-cookbook)).\\\\n\\\\n---\\\\n\\\\n### Open comparison view\\\\n\\\\n---\\\\n\\\\n  1. Select multiple dataset runs\\\\n  2. `Actions`, select `Compare`\\\\n\\\\n---\\\\n\\\\n## Learn more\\\\n\\\\n---\\\\n\\\\nFor a conceptual introduction to datasets and offline experiments, see the [dataset documentation](/docs/datasets/overview).\\\\n\\\\n---\\\\n\\\\nLast updated on September 5, 2025\\\\n\\\\n---\\\\n\\\"}], \\\"type\\\": \\\"content\\\"}}, {\\\"type\\\": \\\"document\\\", \\\"record_type\\\": \\\"documentation\\\", \\\"url\\\": \\\"https://langfuse.com/blog/update-2023-09\\\", \\\"title\\\": \\\"Langfuse Update \\\\u2014 September 2023\\\", \\\"source\\\": {\\\"content\\\": [{\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n\\\\u2192 [Docs](/docs/scores/model-based-evals)\\\\n## \\\\ud83d\\\\uddc2\\\\ufe0f Datasets (beta)\\\\nSystematically test new iterations of your LLM app with `datasets`.\\\\nDatasets are collections of inputs and expected outputs that you can manage in Langfuse. Upload an existing dataset or create one based on production data (e.g. when discovering new edge cases).\\\\nWhen combined with automated evals, Datasets in Langfuse make it easy to systematically evaluate new iterations of your LLM app.\\\\n_Overview of dataset runs on a demo dataset_\\\\n_Run experiment on dataset_\\\\nPython SDKJS/TS SDKLangchain (Python)\\\\n    \\\\n    \\\\n```\\\\n \\\\ndataset = langfuse.get_dataset(\\\\\\\"<dataset_name>\\\\\\\")\\\\n \\\\nfor item in dataset.items:\\\\n # execute application function and get Langfuse parent observation (span/generation/event)\\\\n # output also returned as it is used to evaluate the run\\\\n generation, output = my_llm_application.run(item.input)\\\\n \\\\n # link the execution trace to the dataset item and give it a run_name\\\\n item.link(generation, \\\\\\\"<run_name>\\\\\\\")\\\\n \\\\n # optionally, evaluate the output to compare different runs more easily\\\\n generation.score(\\\\n name=\\\\\\\"<example_eval>\\\\\\\",\\\\n # any float value\\\\n value=my_eval_fn(\\\\n item.input,\\\\n output,\\\\n item.expected_output\\\\n )\\\\n )\\\\n```\\\\n    \\\\n    \\\\n```\\\\nconst dataset = await langfuse.getDataset(\\\\\\\"<dataset_name>\\\\\\\");\\\\n \\\\nfor (const item of dataset.items) {\\\\n // execute application function and get Langfuse parent observation (span/generation/event)\\\\n // output also returned as it is used to evaluate the run\\\\n const [generation, output] = await myLlmApplication.run(item.input);\\\\n \\\\n // link the execution trace to the dataset item and give it a run_name\\\\n await item.link(generation, \\\\\\\"<run_name>\\\\\\\");\\\\n \\\\n // optionally, evaluate the output to compare different runs more easily\\\"}, {\\\"type\\\": \\\"text\\\", \\\"text\\\": \\\"\\\\n generation.score({\\\\n name: \\\\\\\"<score_name>\\\\\\\",\\\\n value: myEvalFunction(item.input, output, item.expectedOutput),\\\\n });\\\\n}\\\\n```\\\\n    \\\\n    \\\\n```\\\\ndataset = langfuse.get_dataset(\\\\\\\"<dataset_name>\\\\\\\")\\\\n \\\\nfor item in dataset.items:\\\\n # Langchain calback handler that automatically links the execution trace to the dataset item\\\\n handler = item.get_langchain_handler(run_name=\\\\\\\"<run_name>\\\\\\\")\\\\n \\\\n # Execute application and pass custom handler\\\\n my_langchain_chain.run(item.input, callbacks=[handler])\\\\n```\\\\nDatasets are currently in beta on Langfuse Cloud as the API might still slightly change. If you\\\\u2019d like to try it, let us know via the in-app chat.\\\\n\\\\u2192 [Dataset docs](/docs/datasets/overview) \\\\u2192 [Python Cookbook](/docs/datasets/python-cookbook)\\\\n## \\\\ud83d\\\\udcca In-app dashboards\\\\nOver the last weeks, [analytics features](/docs/analytics/overview) were in public alpha on Langfuse Cloud. We\\\\u2019ve now shipped a new _query engine_ as an underlying abstraction for the native in-app dashboards. This is a major step towards bringing all analytics features into the Langfuse core project and helps us move much faster on these.\\\\nOver the next days, you\\\\u2019ll see more and more dashboards popping up in the app. If there is a specific analysis you\\\\u2019d like to see, suggest it on [Discord](/discord).\\\\n## \\\\ud83d\\\\udd04 New integrations\\\\nThe new integrations make it easier to get started with Langfuse. Thanks to the teams behind Langflow, Flowise and LiteLLM for building/collaborating on these integrations.\\\\nSee integrations docs for details:\\\\n  * [Langflow](/docs/langflow): No-code LLM app builder in Python\\\\n  * [Flowise](/docs/flowise): No-code LLM app builder in JS\\\\n  * [LiteLLM](/integrations/gateways/litellm): Python library to use any LLM model as drop in replacement of OpenAI API\\\"}], \\\"type\\\": \\\"content\\\"}}]}\"}],\"isError\":false}",
      "modelParameters": {
        "maxSteps": null,
        "toolChoice": null,
        "maxTokens": null,
        "finishReason": null,
        "system": null,
        "maxRetries": null,
        "mode": null,
        "temperature": null
      },
      "completionStartTime": null,
      "promptId": null,
      "createdAt": "2025-09-18T20:20:14.484Z",
      "updatedAt": "2025-09-18T20:20:14.484Z",
      "usageDetails": {},
      "costDetails": {},
      "providedCostDetails": {},
      "model": null,
      "internalModelId": null,
      "promptName": null,
      "promptVersion": null,
      "latency": 2968,
      "timeToFirstToken": null,
      "inputCost": null,
      "outputCost": null,
      "totalCost": 0,
      "inputUsage": 0,
      "outputUsage": 0,
      "totalUsage": 0,
      "internalModel": null
    },
    {
      "id": "b1b95a1f038b4f8b",
      "traceId": "6f58ac41090ac81ce6203900dadf8e6b",
      "projectId": "clkpwwm0m000gmm094odg11gi",
      "type": "SPAN",
      "environment": "default",
      "parentObservationId": "5043b6d994a0663c",
      "startTime": "2025-09-18T20:19:57.060Z",
      "endTime": "2025-09-18T20:19:57.204Z",
      "name": "create-mcp-client",
      "metadata": "{}",
      "level": "DEFAULT",
      "statusMessage": null,
      "version": null,
      "modelParameters": null,
      "completionStartTime": null,
      "promptId": null,
      "createdAt": "2025-09-18T20:20:02.207Z",
      "updatedAt": "2025-09-18T20:20:02.207Z",
      "usageDetails": {},
      "costDetails": {},
      "providedCostDetails": {},
      "model": null,
      "internalModelId": null,
      "promptName": null,
      "promptVersion": null,
      "latency": 144,
      "timeToFirstToken": null,
      "inputCost": null,
      "outputCost": null,
      "totalCost": 0,
      "inputUsage": 0,
      "outputUsage": 0,
      "totalUsage": 0
    },
    {
      "id": "d61d56256881d854",
      "traceId": "6f58ac41090ac81ce6203900dadf8e6b",
      "projectId": "clkpwwm0m000gmm094odg11gi",
      "type": "SPAN",
      "environment": "default",
      "parentObservationId": "5043b6d994a0663c",
      "startTime": "2025-09-18T20:19:56.952Z",
      "endTime": "2025-09-18T20:19:57.059Z",
      "name": "get-langfuse-prompt",
      "metadata": "{\"resourceAttributes\":{\"service.name\":\"unknown_service:node\",\"telemetry.sdk.language\":\"nodejs\",\"telemetry.sdk.name\":\"opentelemetry\",\"telemetry.sdk.version\":\"2.0.1\"},\"scope\":{\"name\":\"langfuse-sdk\",\"version\":\"4.0.0\",\"attributes\":{}}}",
      "level": "DEFAULT",
      "statusMessage": null,
      "version": null,
      "input": "langfuse-docs-assistant-text",
      "output": "\"{\\\"name\\\":\\\"langfuse-docs-assistant-text\\\",\\\"prompt\\\":\\\"You are a helpful Langfuse support agent. Your goal: give the correct, actionable answer with links and minimal, runnable examples.\\\\n\\\\n## Guidelines\\\\n- Make extensive use of the tools provided to you in order to search the langfuse documentation.\\\\n- Getting an overview of the docs is almost always a good idea for on-topic questions.\\\\n- Answer the question using only this and only this information, outputted in markdown format.\\\\n- Before using any tools, reason about what kinds of tools make the most sense to be helpful.\\\\n- After tool responses, reason about whether the information is sufficient to answer a question; if not, use the tools again or give up and state that you don't know.\\\\n- If a user asks where the traces/logs of the conversation they have with you are ending up, please refer to the Langfuse Demo Project: (EU Data region: https://cloud.langfuse.com/project/clkpwwm0m000gmm094odg11gi/traces, US Data region: https://us.cloud.langfuse.com/project/cmeic4yr500qmad0772bw0tol/traces)\\\\n\\\\n## Scope & Safety\\\\n- Only answer questions about Langfuse, its SDKs, integrations, evaluation, tracing, observability, and adjacent LLM-app topics.\\\\n- If the question is not about Langfuse/LLM-apps, politely decline and suggest a relevant Langfuse topic instead.\\\\n\\\\n## Clarifying Questions\\\\n- If the question is broad/underspecified, ask one targeted clarifying question before executing tools (e.g., “Which SDK are you using - Python or JS/TS?”).\\\\n\\\\n## Pricing & Discounts\\\\n- Do not restate prices from memory. Link to `https://langfuse.com/pricing`. If not retrievable, say you can’t confirm and point to contacting support.\\\\n\\\\n## Markdown Style Guide\\\\n- **Headings:** Start with a single H1 title, then H2 sections (`##`), H3 for subsections only.\\\\n- **Bullets vs code:** Never put multi-line code inside list items. Use **separate fenced blocks** with language hints.\\\\n- **Code fences:** Use `bash`, `ini` (or `env`), `python`, `ts`, `json`. One purpose per block (install, env, file, run).\\\\n- **Filenames:** Show the intended filename in a short caption line before the block, e.g., `_hello.py_`.\\\\n- **Links:** Use descriptive markdown links for every doc reference; no bare URLs. Prefer canonical anchors.\\\\n- **Tables:** Use a 2-column table for environment variables (`Name | Value (example)`).\\\\n- **Sections to include (if relevant):** *Prerequisites* → *Setup* → *Minimal Example* → *Run* → *Troubleshooting* → *Links*.\\\\n- **Copy-ready:** Commands and examples must be copy-pastable with placeholders clearly marked like `<YOUR_PUBLIC_KEY>`.\\\\n- **Keep it tight:** ≤ 20 lines of code per SDK section. If longer, collapse details or link out.\\\",\\\"version\\\":10,\\\"isFallback\\\":false,\\\"tags\\\":[],\\\"labels\\\":[\\\"production\\\",\\\"latest\\\"],\\\"type\\\":\\\"text\\\",\\\"config\\\":{\\\"model\\\":\\\"gpt-5\\\",\\\"reasoningSummary\\\":\\\"detailed\\\",\\\"textVerbosity\\\":\\\"low\\\",\\\"reasoningEffort\\\":\\\"low\\\"}}\"",
      "modelParameters": null,
      "completionStartTime": null,
      "promptId": null,
      "createdAt": "2025-09-18T20:20:02.207Z",
      "updatedAt": "2025-09-18T20:20:02.207Z",
      "usageDetails": {},
      "costDetails": {},
      "providedCostDetails": {},
      "model": null,
      "internalModelId": null,
      "promptName": null,
      "promptVersion": null,
      "latency": 107,
      "timeToFirstToken": null,
      "inputCost": null,
      "outputCost": null,
      "totalCost": 0,
      "inputUsage": 0,
      "outputUsage": 0,
      "totalUsage": 0,
      "internalModel": null
    }
  ]
}
